{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-06-01-dissecting-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRje4Lztr2uE",
        "colab_type": "text"
      },
      "source": [
        "In this post, we will revisit the topic of recurrent neural networks, or RNNs. Although we have used RNNs before in a previous post on character-based text prediction, we glossed over LSTM and assumed it as a black box that just worked. Today, we will take a detailed look at how LSTMs work by dissecting its components. \n",
        "\n",
        "Note that this post was inspired by [this article](https://wiseodd.github.io/techblog/2016/08/12/lstm-backprop/) by Kristiadi. I also heavily referenced [this post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah. If you find any part of this article intriguing and intellectually captivating, you will surely enjoy reading their blogs as well. With this in mind, let's jump right into it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lICHYMQ8AYKQ",
        "colab_type": "text"
      },
      "source": [
        "## Disecting LSTMs\n",
        "\n",
        "Long Short-Term Memory networks, or LSTMs for short, are one of the most widely used building blocks of Recurrent Neural Networks, or RNNs. This is because LSTMs overcame many of the limitations of basic vanilla RNNs: while simple RNN gates are bad at retaining long-term information and only remember input information that were fed into it relatively recenty, LSTMs do a great job of retaining important information, even if they were fed into the cell long time ago. In other words, they are able to somewhat mimic the function of the brain, which involves both long and short-term memory. \n",
        "\n",
        "The structure of an LSTM cell might be summarized as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f_t = \\sigma(W_f [h_{t - 1}, x_t] + b_f) \\tag{1} \\\\ \n",
        "i_t = \\sigma(W_i [h_{t - 1}, x_t] + b_i) \\tag{2} \\\\\n",
        "\\tilde{C}_t = \\text{tanh}(W_C [h_{t - 1}, x_t] + b_C) \\tag{3} \\\\\n",
        "C_t = f_t \\odot C_{t - 1} + i_t \\odot \\tilde{C}_t \\tag{4} \\\\\n",
        "o_t = \\sigma(W_o [h_{t - 1}, x_t] + b_o) \\tag{5}\\\\\n",
        "h_t = o_t \\odot \\text{tanh}(C_t) \\tag{6} \\\\\n",
        "\\hat{y} = \\text{softmax}(W_y h_t + b_y) \\tag{7}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Note that $\\odot$ represents the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)), which is nothing more than just the element-wise multiplication of matrices. \n",
        "\n",
        "This long list of equations surely looks like a lot, but each of them has a specific purpose. Let's take a look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ju_fOsFteTn",
        "colab_type": "text"
      },
      "source": [
        "## Forget Gate\n",
        "\n",
        "The first component of LSTM is the forget gate. This corresponds to these two set of equations:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f_t = \\sigma(W_f [h_{t - 1}, x_t] + b_f) \\tag{1} \\\\ \n",
        "C_t = f_t \\odot C_{t - 1} + i_t \\odot \\tilde{C}_t \\tag{4} \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "(1) is nothing more than just the good old forward pass. We concatenate $h_{t - 1}$ and $x_t$, then multiply it with some weights, add a bias, and apply a sigmoid activation. At this point, you might be wondering why we use a sigmoid activation instead of something like ReLU. The reason behind this choice of activation function becomes apparent once we look at (4), which is how LSTM imitates forgetting. For now, we will only focus on the first term in (4).\n",
        "\n",
        "Recall that the output of a sigmoid activatiion is between 0 and 1. Say the output of applying a sigmoid activation results in some value that is very close to 0. In that case, calculating the Hadamard product will also result in a value of an entry very close to 0. Given the interpretation that $C_t$, also known as the cell state, is an artifical way of simulating long-term memory, we can see how having zeros is similar to forgetfulness: a zero entry effectively means that the network deemed a particular piece of information as obsolete and decided to forget it in favor of accepting new information. In short, the sigmoid activation and the Hadamard product form the basis of LSTM's forget gate. By now, it should be apparent why we use sigmoid activations: instead of causing divergence with something like ReLU, we want to deliberately saturate and cause the network to produce some \"vanishing\" values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KEw8eyu_deE",
        "colab_type": "text"
      },
      "source": [
        "## Updating Cell State\n",
        "\n",
        "But if our LSTM network only keeps forgetting, obviously this is going to be problematic. Instead, we also want to update the cell state using the new input values.\n",
        "\n",
        "Let's take a look at the cell state equation again:\n",
        "\n",
        "$$\n",
        "C_t = f_t \\odot C_{t - 1} + i_t \\odot \\tilde{C}_t \\tag{4}\n",
        "$$\n",
        "\n",
        "Previously when discussing the forget gate, we focused only on the first term. Taking a look at the second term, we note that the term is adding some value to the cell state that has been updated to forget information. It only makes sense, then, for the second term to perform the information update sequence. But to understand the second term, we need to take a look at two other equations:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "i_t = \\sigma(W_i [h_{t - 1}, x_t] + b_i) \\tag{2} \\\\\n",
        "\\tilde{C}_t = \\text{tanh}(W_C [h_{t - 1}, x_t] + b_C) \\tag{3} \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "(2) is another forward pass involving concatenation, much like we saw in (1) with $f_t$. The only difference is that, instead of forgetting, $i_t$ is meant to simulate an update of the cell state. In some LSTM variants, $i_t$ is simply replaced with $1 - f_t$, in which cas the cell state update would be rewritten as\n",
        "\n",
        "$$\n",
        "C_t = f_t \\odot C_{t - 1} + (1 - f_t) \\odot \\tilde{C}_t \\tag{4-2}\n",
        "$$\n",
        "\n",
        "However, we will stick to the convention that uses $i_t$ instead of the simpler variant as shown in (4-2). \n",
        "\n",
        "The best way to think of $i_t$ or $f_t$ is a filter: $i_t$ is a filter that determines which information to be updated and passed onto the cell state. Now all we need are the raw materials to pass into that filter. The raw material is $\\tilde{C}_t$, defined in (3). Notice that we use a $\\text{tanh}$ activation instead of a sigmoid, since the aim of (3) is not to produce a filter with sparse entries, but rather to generate substance, or potential information to be stored in memory. This is more in line with the classic vanilla neural network architecture we are familiar with. \n",
        "\n",
        "Now, we can finally glue the pieces together to understand (4): we enforce forgetfulness, then supply the cell state with new information. This is now the updated cell state, which gets passed onto the next sequence as new inputs are fed into the LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUF7DjA1e2Wr",
        "colab_type": "text"
      },
      "source": [
        "## Generating Output\n",
        "\n",
        "So far, we have only looked at the recurrent features of LSTM; in other words, how it uses information from the past to update its knowledge in the present at time $t$. However, we haven't yet discussed the most important part of any neural network: generating output. \n",
        "\n",
        "Obviously, all that hassle of forgetting and updating the cell state would be utterly meaningless if the cell state is not used to generate output. The whole purpose of maintaining a cell state, therefore, is to imitate long and short-term memory of the brain to generate some output. Thus, it is no surprise that the following two equations are structured the way they are:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "o_t = \\sigma(W_o [h_{t - 1}, x_t] + b_o) \\tag{5}\\\\\n",
        "h_t = o_t \\odot \\text{tanh}(C_t) \\tag{6} \n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "First, we see the familiar forward pass, a familiar structure we have seen earlier. Borrowing the analogy we established in the previous post, $o_t$ is a filter that decides which information to use and drop. The raw material that we pass into this filter is in fact the cell state, processed by a $\\text{tanh}$ activation function. This is also a familiar structure we saw earlier in the information update sequence of the network. Only this time, we use the cell state to generate output. This makes sense somewhat intuitively, since the cell state is essentially the memory of the network, and hence to generate output would require the use of this memory. Of course, this should not be construed so literally since what ultimately happens during backpropagation is entirely up to the network, and at that point we simply lay back and hope for the network to learn the best. This point notwithstanding, I find this admittedly coarse heuristic to be nonetheless useful in intuiting the clockwork behind LSTMs. \n",
        "\n",
        "At this point, we're not quite done yet; $h$ is not a vector of probabilities indicating which letter is the most likely in a one-hot encoded representation. Therefore, we will need to pass it through another affine layer, than apply a softmax activation. Hence, \n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{softmax}(W_y h_t + b_y) \\tag{7}\n",
        "$$\n",
        "\n",
        "$\\hat{y}$ is the final output of an LSTM layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AMm1k7hDy4h",
        "colab_type": "text"
      },
      "source": [
        "# Backward Propagation\n",
        "\n",
        "Here comes the tricky part: backprop. Thankfully, backprop is somewhat simple in the case of LSTMs due to the use of Hadamard products. The routine is not so much different from a vanilla neural network, so let's try to hash out the equations. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY4NWzhEHApE",
        "colab_type": "text"
      },
      "source": [
        "As we already know, backpropagation in neural networks is merely an extended application of the chain rule, with some minor caveats that matrix calculus entails. \n",
        "\n",
        "First, let's begin slow and easy by deriving the expressions for the derivative of the sigmoid and the $\\text{tanh}$ functions. First, below is the derivative of the sigmoid with respect to $x$, the input. Recall that the sigmoid function is defined as `1 / (1 + np.exp(-x))`.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{d \\sigma}{dx} \n",
        "&= \\frac{d}{dx} \\left[ \\frac{1}{1 + e^{-x}} \\right] \\\\\n",
        "&= - (1 + e^{-x})^{-2} \\cdot (- e^{-x}) \\\\ \n",
        "&= \\sigma(x) \\cdot \\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
        "&= \\sigma(x) \\cdot (1 - \\sigma(x)) \\\\\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aellAOfMW76-",
        "colab_type": "text"
      },
      "source": [
        "Let's do the same for $\\text{tanh}$. One useful fact about $\\text{tanh}$ is the fact that it is in fact nothing more than just a rescaled sigmoid. This relationship becomes a bit more apparent when we graph the two functions side by side. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WJV1Nhr70o",
        "colab_type": "code",
        "outputId": "b12fe158-6337-4de0-cd48-007358d595b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot') \n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y_sigmoid = sigmoid(x)\n",
        "y_tanh = tanh(x)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, y_sigmoid)\n",
        "ax.plot(x, y_tanh)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 386.845312 248.518125\" width=\"386.845312pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 386.845312 248.518125 \nL 386.845312 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 44.845313 224.64 \nL 379.645313 224.64 \nL 379.645313 7.2 \nL 44.845313 7.2 \nz\n\" style=\"fill:#e5e5e5;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 90.499858 224.64 \nL 90.499858 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m12d93e4cb2\" style=\"stroke:#555555;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"90.499858\" xlink:href=\"#m12d93e4cb2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(83.128764 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 151.372585 224.64 \nL 151.372585 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"151.372585\" xlink:href=\"#m12d93e4cb2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(144.001491 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 212.245313 224.64 \nL 212.245313 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"212.245313\" xlink:href=\"#m12d93e4cb2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(209.064063 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 273.11804 224.64 \nL 273.11804 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"273.11804\" xlink:href=\"#m12d93e4cb2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2 -->\n      <g style=\"fill:#555555;\" transform=\"translate(269.93679 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 333.990767 224.64 \nL 333.990767 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"333.990767\" xlink:href=\"#m12d93e4cb2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g style=\"fill:#555555;\" transform=\"translate(330.809517 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 214.765338 \nL 379.645313 214.765338 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb73d893862\" style=\"stroke:#555555;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"214.765338\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- −1.00 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 218.564557)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 190.054004 \nL 379.645313 190.054004 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"190.054004\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- −0.75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 193.853223)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 165.342669 \nL 379.645313 165.342669 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"165.342669\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −0.50 -->\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 169.141888)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 140.631335 \nL 379.645313 140.631335 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"140.631335\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- −0.25 -->\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 144.430553)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 115.92 \nL 379.645313 115.92 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"115.92\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.00 -->\n      <g style=\"fill:#555555;\" transform=\"translate(15.579688 119.719219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 91.208665 \nL 379.645313 91.208665 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"91.208665\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.25 -->\n      <g style=\"fill:#555555;\" transform=\"translate(15.579688 95.007884)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 66.497331 \nL 379.645313 66.497331 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"66.497331\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.50 -->\n      <g style=\"fill:#555555;\" transform=\"translate(15.579688 70.29655)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 41.785996 \nL 379.645313 41.785996 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"41.785996\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.75 -->\n      <g style=\"fill:#555555;\" transform=\"translate(15.579688 45.585215)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p7e53b31601)\" d=\"M 44.845313 17.074662 \nL 379.645313 17.074662 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#mb73d893862\" y=\"17.074662\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.00 -->\n      <g style=\"fill:#555555;\" transform=\"translate(15.579688 20.87388)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_29\">\n    <path clip-path=\"url(#p7e53b31601)\" d=\"M 60.063494 115.258443 \nL 63.137874 115.188648 \nL 66.212255 115.11155 \nL 69.286635 115.026398 \nL 72.361015 114.932368 \nL 75.435395 114.828554 \nL 78.509775 114.713962 \nL 81.584155 114.587503 \nL 84.658536 114.447984 \nL 87.732916 114.2941 \nL 90.807296 114.124426 \nL 93.881676 113.937406 \nL 96.956056 113.731346 \nL 100.030436 113.504404 \nL 103.104817 113.254579 \nL 106.179197 112.979705 \nL 109.253577 112.677439 \nL 112.327957 112.345258 \nL 115.402337 111.98045 \nL 118.476717 111.580108 \nL 121.551098 111.141132 \nL 124.625478 110.660225 \nL 127.699858 110.1339 \nL 130.774238 109.558488 \nL 133.848618 108.930151 \nL 136.922998 108.244902 \nL 139.997379 107.498634 \nL 143.071759 106.687153 \nL 146.146139 105.806224 \nL 149.220519 104.851624 \nL 152.294899 103.819211 \nL 155.369279 102.704997 \nL 158.44366 101.505238 \nL 161.51804 100.21653 \nL 164.59242 98.835921 \nL 167.6668 97.361028 \nL 170.74118 95.790153 \nL 173.81556 94.122416 \nL 176.889941 92.357867 \nL 179.964321 90.497606 \nL 183.038701 88.543884 \nL 186.113081 86.500179 \nL 189.187461 84.371254 \nL 192.261841 82.163179 \nL 195.336222 79.883318 \nL 198.410602 77.540274 \nL 201.484982 75.14379 \nL 204.559362 72.704616 \nL 207.633742 70.234326 \nL 210.708122 67.745113 \nL 213.782503 65.249549 \nL 216.856883 62.760336 \nL 219.931263 60.290046 \nL 223.005643 57.850871 \nL 226.080023 55.454388 \nL 229.154403 53.111343 \nL 232.228784 50.831482 \nL 235.303164 48.623408 \nL 238.377544 46.494483 \nL 241.451924 44.450777 \nL 244.526304 42.497055 \nL 247.600684 40.636795 \nL 250.675065 38.872246 \nL 253.749445 37.204508 \nL 256.823825 35.633634 \nL 259.898205 34.15874 \nL 262.972585 32.778132 \nL 266.046965 31.489424 \nL 269.121346 30.289664 \nL 272.195726 29.17545 \nL 275.270106 28.143037 \nL 278.344486 27.188438 \nL 281.418866 26.307509 \nL 284.493246 25.496028 \nL 287.567627 24.74976 \nL 290.642007 24.064511 \nL 293.716387 23.436174 \nL 296.790767 22.860762 \nL 299.865147 22.334437 \nL 302.939527 21.85353 \nL 306.013908 21.414553 \nL 309.088288 21.014212 \nL 312.162668 20.649403 \nL 315.237048 20.317223 \nL 318.311428 20.014957 \nL 321.385808 19.740083 \nL 324.460189 19.490258 \nL 327.534569 19.263315 \nL 330.608949 19.057256 \nL 333.683329 18.870236 \nL 336.757709 18.700562 \nL 339.832089 18.546678 \nL 342.90647 18.407159 \nL 345.98085 18.2807 \nL 349.05523 18.166108 \nL 352.12961 18.062294 \nL 355.20399 17.968264 \nL 358.27837 17.883112 \nL 361.352751 17.806014 \nL 364.427131 17.736219 \n\" style=\"fill:none;stroke:#e24a33;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#p7e53b31601)\" d=\"M 60.063494 214.756364 \nL 63.137874 214.754355 \nL 66.212255 214.751896 \nL 69.286635 214.748887 \nL 72.361015 214.745204 \nL 75.435395 214.740697 \nL 78.509775 214.735181 \nL 81.584155 214.728431 \nL 84.658536 214.72017 \nL 87.732916 214.710061 \nL 90.807296 214.69769 \nL 93.881676 214.682552 \nL 96.956056 214.664028 \nL 100.030436 214.641362 \nL 103.104817 214.613628 \nL 106.179197 214.579696 \nL 109.253577 214.538183 \nL 112.327957 214.487401 \nL 115.402337 214.425286 \nL 118.476717 214.349317 \nL 121.551098 214.256421 \nL 124.625478 214.142847 \nL 127.699858 214.004025 \nL 130.774238 213.834391 \nL 133.848618 213.627177 \nL 136.922998 213.374166 \nL 139.997379 213.065399 \nL 143.071759 212.688826 \nL 146.146139 212.229914 \nL 149.220519 211.671185 \nL 152.294899 210.991712 \nL 155.369279 210.166552 \nL 158.44366 209.166169 \nL 161.51804 207.955844 \nL 164.59242 206.495158 \nL 167.6668 204.737605 \nL 170.74118 202.63047 \nL 173.81556 200.115128 \nL 176.889941 197.127957 \nL 179.964321 193.602127 \nL 183.038701 189.470479 \nL 186.113081 184.669735 \nL 189.187461 179.146106 \nL 192.261841 172.862163 \nL 195.336222 165.804514 \nL 198.410602 157.991363 \nL 201.484982 149.478698 \nL 204.559362 140.363559 \nL 207.633742 130.783004 \nL 210.708122 120.907949 \nL 213.782503 110.932051 \nL 216.856883 101.056996 \nL 219.931263 91.476441 \nL 223.005643 82.361302 \nL 226.080023 73.848637 \nL 229.154403 66.035486 \nL 232.228784 58.977837 \nL 235.303164 52.693894 \nL 238.377544 47.170265 \nL 241.451924 42.369521 \nL 244.526304 38.237873 \nL 247.600684 34.712043 \nL 250.675065 31.724872 \nL 253.749445 29.20953 \nL 256.823825 27.102395 \nL 259.898205 25.344842 \nL 262.972585 23.884156 \nL 266.046965 22.673831 \nL 269.121346 21.673448 \nL 272.195726 20.848288 \nL 275.270106 20.168815 \nL 278.344486 19.610086 \nL 281.418866 19.151174 \nL 284.493246 18.774601 \nL 287.567627 18.465834 \nL 290.642007 18.212823 \nL 293.716387 18.005609 \nL 296.790767 17.835975 \nL 299.865147 17.697153 \nL 302.939527 17.583579 \nL 306.013908 17.490683 \nL 309.088288 17.414714 \nL 312.162668 17.352599 \nL 315.237048 17.301817 \nL 318.311428 17.260304 \nL 321.385808 17.226372 \nL 324.460189 17.198638 \nL 327.534569 17.175972 \nL 330.608949 17.157448 \nL 333.683329 17.14231 \nL 336.757709 17.129939 \nL 339.832089 17.11983 \nL 342.90647 17.111569 \nL 345.98085 17.104819 \nL 349.05523 17.099303 \nL 352.12961 17.094796 \nL 355.20399 17.091113 \nL 358.27837 17.088104 \nL 361.352751 17.085645 \nL 364.427131 17.083636 \n\" style=\"fill:none;stroke:#348abd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 44.845313 224.64 \nL 44.845313 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 379.645313 224.64 \nL 379.645313 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 44.845313 224.64 \nL 379.645313 224.64 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 44.845313 7.2 \nL 379.645313 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7e53b31601\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"44.845313\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y739B4Tuueut",
        "colab_type": "text"
      },
      "source": [
        "Let's refer to the definition of $\\text{tanh}$ to derive an expression for its derivative (no pun intended).\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{d}{dx} \\text{tanh}(x) \n",
        "&= \\frac{d}{dx} \\left[ \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\right] \\\\\n",
        "&= 1 - \\left(\\frac{e^x - e^{-x}}{e^x + e^{-x}}\\right)^2 \\\\\n",
        "&= 1 - \\text{tanh}^2(x)\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QXqqivAvszd",
        "colab_type": "text"
      },
      "source": [
        "Now comes the more complicated part. Thankfully, we've already done something very similar in the past when we were building a vanilla neural network from scratch. For instance, we know from [this post](https://jaketae.github.io/study/neural-net/#data-generation) that, given a cross entropy loss function, the gradient of the softmax layer can simply be calculated by subtracting the predicted probabiity from the true distribution. In other words, given an intermediate variabe\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "z = W_y h_t + b_y \\tag{8} \\\\\n",
        "\\hat{y} = \\text{softmax}(z) \\tag{7}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "we know that \n",
        "\n",
        "$$\n",
        "\\frac{dL}{dz} = \\hat{y} - y \n",
        "$$\n",
        "\n",
        "Note that $\\hat{y}$ is denoted as `prob` in the code segment.\n",
        "\n",
        "Since we have this information, now it's just a matter of back propagating the gradients to the lower segments of the acyclic graph that defines the neural network. Given (8), it only makes sense to continue with the next parameter, $W_y$. \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{d W_y} \n",
        "&= \\frac{dL}{dz} \\cdot \\frac{dz}{d W_y} \\\\\n",
        "&= {h_t}^\\top (\\hat{y} - y)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The transpose or the order in which the terms are multiplied may be confusing, but with just some scratch work on paper, it isn't difficult to verify these gradients by checking their dimensions. \n",
        "\n",
        "The equation for $b_y$ is even simpler, since there is no matrix multiplication involved. Thus, the gradient flows backwards without any modification.\n",
        "\n",
        "$$\n",
        "\\frac{dL}{d b_y} = \\frac{dL}{dz} = \\hat{y} - y\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8ivnk2SX_vl",
        "colab_type": "text"
      },
      "source": [
        "Moving down a layer, we come across (6):\n",
        "\n",
        "$$\n",
        "h_t = o_t \\odot \\text{tanh}(C_t) \\tag{6}\n",
        "$$\n",
        "\n",
        "Let's begin by trying to find the gradient for $h$. \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{dh_t} \n",
        "&= \\frac{dL}{dz} \\cdot \\frac{dz}{dh_t} + \\frac{dL}{d h_t} \\\\\n",
        "&= (\\hat{y} - y) {W_y}^\\top + \\frac{dL}{d h_t}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "You might be wondering what the $d h_t$ term is doing in that equation. After all, isn't that quantity precisely what we are tryihg to calculate? This is the one tricky yet also interesting part about RNN backpropagation. Recall that the whole point of a recurrent neural network is its use of variables from the previous forward pass. For example, we know that in the next forward pass, $h_t$ will be concatenated with the input $x_{t + 1}$. In the backpropagation step corresponding to that forward pass, we would have computed $d h_t$; thus, this gradient flows into the current backpropagation as well. \n",
        "\n",
        "Although this diagram applies to a standard RNN instead of an LSTM, the recurrent nature of backprop still stands. I present it here becasue I find this diagram to be very intuitive.\n",
        "\n",
        "![png](https://i.imgur.com/hEtvXnN.png)\n",
        "\n",
        "If you look at the right, the star represents the gradient from the last pass. If you look to the left, you will see that there is going to be a gradient for $h_{t - 2}$ that will eventually be passed over to the next backpropgation scheme. Since the forward pass is recurrent, so is the backward pass. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4_R1vSMmI_j",
        "colab_type": "text"
      },
      "source": [
        "Since we have $d h_t$, now it's time to move further. Let's derive the expression for the gradient for $o_t$. \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "h_t = o_t \\odot \\text{tanh}(C_t) \\tag{6} \\implies \\\\\n",
        "\\frac{dL}{d o_t} \n",
        "&= \\frac{dL}{d h_t} \\cdot \\frac{d h_t}{d o_t} \\\\\n",
        "&= \\frac{dL}{d h_t} \\cdot \\text{tanh}(C_t)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Let's do the same for the other term, $\\text{tanh}(C_t)$. To make things easier, let's make a quick substitution with an intermediate variable, i.e. let $a_C = \\text{tanh}(C_t)$. Then, \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{d a_C} \n",
        "&= \\frac{dL}{dh_t} \\cdot \\frac{dh}{d a_C} \\\\\n",
        "&= \\frac{dL}{dh_t} \\cdot o_t\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "But $a_C$ was just an intermediate variable. How can we get the gradient for $C_t$ itself? Well, since the only transformation was just a $\\text{tanh}$, chain rule tells us that all we need is to multiply the antiderivative of $\\text{tanh}$, which we already derived above. Also keep in mind that since $C$ is a recurrent variable, we have to apply the gradient from the next call as well, just like $h$. \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{d C_t} \n",
        "&= \\frac{dL}{d a_C} \\cdot \\frac{d a_C}{d C_t} + \\frac{dL}{d C_t} \\\\\n",
        "&= \\frac{dL}{dh_t} \\cdot o_t \\cdot (1 - \\text{tanh}^2(C_t)) + \\frac{dL}{d Ct}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Note that all we had to do is to multiply the `d_tangent()` function we derived above, then add the backpropgation from the next iteration to account for the recurrent nature of the network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPhNBT_Ekg7C",
        "colab_type": "text"
      },
      "source": [
        "We still have a decent amount of work to do, but the fortunate news is that once we derive an expression for one parameter, the rest can also be obtained in an identical fashion. Therefore, for the sake of demonstration, we will only deal with $W_o$ and $b_o$. \n",
        "\n",
        "Let's start with the easier of the two, $b_o$. Recall that \n",
        "\n",
        "$$\n",
        "o_t = \\sigma(W_o [h_{t - 1}, x_t] + b_o) \\tag{5}\n",
        "$$\n",
        "\n",
        "As we have done earlier, let's introduce an intermediate variable, $a_o = W_o [h_{t - 1}, x_t] + b_o$, and try deriving the gradient for that variable. Note that with this substitution, $o_t = \\sigma(a_o)$. \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{d a_o} \n",
        "&= \\frac{dL}{d o_t} \\cdot \\frac{d o_t}{d a_o} \\\\\n",
        "&= \\frac{dL}{d h_t} \\cdot \\text{tanh}(C_t) \\cdot \\sigma(o_t) (1 - \\sigma(o_t))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Now we can move onto deriving the expressions for the gradient of the actual parameters, starting with $b_o$. This is extremely simple since $a_o$ and $b_o$ are defined by a linear relationship. \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{d b_o} \n",
        "&= \\frac{dL}{d a_o} \\cdot \\frac{d a_o}{d b_o} \\\\\n",
        "&= \\frac{dL}{d a_o}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The next in line is $W_o$. This is also very simple, since all we need to do is to consider one instance of matrix multiplication. \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{d W_o} \n",
        "&= \\frac{dL}{d a_o} \\cdot \\frac{d a_o}{d W_o} \\\\\n",
        "&= X^\\top \\frac{dL}{d a_o}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where, given a concatenation operator $\\vert$,\n",
        "\n",
        "$$\n",
        "X = (x_t \\vert h_{t - 1})\n",
        "$$\n",
        "\n",
        "Now we are done! The gradient for the rest of the parameters, such as $d W_f$ or $d b_f$ look almost exactly the same as $d W_o$ and $d b_o$ respectively, and not without reason: as we have noted above, what I conveniently called the filter-and-raw-material structure of LSTM gates remain consistent across the forget, input, and output gates. Therefore, we can apply the same chain rule to arrive at the same expressions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaDJi8gEeNyj",
        "colab_type": "text"
      },
      "source": [
        "However, there is one more caveat that requires our last bit of attention, and that is the gradient for $h_{t - 1}$. Note that $h_{t - 1}$ had been concatenated to the input in the form of $[x_t, h_{t - 1}]$ throughout the forward pass. Because this was a variable that was used during computation, we need to calculate its gradient as well. This might appear rather confusing since we are currently looking at time $t$, and it seems as if the gradient for $t - 1$ variables should be happeneing in the next iteration of backpropagation. While this is certainly true for the most part, due to the recurrent nature of LSTMs, we need to compute these gradients for $h_{t - 1}$ in this step as well. This is precisely what we were talking about earlier when discussing the recurrent nature of backprop; the $d h_{t - 1}$ we compute here will be used in the next iteration of backpropagation, just like we added $d h_t$ in the current backprop to calculate $d h_t$. \n",
        "\n",
        "Becaue $h_{t - 1}$ was used in many different places during the forward pass, we need to collect the gradients. Given an intermediate variable\n",
        "\n",
        "$$\n",
        "m_t = W_a X + b_a\n",
        "$$\n",
        "\n",
        "we can express the gradient in the following fashion:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{dX} \n",
        "&= \\sum_{a \\in \\{f, i, c, o \\}} {\\frac{dL}{dX}}_a \\\\\n",
        "&= \\sum_{a \\in \\{f, i, c, o \\}} \\frac{dL}{d a_t} \\cdot \\frac{d a_t}{d m_t} \\cdot \\frac{d m_t}{d X} \\\\\n",
        "&= \\sum_{a \\in \\{f, i, c, o \\}} \\frac{dL}{d a_t} \\cdot \\frac{d a_t}{d m_t} \\cdot {W_a}^\\top\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Then, we can obtain $d h_{t - 1}$ by un-concatenation:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{d h_{t - 1}} = {\\left(\\frac{dL}{dX}_{i, j} \\right)}_{\\substack{1 \\le j \\le H}}\n",
        "$$\n",
        "\n",
        "where $H$ denotes the number of neurons in the LSTM layer. \n",
        "\n",
        "We can do the same for $C_{t - 1}$. This is a lot simpler:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{d C_{t - 1}} \n",
        "&= \\frac{dL}{dC} \\cdot \\frac{dC}{d C_{t - 1}} \\\\\n",
        "&= \\frac{dL}{d C_t} \\cdot f_t\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "These gradients, of course, will be passed onto the next iteration of backpropagation, just like we had assumed that the values of $d C_t$ and $d h_t$ were given from the previous sequence of backpropagation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWf6GS4pvQvY",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Because DL libraries make it extremely easy to declare and train LSTM networks, it's often easy to gloss over what actually happens under the hood. However, there is certainly merit to dissecting and trying to understand the innerworking of DL models like LSTM cells, which offer a fascinating way of understanding the notion of memory. This is also important since RNNs are the basis of other more complicated models such as attention-based models or transformers, which is arguably the hottest topic these days in the field of NLP with the introduction of GPT-3 by OpenAI. \n",
        "\n",
        "I hope you have enjoyed reading this post. Catch you up in the next one!"
      ]
    }
  ]
}