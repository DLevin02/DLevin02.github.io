{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous post, we discussed how we can use tf-idf vectorization to encode documents into vectors. While probing more into this topic and geting a taste of what NLP is like, I decided to take a jab at another closely related, classic topic in NLP: word2vec. word2vec is a technique introduced by Google engineers in 2013, popularized by statements such as \"king - man + woman = queen.\" The gist of it, as you may know, is that we can express words as vectors that encode their semantics in a meaningful way. \n",
    "\n",
    "When I was just getting starting to learn TensorFlow, I came across the embedding layer, which performed exactly this operation: transforming words into vectors. While I thought this process was extremely interesting, I didn't know about the internals of this structure until today, particularly after reading this [wonderful tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) by Chris McCornick. In this post, we will be implementing word2vec, a popular embedding technique, from scratch with NumPy. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data\n",
    "\n",
    "Instead of going over the concepts and implementations separately, let's jump straight into the whole implementation process and elaborate on what is necessary along the way. \n",
    "\n",
    "In order to create word embeddings, we need some sort of data. Here is a text on machine learning from [Wikipedia](https://en.wikipedia.org/wiki/Machine_learning). I've removed some parentheses and citation brackets to make thingss slightly easier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Since we can't feed raw string texts into our model, we will need to preprocess this text. The first step, as is the approach taken in many NLP tasks, is to tokenize the text, *i.e.* splitting the text up into smaller units like words, getting rid of punctuations, and so on. Here is a function that does this trick using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create tokens using the Wikipedia excerpt shown above. The returned object will be a list containing all the tokens in `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful operation is to create a map between tokens and indices, and vice versa. In a sense, we are creating a lookup table that allows us to easily convert from words to indices, and indices to words. This will be particularly useful later on when we perform operations such as one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "    \n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the word-to-index and index-to-word maps have successfully been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 0,\n",
       " 'wide': 1,\n",
       " 'variety': 2,\n",
       " 'build': 3,\n",
       " 'improve': 4,\n",
       " 'computer': 5,\n",
       " 'a': 6,\n",
       " 'make': 7,\n",
       " 'decisions': 8,\n",
       " 'difficult': 9,\n",
       " 'on': 10,\n",
       " 'applications': 11,\n",
       " 'based': 12,\n",
       " 'filtering': 13,\n",
       " 'explicitly': 14,\n",
       " 'email': 15,\n",
       " 'study': 16,\n",
       " 'without': 17,\n",
       " 'learning': 18,\n",
       " 'of': 19,\n",
       " 'vision': 20,\n",
       " 'perform': 21,\n",
       " 'machine': 22,\n",
       " 'known': 23,\n",
       " 'or': 24,\n",
       " 'automatically': 25,\n",
       " 'so': 26,\n",
       " 'seen': 27,\n",
       " 'training': 28,\n",
       " 'sample': 29,\n",
       " 'artificial': 30,\n",
       " 'in': 31,\n",
       " 'to': 32,\n",
       " 'the': 33,\n",
       " 'being': 34,\n",
       " 'where': 35,\n",
       " 'tasks': 36,\n",
       " 'conventional': 37,\n",
       " 'do': 38,\n",
       " 'predictions': 39,\n",
       " 'such': 40,\n",
       " 'mathematical': 41,\n",
       " 'model': 42,\n",
       " 'used': 43,\n",
       " 'and': 44,\n",
       " 'through': 45,\n",
       " 'programmed': 46,\n",
       " 'develop': 47,\n",
       " 'are': 48,\n",
       " 'needed': 49,\n",
       " 'data': 50,\n",
       " 'subset': 51,\n",
       " 'order': 52,\n",
       " 'as': 53,\n",
       " 'intelligence': 54,\n",
       " 'that': 55,\n",
       " 'algorithms': 56,\n",
       " 'is': 57,\n",
       " 'experience': 58,\n",
       " 'infeasible': 59}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the lookup table is a dictionary object containing the relationship between words and ids. Note that each entry in this lookup table is a token created using the `tokenize()` function we defined eariler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training Data\n",
    "\n",
    "Now that we have tokenized the text and created lookup tables, we can now proceed to generating the actual training data, which are going to take the form of matrices. Since tokens are still in the form of strings, we need to encode them numerically using one-hot vectorization. We also need to generate a bunddle of input and target values, as this is a supervised learning technique.\n",
    "\n",
    "This then begs the question of what the input and target values are going to look like. What is the value that we are trying to approximate, and what sort of input will we be feeding into the model to generate predictions? The answer to these questions and how they tie into word2vec is at the heart of understanding word embeddings---as you may be able to tell, word2vec is not some sort of blackbox magic, but a result of careful training with input and output values, just like any other machine learning task.\n",
    "\n",
    "So here comes the crux of word2vec: we loop through each word (or token) in the sentence. In each loop, we look at words to the left and right of the input word, as shown below. This illustration was taken from [this article](https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484) by Ramzi Karam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*Mmp1vbFOxrmiCF17lYJWRA.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the particular example as shown above, we would generate the following input and prediction pairs part of the training data.\n",
    "\n",
    "```python\n",
    "[\"back-alleys\", \"little\"]\n",
    "[\"back-alleys\", \"dark\"]\n",
    "[\"back-alleys\", \"behind\"]\n",
    "[\"back-alleys\", \"the\"]\n",
    "```\n",
    "\n",
    "Note that the window size is two, which is why we look up to two words to the left and right of the input word. So in a way, we can understand this as forcing the model to understand a rough sense of context---the ability to see which words tend to stick together. In our own example, for instance, we would see a lot of `[\"machine\", \"learning\"]`, meaning that the model should be able to caputre the close contextual affinity between these two words.\n",
    "\n",
    "Below is the code that generates training data using the algorithm described above. We basically iterate over the tokenized data and generate pairs. One technicality here is that, for the first and last few tokens, it may not be possible to obtain words to the left or right of that input token. In those cases, we simply don't consider these word pairs and look at only what is feasible without causing `IndexError`s. Also note that we create `X` and `y` separately instead of putting them in tuple form as demonstrated above. This is just for convenience with other matrix operations later on in the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        idx = concat(\n",
    "            range(max(0, i - window), i), \n",
    "            range(i, min(n_tokens, i + window + 1))\n",
    "        )\n",
    "        for j in idx:\n",
    "            if i == j:\n",
    "                continue\n",
    "            X.append(one_hot_encode(word_to_id[tokens[i]], len(word_to_id)))\n",
    "            y.append(one_hot_encode(word_to_id[tokens[j]], len(word_to_id)))\n",
    "    \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the definition for `concat`, an auxiliary function we used above to combine two `range()` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, here is the code we use to one-hot vectorize tokens. This process is necessary in order to represent each token as a vector, which can then be stacked to create the matrices `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(id, vocab_size):\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate some training data with a window size of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_training_data(tokens, word_to_id, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the dimensionality of the data to get a sense of what matrices we are working with. This intuition will become important in particular when training and writing equations for backpropagation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 60)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `X` and `y` are matrices with 330 rows and 60 columns. Here, 330 is the number of training examples we have. We would expect this number to have been larger had we used a larger window. 60 is the size of our corpus, or the number of unique tokens we have in the original text. Since we have one-hot encoded both the input and output as 60-dimensional sparse vectors, this is expected.\n",
    "\n",
    "Now, we are finally ready to build and train our embedding network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Embedding Model\n",
    "\n",
    "At this point, you might be wondering how it is that training a neural network that predicts some nearby context word given an input token can be used to embed words into vectors. After all, the output of the network is going to be some probability vector that passed through a softmax layer, not an embedding vector. \n",
    "\n",
    "This is entirely correct, and this is a question that came to my mind as well. However, this is the part that gets the most interesting: the rows of the intermediate weight matrix is the embedding we are looking for! This becomes much more apparent once we consider the dimensions of the weight matrices that compose the model. For simplicity purposes, say we have a total of 5 words in the corpus, and that we want to embed these words as three-dimensional vectors. \n",
    "\n",
    "More specifically, here is the first weight layer of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\underbrace{\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 & 0 & 0 \\\\ \n",
    "1 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\n",
    "\\end{pmatrix}\n",
    "}_{\\text{input}}\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "\\begin{pmatrix}\n",
    "2 & 1 & 7 \\\\ \n",
    "1 & 8 & 6 \\\\ \n",
    "9 & 0 & 4 \\\\ \n",
    "7 & 5 & 5 \\\\ \n",
    "5 & 1 & 2 \\\\ \n",
    "\\end{pmatrix}\n",
    "}_{\\text{weight}}\n",
    "= \n",
    "\\underbrace{\n",
    "\\begin{pmatrix}\n",
    "1 & 8 & 6 \\\\ \n",
    "2 & 1 & 7 \\\\ \n",
    "7 & 5 & 5 \\\\ \n",
    "\\vdots & \\vdots & \\vdots\n",
    "\\end{pmatrix}\n",
    "}_{\\text{embedding}} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial observation to make is that, because the input is a sparse vector containing one-hot encoded vectors, the weight matrix effectively acts as a lookup table that moves one-hot encoded vectors to dense vectors in a different dimension---more precisely, the row space of the weight matrix. In this particular example, the weight matrix was a transformation of $\\mathbb{R}^5 \\to \\mathbb{R}^3$. This is exactly what we want to achieve with embedding: representing words as dense vectors, a step-up from simple one-hot encoding. This proess is exactly what embedding is: as we start training this model with the training data generated above, we would expect the row space of this weight matrix to encode meaningful semantic information from the training data. \n",
    "\n",
    "Continuing onwards, here is the second layer that receives as input the embeddings, then uses them to generate a set of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\underbrace{\n",
    "\\begin{pmatrix}\n",
    "1 & 8 & 6 \\\\ \n",
    "2 & 1 & 7 \\\\ \n",
    "7 & 5 & 5 \\\\ \n",
    "\\vdots & \\vdots & \\vdots\n",
    "\\end{pmatrix}\n",
    "}_{\\text{embedding}}\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "\\begin{pmatrix}\n",
    "8 & 4 & 5 & 1 & 8 \\\\ \n",
    "1 & 6 & 2 & 5 & 7 \\\\\n",
    "0 & 2 & 0 & 3 & 4 \\\\\n",
    "\\end{pmatrix}\n",
    "}_{\\text{weight}}\n",
    "=\n",
    "\\underbrace{\n",
    "\\begin{pmatrix}\n",
    "16 & 64 & 21 & 59 & 84 \\\\ \n",
    "17 & 28 & 12 & 26 & 51 \\\\\n",
    "61 & 68 & 45 & 47 & 111 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\n",
    "\\end{pmatrix}\n",
    "}_{\\text{output}} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost done. All we now need in the last layer is a softmax layer. When the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. This final output can be considered as context predictions, *i.e.* which words are likely to be in the window vicinity of the input word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{softmax}\n",
    "\\left (\n",
    "\\underbrace{\n",
    "\\begin{pmatrix}\n",
    "16 & 64 & 21 & 59 & 84 \\\\ \n",
    "17 & 28 & 12 & 26 & 51 \\\\\n",
    "61 & 68 & 45 & 47 & 111 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\n",
    "\\end{pmatrix}\n",
    "}_{\\text{output}}\n",
    "\\right )\n",
    "=\n",
    "\\underbrace{\n",
    "\\begin{pmatrix}\n",
    "0.1 & 0.2 & 0.1 & 0.2 & 0.4 \\\\ \n",
    "0.2 & 0.2 & 0.1 & 0.2 & 0.3 \\\\\n",
    "0.2 & 0.2 & 0.1 & 0.1 & 0.4 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\n",
    "\\end{pmatrix}\n",
    "}_{\\text{prediction}} \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training---specifically error calculation and backpropagation---we would be comparing this prediction of probability vectors with its true one-hot encoded targets. The error function that we use with softmax is cross entropy, defined as\n",
    "\n",
    "$$\n",
    "H(p, q) = - \\sum_{x \\in \\chi} p(x) \\log q(x) \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like to think of this as a dot product of the target vector and the log of the prediction, because that is essentially what the summation is doing. In this alternate formulation, the cross entropy formula can be rewritten as\n",
    "\n",
    "$$\n",
    "H(p, q) = - p \\cdot \\log(q) \\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $p$ a one-hot encoded vector in this case, all the elements in $p$ whose entry is zero will have no effect on the final outcome. Indeed, we simply end up taking the negative log of the prediction. Notice that the closer the value of the prediction is to 1, the smaller the cross entropy, and vice versa. This aligns with the behavior we want, since we want the predicted probability to be as close to 1 as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's summarize the entire process a little bit. First, embeddings are simply the rows of the first weight matrix, denoted ass $W_1$. Through training and backpropgation, we adjust the weights of $W_1$, along with the weight matrix in the second layer, denoted as $W_2$, using cross entropy loss. Overall, our model takes on the following structure:\n",
    "\n",
    "$$\n",
    "A_1 = X W_1 \\\\\n",
    "A_2 = A_1 W_2 \\\\\n",
    "Z = \\text{softmax}(A_2) \\tag{6}\n",
    "$$\n",
    "\n",
    "where $Z$ is the matrix contains the prediction probability vectors.\n",
    "\n",
    "With this in mind, let's actual start building and train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation\n",
    "\n",
    "Let's start implement this model in code. The implementation we took here is extremely similar to the approach we took in [this post](https://jaketae.github.io/study/neural-net/). For an in-depth review of backpropagation derivation with matrix calculus, I highly recommend that you check out the linked post. \n",
    "\n",
    "The representation we will use for the model is a Python dictionary, whose values are the weight matrices and keys, the name with which we will refer to the weight matrices. In accordance with the nomenclature established earlier, we stick with `\"w1\"` and `\"w2\"` to refer to these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(vocab_size, n_embedding):\n",
    "    model = {\n",
    "        \"w1\": np.random.randn(vocab_size, n_embedding),\n",
    "        \"w2\": np.random.randn(n_embedding, vocab_size)\n",
    "    }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify our model to create ten-dimensional embeddings. In other words, each token will be represented as vectors living in ten-dimensional space. Note that actual models tend to use much higher dimensions, most commonly 300, but for our purposes this is not necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_network(len(word_to_id), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Let's begin with forward propagation. Coding the forward propagation process simply amounts to transcribing the three matrix multiplication equations in (6) into NumPy code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, X, return_cache=True):\n",
    "    cache = {}\n",
    "    \n",
    "    cache[\"a1\"] = X @ model[\"w1\"]\n",
    "    cache[\"a2\"] = cache[\"a1\"] @ model[\"w2\"]\n",
    "    cache[\"z\"] = softmax(cache[\"a2\"])\n",
    "    \n",
    "    if not return_cache:\n",
    "        return cache[\"z\"]\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backpropagation, we will need all the intermediate variables, so we hold them in a dictionary called `cache`. However, if we simply want the final prediction vectors only, not the cache, we set `return_cache` to `False`. This is just a little auxilliary feature to make things slightly easier later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to implement the `softmax()` function we used above. Note that this function receives a matrix as input, not a vector, so we will need to slightly tune things up a bit using a simple loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    res = []\n",
    "    for x in X:\n",
    "        exp = np.exp(x)\n",
    "        res.append(exp / exp.sum())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are done with implementing the forward pass. However, before we move on, it's always a good idea to check the dimensionality of the matrices, as this will provide us with some useful intuition while coding backward propagation later on. \n",
    "\n",
    "The dimensionality of the matrix after passing the first layer, or the embedding layer, is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X @ model[\"w1\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is expected, since we want all the 330 tokens in the text to be converted into ten-dimensional vectors. \n",
    "\n",
    "Next, let's check the dimensionality after passing through the second layer. This time, it is a 330-by-60 matrix. This also makes sense, since we want the output to be sixty dimensional, back to the original dimensions following one-hot encoding. This result can then be passed onto the softmax layer, the result of which will be a bunch probability vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 60)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X @ model[\"w1\"] @ model[\"w2\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Implementing backward propagation is slightly more difficult than forward propagation. However, the good news is that we have already derived the equation for backpropagation given a softmax layer with cross entropy loss in [this post](https://jaketae.github.io/study/neural-net/), where we built a neural network from scratch. The conclusion of the lengthy derivation was ultimately that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial A_2} = Z - y\n",
    "$$\n",
    "\n",
    "given our model \n",
    "\n",
    "$$\n",
    "A_1 = X W_1 \\\\\n",
    "A_2 = A_1 W_2 \\\\\n",
    "Z = \\text{softmax}(A_2) \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the error, we can now backpropagate it throughout the entire network, recalling basic principles of matrix calculus. If backprop is still confusing to you due to all the tranposes going on, one pro-tip is to think in terms of dimensions. After all, the dimension of the gradient must equal to the dimension of the original matrix. With that in mind, let's implement the backpropagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(model, X, y, alpha):\n",
    "    cache  = forward(model, X)\n",
    "    da2 = cache[\"z\"] - y\n",
    "    dw2 = cache[\"a1\"].T @ da2\n",
    "    da1 = da2 @ model[\"w2\"].T\n",
    "    dw1 = X.T @ da1\n",
    "    assert(dw2.shape == model[\"w2\"].shape)\n",
    "    assert(dw1.shape == model[\"w1\"].shape)\n",
    "    model[\"w1\"] -= alpha * dw1\n",
    "    model[\"w2\"] -= alpha * dw2\n",
    "    return cross_entropy(cache[\"z\"], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep a log of the value of the error throughout the backpropagation process, I decided to make the final return value of `backward()` to be the cross entropy loss between the prediction and the target labels. The cross entropy loss function can easily be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(z, y):\n",
    "    return - np.sum(np.log(z) * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train and test the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "\n",
    "As we only have a smalll number of training data---coupled with the fact that the backpropagation algorithm is simple batch gradient descent---let's just iterate for 50 epochs. While training, we will be caching the value of the cross entropy error function in a `history` list. We can then plot this result to get a better sense of whether the training worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"329.525312pt\" version=\"1.1\" viewBox=\"0 0 490.04375 329.525312\" width=\"490.04375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 329.525312 \n",
       "L 490.04375 329.525312 \n",
       "L 490.04375 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 36.44375 306.18 \n",
       "L 482.84375 306.18 \n",
       "L 482.84375 7.2 \n",
       "L 36.44375 7.2 \n",
       "z\n",
       "\" style=\"fill:#eaeaf2;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 56.734659 306.18 \n",
       "L 56.734659 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\"/>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 4.15625 35.296875 \n",
       "Q 4.15625 48 6.765625 55.734375 \n",
       "Q 9.375 63.484375 14.515625 67.671875 \n",
       "Q 19.671875 71.875 27.484375 71.875 \n",
       "Q 33.25 71.875 37.59375 69.546875 \n",
       "Q 41.9375 67.234375 44.765625 62.859375 \n",
       "Q 47.609375 58.5 49.21875 52.21875 \n",
       "Q 50.828125 45.953125 50.828125 35.296875 \n",
       "Q 50.828125 22.703125 48.234375 14.96875 \n",
       "Q 45.65625 7.234375 40.5 3 \n",
       "Q 35.359375 -1.21875 27.484375 -1.21875 \n",
       "Q 17.140625 -1.21875 11.234375 6.203125 \n",
       "Q 4.15625 15.140625 4.15625 35.296875 \n",
       "z\n",
       "M 13.1875 35.296875 \n",
       "Q 13.1875 17.671875 17.3125 11.828125 \n",
       "Q 21.4375 6 27.484375 6 \n",
       "Q 33.546875 6 37.671875 11.859375 \n",
       "Q 41.796875 17.71875 41.796875 35.296875 \n",
       "Q 41.796875 52.984375 37.671875 58.78125 \n",
       "Q 33.546875 64.59375 27.390625 64.59375 \n",
       "Q 21.34375 64.59375 17.71875 59.46875 \n",
       "Q 13.1875 52.9375 13.1875 35.296875 \n",
       "z\n",
       "\" id=\"ArialMT-48\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(53.95419 320.337812)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 139.554696 306.18 \n",
       "L 139.554696 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\"/>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.25 0 \n",
       "L 28.46875 0 \n",
       "L 28.46875 56 \n",
       "Q 25.296875 52.984375 20.140625 49.953125 \n",
       "Q 14.984375 46.921875 10.890625 45.40625 \n",
       "L 10.890625 53.90625 \n",
       "Q 18.265625 57.375 23.78125 62.296875 \n",
       "Q 29.296875 67.234375 31.59375 71.875 \n",
       "L 37.25 71.875 \n",
       "z\n",
       "\" id=\"ArialMT-49\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(133.993759 320.337812)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 222.374733 306.18 \n",
       "L 222.374733 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\"/>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 50.34375 8.453125 \n",
       "L 50.34375 0 \n",
       "L 3.03125 0 \n",
       "Q 2.9375 3.171875 4.046875 6.109375 \n",
       "Q 5.859375 10.9375 9.828125 15.625 \n",
       "Q 13.8125 20.3125 21.34375 26.46875 \n",
       "Q 33.015625 36.03125 37.109375 41.625 \n",
       "Q 41.21875 47.21875 41.21875 52.203125 \n",
       "Q 41.21875 57.421875 37.46875 61 \n",
       "Q 33.734375 64.59375 27.734375 64.59375 \n",
       "Q 21.390625 64.59375 17.578125 60.78125 \n",
       "Q 13.765625 56.984375 13.71875 50.25 \n",
       "L 4.6875 51.171875 \n",
       "Q 5.609375 61.28125 11.65625 66.578125 \n",
       "Q 17.71875 71.875 27.9375 71.875 \n",
       "Q 38.234375 71.875 44.234375 66.15625 \n",
       "Q 50.25 60.453125 50.25 52 \n",
       "Q 50.25 47.703125 48.484375 43.546875 \n",
       "Q 46.734375 39.40625 42.65625 34.8125 \n",
       "Q 38.578125 30.21875 29.109375 22.21875 \n",
       "Q 21.1875 15.578125 18.9375 13.203125 \n",
       "Q 16.703125 10.84375 15.234375 8.453125 \n",
       "z\n",
       "\" id=\"ArialMT-50\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(216.813796 320.337812)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 305.19477 306.18 \n",
       "L 305.19477 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\"/>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 30 -->\n",
       "      <defs>\n",
       "       <path d=\"M 4.203125 18.890625 \n",
       "L 12.984375 20.0625 \n",
       "Q 14.5 12.59375 18.140625 9.296875 \n",
       "Q 21.78125 6 27 6 \n",
       "Q 33.203125 6 37.46875 10.296875 \n",
       "Q 41.75 14.59375 41.75 20.953125 \n",
       "Q 41.75 27 37.796875 30.921875 \n",
       "Q 33.84375 34.859375 27.734375 34.859375 \n",
       "Q 25.25 34.859375 21.53125 33.890625 \n",
       "L 22.515625 41.609375 \n",
       "Q 23.390625 41.5 23.921875 41.5 \n",
       "Q 29.546875 41.5 34.03125 44.421875 \n",
       "Q 38.53125 47.359375 38.53125 53.46875 \n",
       "Q 38.53125 58.296875 35.25 61.46875 \n",
       "Q 31.984375 64.65625 26.8125 64.65625 \n",
       "Q 21.6875 64.65625 18.265625 61.421875 \n",
       "Q 14.84375 58.203125 13.875 51.765625 \n",
       "L 5.078125 53.328125 \n",
       "Q 6.6875 62.15625 12.390625 67.015625 \n",
       "Q 18.109375 71.875 26.609375 71.875 \n",
       "Q 32.46875 71.875 37.390625 69.359375 \n",
       "Q 42.328125 66.84375 44.9375 62.5 \n",
       "Q 47.5625 58.15625 47.5625 53.265625 \n",
       "Q 47.5625 48.640625 45.0625 44.828125 \n",
       "Q 42.578125 41.015625 37.703125 38.765625 \n",
       "Q 44.046875 37.3125 47.5625 32.6875 \n",
       "Q 51.078125 28.078125 51.078125 21.140625 \n",
       "Q 51.078125 11.765625 44.234375 5.25 \n",
       "Q 37.40625 -1.265625 26.953125 -1.265625 \n",
       "Q 17.53125 -1.265625 11.296875 4.34375 \n",
       "Q 5.078125 9.96875 4.203125 18.890625 \n",
       "z\n",
       "\" id=\"ArialMT-51\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(299.633833 320.337812)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-51\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 388.014808 306.18 \n",
       "L 388.014808 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\"/>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 40 -->\n",
       "      <defs>\n",
       "       <path d=\"M 32.328125 0 \n",
       "L 32.328125 17.140625 \n",
       "L 1.265625 17.140625 \n",
       "L 1.265625 25.203125 \n",
       "L 33.9375 71.578125 \n",
       "L 41.109375 71.578125 \n",
       "L 41.109375 25.203125 \n",
       "L 50.78125 25.203125 \n",
       "L 50.78125 17.140625 \n",
       "L 41.109375 17.140625 \n",
       "L 41.109375 0 \n",
       "z\n",
       "M 32.328125 25.203125 \n",
       "L 32.328125 57.46875 \n",
       "L 9.90625 25.203125 \n",
       "z\n",
       "\" id=\"ArialMT-52\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(382.45387 320.337812)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-52\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 470.834845 306.18 \n",
       "L 470.834845 7.2 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\"/>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 50 -->\n",
       "      <defs>\n",
       "       <path d=\"M 4.15625 18.75 \n",
       "L 13.375 19.53125 \n",
       "Q 14.40625 12.796875 18.140625 9.390625 \n",
       "Q 21.875 6 27.15625 6 \n",
       "Q 33.5 6 37.890625 10.78125 \n",
       "Q 42.28125 15.578125 42.28125 23.484375 \n",
       "Q 42.28125 31 38.0625 35.34375 \n",
       "Q 33.84375 39.703125 27 39.703125 \n",
       "Q 22.75 39.703125 19.328125 37.765625 \n",
       "Q 15.921875 35.84375 13.96875 32.765625 \n",
       "L 5.71875 33.84375 \n",
       "L 12.640625 70.609375 \n",
       "L 48.25 70.609375 \n",
       "L 48.25 62.203125 \n",
       "L 19.671875 62.203125 \n",
       "L 15.828125 42.96875 \n",
       "Q 22.265625 47.46875 29.34375 47.46875 \n",
       "Q 38.71875 47.46875 45.15625 40.96875 \n",
       "Q 51.609375 34.46875 51.609375 24.265625 \n",
       "Q 51.609375 14.546875 45.953125 7.46875 \n",
       "Q 39.0625 -1.21875 27.15625 -1.21875 \n",
       "Q 17.390625 -1.21875 11.203125 4.25 \n",
       "Q 5.03125 9.71875 4.15625 18.75 \n",
       "z\n",
       "\" id=\"ArialMT-53\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(465.273907 320.337812)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 36.44375 280.654785 \n",
       "L 482.84375 280.654785 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\"/>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 750 -->\n",
       "      <defs>\n",
       "       <path d=\"M 4.734375 62.203125 \n",
       "L 4.734375 70.65625 \n",
       "L 51.078125 70.65625 \n",
       "L 51.078125 63.8125 \n",
       "Q 44.234375 56.546875 37.515625 44.484375 \n",
       "Q 30.8125 32.421875 27.15625 19.671875 \n",
       "Q 24.515625 10.6875 23.78125 0 \n",
       "L 14.75 0 \n",
       "Q 14.890625 8.453125 18.0625 20.40625 \n",
       "Q 21.234375 32.375 27.171875 43.484375 \n",
       "Q 33.109375 54.59375 39.796875 62.203125 \n",
       "z\n",
       "\" id=\"ArialMT-55\"/>\n",
       "      </defs>\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(12.760938 284.233691)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-55\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 36.44375 246.373935 \n",
       "L 482.84375 246.373935 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\"/>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1000 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 249.952841)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 36.44375 212.093085 \n",
       "L 482.84375 212.093085 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\"/>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1250 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 215.671991)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 36.44375 177.812235 \n",
       "L 482.84375 177.812235 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\"/>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1500 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 181.391141)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 36.44375 143.531385 \n",
       "L 482.84375 143.531385 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\"/>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1750 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 147.110291)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-49\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-55\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_23\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 36.44375 109.250535 \n",
       "L 482.84375 109.250535 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_24\"/>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 2000 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 112.829441)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_25\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 36.44375 74.969685 \n",
       "L 482.84375 74.969685 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_26\"/>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 2250 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 78.548591)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_27\">\n",
       "      <path clip-path=\"url(#pddf8c6462d)\" d=\"M 36.44375 40.688835 \n",
       "L 482.84375 40.688835 \n",
       "\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_28\"/>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 2500 -->\n",
       "      <g style=\"fill:#262626;\" transform=\"translate(7.2 44.267741)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#ArialMT-50\"/>\n",
       "       <use x=\"55.615234\" xlink:href=\"#ArialMT-53\"/>\n",
       "       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\n",
       "       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_29\">\n",
       "    <path clip-path=\"url(#pddf8c6462d)\" d=\"M 56.734659 20.79 \n",
       "L 65.016663 151.074433 \n",
       "L 73.298667 194.313376 \n",
       "L 81.58067 212.840574 \n",
       "L 89.862674 224.557585 \n",
       "L 98.144678 233.376374 \n",
       "L 106.426681 240.573494 \n",
       "L 114.708685 246.720904 \n",
       "L 122.990689 252.067989 \n",
       "L 131.272692 256.76215 \n",
       "L 139.554696 260.901143 \n",
       "L 147.8367 264.548694 \n",
       "L 156.118704 267.758534 \n",
       "L 164.400707 270.588489 \n",
       "L 172.682711 273.09713 \n",
       "L 180.964715 275.338338 \n",
       "L 189.246718 277.356739 \n",
       "L 197.528722 279.188349 \n",
       "L 205.810726 280.857355 \n",
       "L 214.09273 282.379502 \n",
       "L 222.374733 283.749603 \n",
       "L 230.656737 284.963655 \n",
       "L 238.938741 285.886931 \n",
       "L 247.220744 286.729234 \n",
       "L 255.502748 286.72914 \n",
       "L 263.784752 288.387773 \n",
       "L 272.066756 288.204643 \n",
       "L 280.348759 289.094666 \n",
       "L 288.630763 287.621195 \n",
       "L 296.912767 289.686565 \n",
       "L 305.19477 288.430916 \n",
       "L 313.476774 288.95566 \n",
       "L 321.758778 287.700136 \n",
       "L 330.040782 290.878703 \n",
       "L 338.322785 289.317622 \n",
       "L 346.604789 290.461112 \n",
       "L 354.886793 287.572814 \n",
       "L 363.168796 291.483789 \n",
       "L 371.4508 290.540741 \n",
       "L 379.732804 291.878303 \n",
       "L 388.014808 289.112104 \n",
       "L 396.296811 291.520992 \n",
       "L 404.578815 289.890698 \n",
       "L 412.860819 292.261828 \n",
       "L 421.142822 289.461968 \n",
       "L 429.424826 292.105522 \n",
       "L 437.70683 290.922041 \n",
       "L 445.988833 292.59 \n",
       "L 454.270837 289.770319 \n",
       "L 462.552841 292.568745 \n",
       "\" style=\"fill:none;stroke:#87ceeb;stroke-linecap:round;stroke-width:1.75;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 36.44375 306.18 \n",
       "L 36.44375 7.2 \n",
       "\" style=\"fill:none;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 482.84375 306.18 \n",
       "L 482.84375 7.2 \n",
       "\" style=\"fill:none;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 36.44375 306.18 \n",
       "L 482.84375 306.18 \n",
       "\" style=\"fill:none;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 36.44375 7.2 \n",
       "L 482.84375 7.2 \n",
       "\" style=\"fill:none;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pddf8c6462d\">\n",
       "   <rect height=\"298.98\" width=\"446.4\" x=\"36.44375\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "n_iter = 50\n",
    "learning_rate = 0.05\n",
    "\n",
    "history = [backward(model, X, y, learning_rate) for _ in range(n_iter)]\n",
    "\n",
    "plt.plot(range(len(history)), history, color=\"skyblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed it seems like we did well! We can thus say with some degree of confidence that the embedding layer has been trained as well. \n",
    "\n",
    "An obvious sanity check we can perform is to see which token our model predicts given the word \"learning.\" If the model was trained properly, the most likely word should understandably be \"machine.\" And indeed, when that is the result we get: notice that \"machine\" is at the top of the list of tokens, sorted by degree of affinity with \"learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine\n",
      "intelligence\n",
      "the\n",
      "is\n",
      "so\n",
      "build\n",
      "are\n",
      "computer\n",
      "perform\n",
      "it\n",
      "learning\n",
      "conventional\n",
      "a\n",
      "improve\n",
      "subset\n",
      "automatically\n",
      "model\n",
      "algorithms\n",
      "do\n",
      "based\n",
      "artificial\n",
      "through\n",
      "that\n",
      "known\n",
      "experience\n",
      "vision\n",
      "wide\n",
      "programmed\n",
      "data\n",
      "tasks\n",
      "infeasible\n",
      "develop\n",
      "applications\n",
      "used\n",
      "seen\n",
      "on\n",
      "explicitly\n",
      "of\n",
      "study\n",
      "predictions\n",
      "such\n",
      "filtering\n",
      "where\n",
      "needed\n",
      "decisions\n",
      "mathematical\n",
      "email\n",
      "variety\n",
      "or\n",
      "order\n",
      "training\n",
      "and\n",
      "being\n",
      "without\n",
      "in\n",
      "sample\n",
      "to\n",
      "make\n",
      "difficult\n",
      "as\n"
     ]
    }
   ],
   "source": [
    "learning = one_hot_encode(word_to_id[\"learning\"], len(word_to_id))\n",
    "result = forward(model, [learning], return_cache=False)[0]\n",
    "\n",
    "for word in (id_to_word[id] for id in np.argsort(result)[::-1]):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "Building and training was fun and all, but our end goal was not to build a neural network; we wanted to get word embeddings. As stated earlier in this post, the key behind word embeddings is that the rows of the first weight matrix is effectively a dense representation of one-hot encoded vectors each corresponding to various tokens in the text dataset. In our example, therefore, the embedding can simply be obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76943888,  0.66419101,  0.52185522,  0.53416045,  0.74682309,\n",
       "         1.29778774,  0.80174038, -0.42762792, -2.44509237,  0.69461262],\n",
       "       [-0.03049168,  0.72713564, -1.07892615, -2.12632703, -1.40504585,\n",
       "         0.16007463, -1.44169316,  0.06812903,  0.15341611, -2.16828595],\n",
       "       [-0.68684491, -1.53318024,  0.27274833, -2.04037677,  0.13802059,\n",
       "        -0.3005966 , -0.80421765, -0.31677644, -0.46332806, -1.34717872],\n",
       "       [ 1.0935606 ,  1.24492109, -0.035054  , -0.75192887,  0.15263928,\n",
       "        -1.26221765, -0.50342996, -2.77013745,  0.1399199 , -0.77001316],\n",
       "       [ 0.4748796 ,  0.71693722, -0.79135941,  2.60869716, -0.58760833,\n",
       "        -0.08669239, -0.01178457, -1.0893234 , -0.66961562, -0.7323576 ],\n",
       "       [-1.21903306, -0.9770747 ,  0.82938815,  1.66171912,  0.49097782,\n",
       "         1.70764463,  0.21741346, -1.27341364,  0.7001402 , -1.17027829],\n",
       "       [ 0.76302718, -1.45370483, -0.00798623, -1.54434253, -0.02672187,\n",
       "         1.73680874, -0.81259019, -1.41251393,  1.29134638,  0.4373011 ],\n",
       "       [-0.25147232,  2.07658396,  0.04017748,  1.47709408, -2.49219074,\n",
       "        -0.54824483,  0.34565281, -0.1765285 ,  1.63189504, -0.26635877],\n",
       "       [-1.07492131,  0.86480382, -0.33732251,  1.8539463 , -1.8351204 ,\n",
       "         0.32488649,  0.07781584, -0.79155451,  0.22268128,  1.45353606],\n",
       "       [ 0.96564502,  0.40326434,  0.39062086, -0.07369607, -2.08306135,\n",
       "        -0.24240214,  1.08098237,  0.62831061, -0.28851627,  1.88856504],\n",
       "       [ 0.67039387,  0.11089601, -0.06260896, -1.14201765,  0.85214818,\n",
       "         0.79699652,  1.60140494,  1.55860074,  1.48830133,  0.62505581],\n",
       "       [-1.7260562 ,  0.27865698, -0.02611865, -1.79222164, -0.26568484,\n",
       "         1.02098073,  0.13923429,  0.12880677,  1.14611787, -0.21745726],\n",
       "       [ 2.34618366,  0.09627182,  0.73429031, -0.3795417 ,  1.00384797,\n",
       "         2.29688517, -0.54519017, -0.15382528,  1.02290422,  0.67138609],\n",
       "       [-0.8199847 ,  0.81764305,  0.77442068,  0.41361639, -0.815798  ,\n",
       "         3.25822674, -0.1863631 , -0.04673113,  0.56506795, -0.02778286],\n",
       "       [-1.76193113,  0.18080273, -1.61259579,  1.13134183, -0.39194156,\n",
       "        -0.14657422, -1.13627271, -1.5667357 , -0.27150137,  0.74683337],\n",
       "       [-1.66879205,  0.43920843, -0.1663464 , -0.64556484,  1.15383694,\n",
       "         1.99026613,  1.29054758, -0.51452775,  0.36076408,  0.13917934],\n",
       "       [-0.26512035,  0.79767559,  1.80033249, -1.14609988, -0.78927055,\n",
       "        -0.43497097,  0.70769172,  1.83098455, -0.96178864, -1.30140586],\n",
       "       [-0.36679925, -0.08888034, -0.69805165,  1.01032096,  0.53327293,\n",
       "         0.71142215, -0.61919103,  0.91489913, -0.23415597,  3.07246695],\n",
       "       [ 1.54042926, -1.49268108,  0.06412402,  0.27555874, -0.5140873 ,\n",
       "        -1.00779784,  1.06187732,  0.64988687, -0.84064087, -0.62033715],\n",
       "       [-0.24097777, -0.53801309,  0.53763453, -0.5195818 , -0.52441574,\n",
       "         0.24175162,  1.33209536, -1.18518131, -0.1644457 , -0.98436401],\n",
       "       [ 0.09167709,  0.59763758,  2.780052  ,  0.42059586, -0.61414259,\n",
       "         0.53536983,  2.42227754,  0.13680908,  0.28311335,  1.31975233],\n",
       "       [ 0.78859384,  1.26704417,  1.3230788 ,  1.75579178, -0.45166329,\n",
       "        -1.27266035, -0.740434  , -1.21728119,  0.68298206, -1.15872753],\n",
       "       [ 1.03411644, -0.68148228, -0.49401597, -1.04198152, -1.96143292,\n",
       "        -0.68325082,  0.74263224, -0.46497574, -0.33083338,  0.29300595],\n",
       "       [-0.2745763 ,  0.56636408,  0.03976089, -1.05114299,  2.55634587,\n",
       "         1.13019543, -1.30645221,  0.84024957,  0.1047002 ,  0.73783628],\n",
       "       [-1.65721097,  0.95894771,  0.37856505,  0.19945248, -2.24372733,\n",
       "         0.06383841, -0.9127448 ,  0.46130851, -0.50089233,  0.36800566],\n",
       "       [-0.45980465,  1.25934841,  0.62539097,  3.30279986,  0.04255758,\n",
       "        -0.13354921,  0.79207247,  0.06972181, -0.69901224,  0.21201475],\n",
       "       [ 0.85073146,  0.08781509, -1.92623443, -0.16817443, -1.30259218,\n",
       "        -2.38615797, -0.24495606,  0.17137503,  0.77936833, -1.01965967],\n",
       "       [ 1.05778013,  0.76342693,  1.77232962, -0.96104539, -0.08811641,\n",
       "         0.33674077, -0.21148717, -1.49066675, -1.32453886, -1.3802293 ],\n",
       "       [ 0.80501948,  1.2564309 , -0.05850006, -1.96858002,  1.5098404 ,\n",
       "        -0.82138883, -1.43376613, -0.20117943,  0.09781934, -0.11332511],\n",
       "       [ 1.77450867,  0.67732937, -0.34216724, -0.44971138,  1.79136373,\n",
       "        -0.66177269,  1.00676265,  0.29106454,  0.89493555, -0.21369749],\n",
       "       [ 1.00692478, -0.70579108,  0.37119276, -1.7544736 , -0.50768877,\n",
       "        -0.13920805,  0.98780768,  1.05706905,  0.43222048, -1.56338747],\n",
       "       [-0.65672547, -0.09103197, -1.178256  , -0.36917564,  2.00815605,\n",
       "        -1.11143611, -0.19733534, -0.67585454,  0.91811814, -0.60532824],\n",
       "       [ 0.08459137,  0.49163205, -1.13671431,  0.2685497 , -0.26500548,\n",
       "        -1.88354056,  0.84902147, -0.14644722,  0.76311656,  1.44498978],\n",
       "       [-0.54624827,  0.38542096,  1.63246864, -0.2585312 , -0.91037463,\n",
       "        -1.11468131, -1.97914078,  0.03496614, -1.06421765, -0.8295912 ],\n",
       "       [-1.42148053, -0.95180815, -0.25972599,  0.4707728 ,  0.06931999,\n",
       "        -0.82176286, -0.88083009, -1.00956624, -0.00897779,  2.2308322 ],\n",
       "       [ 0.84266984,  1.30408572,  0.20015039,  0.5615169 , -0.65296663,\n",
       "         0.97015807,  1.38700084,  0.4951084 , -1.44448973, -1.16752713],\n",
       "       [ 0.4985632 ,  2.3945698 ,  1.38710141,  0.46910109,  0.33734053,\n",
       "        -1.23949995, -0.61190318,  0.32451539, -0.05789326, -1.23558975],\n",
       "       [ 0.38291206, -0.00759767,  0.93585618,  1.70632753,  1.01893822,\n",
       "        -1.06911745, -0.84419934, -0.67012947,  1.66929492, -0.25548583],\n",
       "       [-0.85385692, -1.56233021, -1.78991592, -0.37255036, -0.9249025 ,\n",
       "        -1.6846726 ,  0.32114733,  1.31372489,  1.05408442, -1.39430468],\n",
       "       [-1.5018825 ,  0.26488359, -0.53076249,  1.6708959 ,  0.27143629,\n",
       "         0.1466047 , -1.31955686,  1.71370231,  0.65300717,  1.61281628],\n",
       "       [-1.51203259, -0.39572513,  0.38419586, -0.6823474 ,  0.07519616,\n",
       "         1.66390286, -1.97019699, -0.38811612, -0.44951716, -0.77662094],\n",
       "       [ 0.83704632, -1.96031949,  0.18797492,  0.91590376,  1.15283072,\n",
       "         0.51642011,  0.84241534,  0.44585173,  1.45640509, -1.83124556],\n",
       "       [ 1.92206704,  1.47056032, -0.00406   ,  0.76778018,  0.4053105 ,\n",
       "         1.07126801, -0.026245  , -0.89590521,  1.41038388, -2.36215574],\n",
       "       [ 1.29458536, -0.83621381, -1.9517757 , -0.1324523 ,  0.83403392,\n",
       "        -0.1774459 , -1.02376095, -1.16475156, -0.17342272, -0.6319843 ],\n",
       "       [ 0.17336049,  0.04089555, -1.27560476,  0.60595067, -1.23526217,\n",
       "         2.11755679,  0.67800316,  0.9816548 , -0.53819726, -1.48369522],\n",
       "       [ 1.35176837,  1.12930276,  0.45335844,  2.09004711, -0.2852887 ,\n",
       "        -0.52948827, -0.21326742,  1.09567641, -1.45861346,  0.07245137],\n",
       "       [ 0.21232134,  0.07740626, -2.60893894,  1.32348304, -0.08148714,\n",
       "        -0.95045501, -0.63220659,  0.03582499,  0.07313937,  0.93465466],\n",
       "       [-0.6344957 , -2.01719472, -1.58639108,  1.92451622,  0.0250036 ,\n",
       "        -0.2535994 ,  0.27991245,  0.28675205,  3.23885619,  1.23512829],\n",
       "       [ 0.23324629, -0.81385165, -1.08401507, -0.80369496, -0.11238495,\n",
       "        -0.80819993, -1.69063394, -2.0135628 ,  1.31387044,  0.47585681],\n",
       "       [-0.49963994,  1.35849991,  0.28112175, -1.28978621,  0.15313457,\n",
       "        -2.10441061, -0.37593203,  0.35531336, -1.37029975, -0.92553935],\n",
       "       [ 0.25499162,  2.6937966 , -1.22866972, -0.6009424 ,  1.15582671,\n",
       "         0.17963652, -0.35502042, -0.07955323,  0.61564226, -0.67431376],\n",
       "       [-0.71675356,  0.21037284,  1.06208761, -1.26838047, -0.98813322,\n",
       "        -0.19912103,  0.2661799 , -1.84027428,  0.4893086 , -1.40526423],\n",
       "       [-0.91220952,  1.05346427, -1.27416819, -1.38112843,  0.5872168 ,\n",
       "        -1.07982749, -1.03267073,  1.16666964,  0.88234566, -0.00877552],\n",
       "       [ 0.59294741, -0.49930866, -0.34625371, -1.74482878,  1.43377496,\n",
       "         0.83467315, -0.31859294,  0.65827638, -1.56715242, -1.13192011],\n",
       "       [ 0.08064387, -1.07407349,  0.65194826, -0.78843376, -2.35459065,\n",
       "        -1.18905639, -0.03351356,  0.22030413,  0.90017831, -0.94312275],\n",
       "       [ 1.51333243, -0.51659202,  0.46177897,  2.30785006, -1.41201759,\n",
       "         0.20665316, -0.45131595,  0.04248109, -0.13719135, -0.80603153],\n",
       "       [ 0.50464748, -1.45981521,  0.98524436,  0.55793713, -0.13645376,\n",
       "        -0.83605131, -1.02010683,  0.47506778,  0.01444452, -0.54525131],\n",
       "       [ 0.6517877 ,  0.03670637,  2.00810018, -0.30927333,  0.28418133,\n",
       "        -0.60868821, -0.57513864, -0.21019374,  0.09730159,  0.78116411],\n",
       "       [ 1.16652747,  0.14898179,  0.604168  ,  1.70981243, -0.42754722,\n",
       "        -0.4983158 , -0.20120526, -0.98475908, -2.10225298, -0.0247042 ],\n",
       "       [-0.18418601,  1.17537944,  1.11715622,  1.01818565,  0.45409776,\n",
       "        -0.55228828, -1.51367743, -0.02330498,  0.54634913,  0.83452307]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"w1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But of course, this is not a user-friendly way of displaying the embeddings. In particular, what we want is to be able to input a word through a function and receive as output the embedding vector for that given word. Below is a function that implements this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, word):\n",
    "    try:\n",
    "        idx = word_to_id[word]\n",
    "    except KeyError:\n",
    "        print(\"`word` not in corpus\")\n",
    "    one_hot = one_hot_encode(idx, len(word_to_id))\n",
    "    return forward(model, one_hot)[\"a1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we test out the word \"machine,\" we get a dense ten-dimensional vector as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.03411644, -0.68148228, -0.49401597, -1.04198152, -1.96143292,\n",
       "       -0.68325082,  0.74263224, -0.46497574, -0.33083338,  0.29300595])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding(model, \"machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, this vector is not a collection of some randomly initialized numbers, but a result of training with context data generated through the sliding window algorithm described above. In other words, these vectors encode meaningful semantic information that tells us which words tend to go along with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "While this is a relatively simple, basic implementation of word2vec, the underlying principle remains the same nonetheless. The idea is that, we can train a neural network to generate word embeddings in the form of a weight matrix. This is why embedding layers can be trained to generate custom embeddings in popular neural network libraries like TensorFlow or PyTorch. If you end up training word embeddings on large datasets like Wikipedia, you end up with things like word2vec and [GloVe](https://nlp.stanford.edu/projects/glove/), another extremely popular alternative to word2vec. In general, it's fascinating to think that, with enough data, we can encode enough semantics into these embedding vectors to see relationships such as \"king - man + woman = queen.\"\n",
    "\n",
    "I hope you've enjoyed reading this post. See you in the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
