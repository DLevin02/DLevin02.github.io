var store = [{
        "title": "Live Corona Map",
        "excerpt":"Live Corona Map is a COVID-19 data visuaization and dashboard project I joined in the April of 2020 as a Python developer.   About the Project   When reported cases of COVID-19 in Korea started to erupt in late Febuary and early March, there existed no centrailzed platform that provided up-to-date, reliable information to the general public. As a matter of fact, even government entities were poorly equipped to flexibly respond to the pandemic. Live Corona Map was created at this critical point in time to provide a trustworthy channel of data to reduce misinformation and the asymmetry of information. It was also used by government agencies—most notably the Jeju Self-governing Provincial Government—to monitor the status quo. Even today, the link redirecting users to Live Corona Map is available on the website of the Jeju Provincial Government.   Given the public nature of Live Corona Map, perhaps it is unsurprising that te code base for this project is entirely open-source. Until recently, the web server for the website was sponsored by Naver. It is also worth mentioning that the project is run entirely by volunteers and civic hackers who commit their time without any form of renumeration.   Contributions   When I joined the team in April, the project was at a transition phase: reported cases were somewhat starting to drop, as was the volume of user traffic to the website. However, the number of COVID-19 cases started to spike up again in May and June, making it necessary to maintain and improve the web application.   Summarized below is a list of my contributions as a Python developer, which would not have been possible without the support and help of my team members.   Web Crawlers   The bulk of my work revolved around writing and debugging Python web crawlers. Although there were scripts for different web crawlers, a lot of them were either missing or broken. When I joined the team, people were hand-scraping data from websites instead of relying on automation, saying that web crawlers were highly unreliable.   Using bs4, pandas, and other libraries, I rewrote and engineered 8 web crawlers to make them fully functional and more robust. We were then able to use GitHub Actions CI to schedule automatic web scraping. This vastly improved the reliability and timeliness of the data and graphics posted on the website.   Refactoring   Instead of the more conventional SQL-flavored database, the website directly saves web craped data as .js files, hosted directly on GitHub. The reason behind this seemingly atypical choice is that we want to share the data with other groups and developers—after all, this is the very spirit of open-source.   The issue I saw was that a lot of the code was repeated. For instance, all Python crawler scripts basically used the same routine to create a bs4.BeautifulSoup object using requests. The same logic was also used for other operations, most notably encoding Python dictionaries as JSON files for use later on the client-side.   I created a utils.py file to reduce unnecessary boilerplate and performed some micro-optimizations to streamline the codebase. This vastly improved code readability across the Python portion of the repository.   News Scraping   A new feature I added to the website was the “Major Headlines” widget. Previously, the website only displayed numerical, qualitative data. However, I thought it would be informative and helpful to dispay the hottest news headlines regarding COVID-19 so that interested users could be redirected to media websites.   Adding this functionality required use of not only Python for scraping and saving data, but also Javascript for rendering it on the website. It was a challenging, yet also an incrediby rewarding experience.      Daily Graph   I added a graph to the website that displays per-day increases in reported cases of COVID-19. This was not an entirely new graph; however, the graph was somewhat ineffective in presenting information since the x-axis was fixed to start from Febuary 10th. Due to the huge spikes in March—at one point, the estimate is over 400 on a single day—more recent cases of 40 to 50 patients on the daily were completely overshadowed in the visualization.   Using simple Javascript and chart.js, a team member and I added a new visualization that displays displays this information for only the last 10 days. I also plan on adding an exponential moving average estimate to smoothen out the graphs we already have.   More   You can visit the GitHub repository for the website through this link. Feel free to submit a pull request if you find any bugs if there are improvements you want to make yourself!   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/coronamap/",
        "teaser": "http://localhost:4000/assets/images/portfolio/coronamap-th.png"
      },{
        "title": "ML from Scratch",
        "excerpt":"For more information, visit this link.   (This is an unfinished page.)  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/ml-from-scratch/",
        "teaser": null
      },{
        "title": "PPETrackr",
        "excerpt":"For more information, visit this link.   (This is an unfinished page.)   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/ppetrackr/",
        "teaser": null
      },{
        "title": "PX Chatbot",
        "excerpt":"For more information, visit this link.   (This is an unfinished page.)  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/px-chatbot/",
        "teaser": null
      },{
        "title": "Shape of You",
        "excerpt":"For more information, visit this link.   (This is an unfinished page.)   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/shape-of-you/",
        "teaser": null
      },{
        "title": "A Self-Created Curriculum",
        "excerpt":"My Machine Learning journey  Welcome to my blog! I’m embarking on a journey to learn Machine Learning, and I’m excited to share my experiences here. Follow me as I explore the world of Machine Learning algorithms and technologies!   My First Steps  I’m starting my journey by exploring the fundamentals of Machine Learning. I’m brushing up on my math and programming skills, and I’m getting familiar with the different types of algorithms and techniques. I’m also reading up on the latest Machine Learning research.   My Next Steps  My next steps involve diving deeper into the various techniques and algorithms. I’m also exploring different types of Machine Learning tools and libraries, and I’m working on some basic projects to get hands-on experience.   Update 01/01/2023:  My first update: I have started this blog a little bit late as I am in the Sequential Models Course of Andrew Ngâ€™s Deep Learning Specialization. Today I got bored of watching lectures and wanted to code so I implemented MNIST with TensorFlow. I watched the first Stanford CS244N Lecuture on NLP and realized that I just need to write code so I went through a build a house price prediction model for a Kaggle Competition.  ","categories": ["blog"],
        "tags": ["update"],
        "url": "http://localhost:4000/blog/studying-deep-learning/",
        "teaser": null
      },{
        "title": "AI RESOURCES",
        "excerpt":"Introduction  AI is revolutionizing various aspects of our daily lives, with new applications and advancements emerging every week. Here’s a roundup of some of the most impactful and intriguing AI news from the past week.   AI Resource Highlights  To stay informed and explore the latest developments in AI, I rely on a variety of trusted resources. Here are some of my favorites:           The Batch by Andrew Ng  Andrew Ng, a globally recognized leader in AI, provides valuable insights through his platform, The Batch. As the founder of DeepLearning.AI and other prominent organizations, his expertise and contributions have made a significant impact on the AI community. Website            MarkeTechPost  MarkeTechPost is a California-based AI News Platform with a thriving community of over 1.5 million AI professionals and developers. They deliver technical AI research news in a digestible and applicable format, making it a valuable resource for staying updated with the latest trends. Website            Papers with Code  Papers with Code aims to create a free and open resource that combines machine learning papers, code, datasets, methods, and evaluation tables. This platform provides a comprehensive collection of research papers, making it easier to access and implement cutting-edge AI techniques. Website            TLDR AI  TLDR AI offers quick and concise summaries of AI, machine learning, and data science topics, all within a five-minute read. This resource is perfect for busy individuals who want to stay informed about the latest AI advancements without spending too much time. Website            Bloomberg Technology  Bloomberg Technology covers a wide range of technology-related news, including AI. Their insightful articles and analysis provide valuable perspectives on the intersection of AI and various industries, making it a reliable resource for industry trends and developments. Website       In addition to these platforms, I also find Twitter to be a valuable source of AI news and insights. Here are some Twitter accounts I follow:      @dair_ai: Focuses on democratizing AI research, education, and technologies.   @papers_daily: Tweets popular AI papers, curated by @labmlai and AK @_akhaliq.   @Gradio: Shares information about AI, ML, and data science, often focusing on practical applications and tools.   @ai__pub: Provides AI papers and AI research explanations for technical individuals.   @Deeplearning_AI: Shares information about deep learning, AI, and machine learning advancements.   @DLdotHub: Focuses on open-source software, tools, datasets, news, and research in the machine learning and data science domain, with an emphasis on deep learning.   By utilizing these resources, I can stay up-to-date with the latest AI developments, learn from industry leaders, and explore practical applications of AI.   Conclusion  As AI continues to evolve at a rapid pace, it is more important than ever to stay informed about the latest developments and discussions. The resources mentioned in this blog post provide valuable insights, technical knowledge, and practical applications of AI. By leveraging these resources, we can actively participate in shaping the future of AI and harness its transformative power across various domains.   ","categories": ["blog"],
        "tags": ["pytorch","news"],
        "url": "http://localhost:4000/blog/AI-Resources/",
        "teaser": null
      },{
        "title": "PyTorch RNN from Scratch",
        "excerpt":"In this post, we’ll take a look at RNNs, or recurrent neural networks, and attempt to implement parts of it in scratch through PyTorch. Yes, it’s not entirely from scratch in the sense that we’re still relying on PyTorch autograd to compute gradients and implement backprop, but I still think there are valuable insights we can glean from this implementation as well.   Data Preparation   The task is to build a simple classification model that can correctly determine the nationality of a person given their name. Put more simply, we want to be able to tell where a particular name is from.   Download   We will be using some labeled data from the PyTorch tutorial. We can download it simply by typing   !curl -O https://download.pytorch.org/tutorial/data.zip; unzip data.zip   This command will download and unzip the files into the current directory, under the folder name of data.   Now that we have downloaded the data we need, let’s take a look at the data in more detail. First, here are the dependencies we will need.   import os import random from string import ascii_letters  import torch from torch import nn import torch.nn.functional as F from unidecode import unidecode  _ = torch.manual_seed(42) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   We first specify a directory, then try to print out all the labels there are. We can then construct a dictionary that maps a language to a numerical label.   data_dir = \"./data/names\"  lang2label = {     file_name.split(\".\")[0]: torch.tensor([i], dtype=torch.long)     for i, file_name in enumerate(os.listdir(data_dir)) }   We see that there are a total of 18 languages. I wrapped each label as a tensor so that we can use them directly during training.   lang2label   {'Czech': tensor([0]),  'German': tensor([1]),  'Arabic': tensor([2]),  'Japanese': tensor([3]),  'Chinese': tensor([4]),  'Vietnamese': tensor([5]),  'Russian': tensor([6]),  'French': tensor([7]),  'Irish': tensor([8]),  'English': tensor([9]),  'Spanish': tensor([10]),  'Greek': tensor([11]),  'Italian': tensor([12]),  'Portuguese': tensor([13]),  'Scottish': tensor([14]),  'Dutch': tensor([15]),  'Korean': tensor([16]),  'Polish': tensor([17])}   Let’s store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer.   num_langs = len(lang2label)   Preprocessing   Now, let’s preprocess the names. We first want to use unidecode to standardize all names and remove any acute symbols or the likes. For example,   unidecode(\"Ślusàrski\")   'Slusarski'   Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. This can first be done by constructing a char2idx mapping, as shown below.   char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")} num_letters = len(char2idx); num_letters   59   We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-‘. This also means that each name will now be expressed as a tensor of size (num_char, 59); in other words, each character will be a tensor of size (59,)`. We can now build a function that accomplishes this task, as shown below:   def name2tensor(name):     tensor = torch.zeros(len(name), 1, num_letters)     for i, char in enumerate(name):         tensor[i][0][char2idx[char]] = 1     return tensor   If you read the code carefully, you’ll realize that the output tensor is of size (num_char, 1, 59), which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case. In PyTorch, RNN layers expect the input tensor to be of size (seq_len, batch_size, input_size). Since every name is going to have a different length, we don’t batch the inputs for simplicity purposes and simply use each input as a single batch. For a more detailed discussion, check out this forum discussion.   Let’s quickly verify the output of the name2tensor() function with a dummy input.   name2tensor(\"abc\")   tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0.]],          [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0.]],          [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0.]]])   Dataset Creation   Now we need to build a our dataset with all the preprocessing steps. Let’s collect all the decoded and converted tensors in a list, with accompanying labels. The labels can be obtained easily from the file name, for example german.txt.   tensor_names = [] target_langs = []  for file in os.listdir(data_dir):     with open(os.path.join(data_dir, file)) as f:         lang = file.split(\".\")[0]         names = [unidecode(line.rstrip()) for line in f]         for name in names:             try:                 tensor_names.append(name2tensor(name))                 target_langs.append(lang2label[lang])             except KeyError:                 pass   We could wrap this in a PyTorch Dataset class, but for simplicity sake let’s just use a good old for loop to feed this data into our model. Since we are dealing with normal lists, we can easily use sklearn’s train_test_split() to separate the training data from the testing data.   from sklearn.model_selection import train_test_split  train_idx, test_idx = train_test_split(     range(len(target_langs)),      test_size=0.1,      shuffle=True,      stratify=target_langs )  train_dataset = [     (tensor_names[i], target_langs[i])     for i in train_idx ]  test_dataset = [     (tensor_names[i], target_langs[i])     for i in test_idx ]   Let’s see how many training and testing data we have. Note that we used a test_size of 0.1.   print(f\"Train: {len(train_dataset)}\") print(f\"Test: {len(test_dataset)}\")   Train: 18063 Test: 2007   Model   We will be building two models: a simple RNN, which is going to be built from scratch, and a GRU-based model using PyTorch’s layers.   Simple RNN   Now we can build our model. This is a very simple RNN that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. Notice that it is just some fully connected layers with a sigmoid non-linearity applied during the hidden state computation.   class MyRNN(nn.Module):     def __init__(self, input_size, hidden_size, output_size):         super(MyRNN, self).__init__()         self.hidden_size = hidden_size         self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)         self.in2output = nn.Linear(input_size + hidden_size, output_size)          def forward(self, x, hidden_state):         combined = torch.cat((x, hidden_state), 1)         hidden = torch.sigmoid(self.in2hidden(combined))         output = self.in2output(combined)         return output, hidden          def init_hidden(self):         return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))   We call init_hidden() at the start of every new batch. For easier training and learning, I decided to use kaiming_uniform_() to initialize these hidden states.   We can now build our model and start training it.   hidden_size = 256 learning_rate = 0.001  model = MyRNN(num_letters, hidden_size, num_langs) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   I realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. Nonetheless, I didn’t want to cook my 13-inch MacBook Pro so I decided to stop at two epochs.   num_epochs = 2 print_interval = 3000  for epoch in range(num_epochs):     random.shuffle(train_dataset)     for i, (name, label) in enumerate(train_dataset):         hidden_state = model.init_hidden()         for char in name:             output, hidden_state = model(char, hidden_state)         loss = criterion(output, label)          optimizer.zero_grad()         loss.backward()         nn.utils.clip_grad_norm_(model.parameters(), 1)         optimizer.step()                  if (i + 1) % print_interval == 0:             print(                 f\"Epoch [{epoch + 1}/{num_epochs}], \"                 f\"Step [{i + 1}/{len(train_dataset)}], \"                 f\"Loss: {loss.item():.4f}\"             )   Epoch [1/2], Step [3000/18063], Loss: 0.0390 Epoch [1/2], Step [6000/18063], Loss: 1.0368 Epoch [1/2], Step [9000/18063], Loss: 0.6718 Epoch [1/2], Step [12000/18063], Loss: 0.0003 Epoch [1/2], Step [15000/18063], Loss: 1.0658 Epoch [1/2], Step [18000/18063], Loss: 1.0021 Epoch [2/2], Step [3000/18063], Loss: 0.0021 Epoch [2/2], Step [6000/18063], Loss: 0.0131 Epoch [2/2], Step [9000/18063], Loss: 0.3842 Epoch [2/2], Step [12000/18063], Loss: 0.0002 Epoch [2/2], Step [15000/18063], Loss: 2.5420 Epoch [2/2], Step [18000/18063], Loss: 0.0172   Now we can test our model. We could look at other metrics, but accuracy is by far the simplest, so let’s go with that.   num_correct = 0 num_samples = len(test_dataset)  model.eval()  with torch.no_grad():     for name, label in test_dataset:         hidden_state = model.init_hidden()         for char in name:             output, hidden_state = model(char, hidden_state)         _, pred = torch.max(output, dim=1)         num_correct += bool(pred == label)  print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")   Accuracy: 72.2471%   The model records a 72 percent accuracy rate. This is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple RNN model was at least able to learn something.   Let’s see how well our model does with some concrete examples. Below is a function that accepts a string as input and outputs a decoded prediction.   label2lang = {label.item(): lang for lang, label in lang2label.items()}  def myrnn_predict(name):     model.eval()     tensor_name = name2tensor(name)     with torch.no_grad():         hidden_state = model.init_hidden()         for char in tensor_name:             output, hidden_state = model(char, hidden_state)         _, pred = torch.max(output, dim=1)     model.train()         return label2lang[pred.item()]   I don’t know if any of these names were actually in the training or testing set; these are just some random names I came up with that I thought would be pretty reasonable. And voila, the results are promising.   myrnn_predict(\"Mike\")   'English'   myrnn_predict(\"Qin\")   'Chinese'   myrnn_predict(\"Slaveya\")   'Russian'   The model seems to have classified all the names into correct categories!   PyTorch GRU   This is cool and all, and I could probably stop here, but I wanted to see how this custom model fares in comparison to, say, a model using PyTorch layers. GRU is probably not fair game for our simple RNN, but let’s see how well it does.   class GRUModel(nn.Module):     def __init__(self, num_layers, hidden_size):         super(GRUModel, self).__init__()         self.num_layers = num_layers         self.hidden_size = hidden_size         self.gru = nn.GRU(             input_size=num_letters,              hidden_size=hidden_size,              num_layers=num_layers,         )         self.fc = nn.Linear(hidden_size, num_langs)          def forward(self, x):         hidden_state = self.init_hidden()         output, hidden_state = self.gru(x, hidden_state)         output = self.fc(output[-1])         return output          def init_hidden(self):         return torch.zeros(self.num_layers, 1, self.hidden_size).to(device)   Let’s declare the model and an optimizer to go with it. Notice that we are using a two-layer GRU, which is already one more than our current RNN implementation.   model = GRUModel(num_layers=2, hidden_size=hidden_size) optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   for epoch in range(num_epochs):     random.shuffle(train_dataset)     for i, (name, label) in enumerate(train_dataset):         output = model(name)         loss = criterion(output, label)          optimizer.zero_grad()         loss.backward()         optimizer.step()                   if (i + 1) % print_interval == 0:             print(                 f\"Epoch [{epoch + 1}/{num_epochs}], \"                 f\"Step [{i + 1}/{len(train_dataset)}], \"                 f\"Loss: {loss.item():.4f}\"             )   Epoch [1/2], Step [3000/18063], Loss: 1.8497 Epoch [1/2], Step [6000/18063], Loss: 0.4908 Epoch [1/2], Step [9000/18063], Loss: 1.0299 Epoch [1/2], Step [12000/18063], Loss: 0.0855 Epoch [1/2], Step [15000/18063], Loss: 0.0053 Epoch [1/2], Step [18000/18063], Loss: 2.6417 Epoch [2/2], Step [3000/18063], Loss: 0.0004 Epoch [2/2], Step [6000/18063], Loss: 0.0008 Epoch [2/2], Step [9000/18063], Loss: 0.1446 Epoch [2/2], Step [12000/18063], Loss: 0.2125 Epoch [2/2], Step [15000/18063], Loss: 3.7883 Epoch [2/2], Step [18000/18063], Loss: 0.4862   The training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. This is partially because I didn’t use gradient clipping for this GRU model, and we might see better results with clipping applied.   Let’s see the accuracy of this model.   num_correct = 0  model.eval()  with torch.no_grad():     for name, label in test_dataset:         output = model(name)         _, pred = torch.max(output, dim=1)         num_correct += bool(pred == label)  print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")   Accuracy: 81.4150%   And we get an accuracy of around 80 percent for this model. This is better than our simple RNN model, which is somewhat expected given that it had one additional layer and was using a more complicated RNN cell model.   Let’s see how this model predicts given some raw name string.   def pytorch_predict(name):     model.eval()     tensor_name = name2tensor(name)     with torch.no_grad():         output = model(tensor_name)         _, pred = torch.max(output, dim=1)     model.train()     return label2lang[pred.item()]   pytorch_predict(\"Jake\")   'English'   pytorch_predict(\"Qin\")   'Chinese'   pytorch_predict(\"Fernando\")   'Spanish'   pytorch_predict(\"Demirkan\")   'Russian'   The last one is interesting, because it is the name of a close Turkish friend of mine. The model obviously isn’t able to tell us that the name is Turkish since it didn’t see any data points that were labeled as Turkish, but it tells us what nationality the name might fall under among the 18 labels it has been trained on. It’s obviously wrong, but perhaps not too far off in some regards; at least it didn’t say Japanese, for instance. It’s also not entirely fair game for the model since there are many names that might be described as multi-national: perhaps there is a Russian person with the name of Demirkan.   Conclusion   I learned quite a bit about RNNs by implementing this RNN. It is admittedly simple, and it is somewhat different from the PyTorch layer-based approach in that it requires us to loop through each character manually, but the low-level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. It was also a healthy reminder of how RNNs can be difficult to train.   In the coming posts, we will be looking at sequence-to-sequence models, or seq2seq for short. Ever since I heard about seq2seq, I was fascinated by tthe power of transforming one form of data to another. Although these models cannot be realistically trained on a CPU given the constraints of my local machine, I think implementing them themselves will be an exciting challenge.   Catch you up in the next one!  ","categories": ["study"],
        "tags": ["pytorch","deep_learning","from_scratch"],
        "url": "http://localhost:4000/study/pytorch-rnn/",
        "teaser": null
      },{
        "title": "AI Evolution: Weekly News Digest",
        "excerpt":"Introduction  AI is revolutionizing various aspects of our daily lives, with new applications and advancements emerging every week. Here’s a roundup of some of the most impactful and intriguing AI news from the past week.   Chatbots in the Classroom  In a groundbreaking move, certain US primary and secondary schools are testing an automated tutor called ‘Khanmigo’, built by online educator Khan Academy. Based on the GPT-4 architecture, the bot challenges students by replying to queries with questions, fostering critical thinking. Integrated with the Khan Academy’s previous tutoring software, it aids students in vocabulary practice, creative writing, debates, and even in navigating university admissions and financial aid. While some educators fear it may encourage cheating or spread misinformation, others see it as an invaluable 24/7 learning resource.  Training Data Free-For-All in Japan   In a striking legislative decision, Japan has given the green light for AI developers to train models on copyrighted works, a move that has sparked debates about fairness and copyright laws. This law permits developers to use copyrighted works for commercial purposes, which is quite unique globally. As the G7 countries strive to create mutually compatible regulations for generative AI, Japan’s stance could influence the direction these policies take.   Image Generation Becomes Swift with Paella  A new system, Paella, developed by Dominic Rampas and colleagues, leverages diffusion processes to generate high-quality images swiftly. By utilizing tokens from a predefined list, the number of steps needed for image generation is greatly reduced, making the process quicker without sacrificing quality. This technique could pave the way for a host of applications, from engineering to entertainment.   Google’s Confidentiality Concerns with Chatbots  In light of potential information leaks, Google has advised its employees against entering confidential information into chatbots like OpenAI’s ChatGPT or Google’s own Bard. It highlights the security concerns related to AI, prompting an essential conversation about data privacy and confidentiality in the age of AI.   Self-driving Cars: Motion Prediction Improves  The research into self-driving cars is far from over. Waymo has showcased how diffusion models can be used to predict distributions of motion for multiple “agents” on the road. This innovation improves performance over physics-based methods and other neural algorithms, nudging us closer to the reality of self-driving cars on our streets.   Fine-tuning Large Models Becomes More Accessible  A significant development in the field of AI is the advancement of Low Rank Adaptation (LoRA), a task-specific and model-specific module used for fine-tuning large models. The improved LoRA enables fine-tuning on relatively inexpensive hardware, thus democratizing access to sophisticated AI capabilities.   Research Section: FinGPT: An Open-Source Framework for FinLLMs  In response to the aforementioned challenges and in pursuit of democratizing FinLLMs, we present FinGPT. A data-centric, open-source framework, FinGPT aims to provide a robust and comprehensive solution for the development and application of FinLLMs.   4.1 Data Source Layer   As a starting point, we put forward the data source layer, which ensures thorough market coverage. In consideration of the temporal sensitivity of financial data, this layer is designed to provide real-time information capture from various sources, including financial news, company filings and announcements, social media discussions, and financial trends. This rich collection of diverse data types allows FinGPT to provide multi-faceted insights into the financial landscape.   4.2 Data Engineering Layer   The next layer in the FinGPT framework is the data engineering layer. Primed for real-time NLP data processing, this layer grapples with the inherent challenges of high temporal sensitivity and low signal-to-noise ratio in financial data. The data engineering layer involves comprehensive data cleaning, preprocessing, and formatting to ensure the quality and usefulness of the financial data fed into the subsequent layers of the FinGPT framework.   4.3 LLMs Layer   Building on the previous layers, we introduce the LLMs layer, which focuses on the implementation of a range of fine-tuning methodologies for the LLMs. Recognizing the highly dynamic nature of financial data, this layer prioritizes keeping the model’s relevance and accuracy up-to-date, ensuring that FinGPT maintains its ability to provide accurate and actionable insights.   4.4 Application Layer   The final layer of the FinGPT framework is the application layer, which showcases practical applications and demonstrations of FinGPT in the financial sector. This layer provides concrete examples of how FinGPT can be utilized in various contexts, such as robo-advising, algorithmic trading, and low-code development. It serves as a testament to FinGPT’s potential to revolutionize financial operations and decision-making processes.   5 Paper Conclusion   Through our development of FinGPT, we aim to foster a thriving, open-source ecosystem for financial large language models (FinLLMs). We hope that this framework will stimulate further innovation in the finance domain, facilitate the democratization of FinLLMs, and unlock new opportunities in open finance. By championing data accessibility and transparency, FinGPT is positioned to reshape the understanding and application of FinLLMs in financial research and practice.   Data Preparation for Price Data and Tweets First, we fetch price data and Tweets data from stocknet-dataset Second, we input Tweets data to a GPT model, say \"text-curie-001\" or \"text-davinci-003\", and get the corresponding sentiment scores Third, we save the sentiment scores to a file under ./data   ChatGPT Trading Agent We calculate the average sentiment score S.  We implement a simple strategy that buys 100 shares when S &gt;= 0.3 and sells 100 shares when S &lt;= -0.3  Parameters of GPT Model are:  \"model_name\": \"text-davinci-003\",  # \"text-curie-001\",\"text-davinci-003\" \"source\": \"local\",                 # \"local\",\"openai\" \"api_key\": OPEN_AI_TOKEN,          # not necessary when the \"source\" is \"local\" \"buy_threshold\": 0.3,              # the max positive sentiment is 1, so this should range from 0 to 1  \"sell_threshold\": -0.3             # the min negative sentiment is -1, so this should range from -1 to 0   Backtest We backtest the agent's performance from '2014-01-01' to '2015-12-30'.  Parameters are:  \"stock_name\" : \"AAPL\",        # please refer to the stocks provided by stocknet-dataset \"start_date\":\"2014-01-01\",    # should be later than 2014-01-01 \"end_date\":\"2015-12-30\",      # should be earlier than 2015-12-30 \"init_cash\": 100,             # initial available cash \"init_hold\": 0,               # initial available stock holdings \"cal_on\": \"Close\",            # The column that used to calculate prices \"trade_volumn\": 100,          # Volumns to trade   The result is shown as follows:      The performance metrics are as follows metrics\tresult Annual return\t30.603% Cumulative returns\t66.112% Annual volatility\t13.453% Sharpe ratio\t2.06 Calmar ratio\t4.51 Stability\t0.87 Max drawdown\t-6.778% Omega ratio\t2.00 Sortino ratio\t4.30 Tail ratio\t1.84 Daily value at risk\t-1.585% Alpha\t0.24 Beta\t0.31   Conclusion  As AI continues to evolve at a rapid pace, it is more important than ever to stay informed about the latest developments and discussions. The articles featured in this week’s AI Evolution: Weekly News Digest provide a glimpse into the diverse and transformative applications of AI.   From the integration of chatbots in classrooms to the implications of training data free-for-all in Japan, these stories shed light on the opportunities and challenges presented by AI. The advancements in image generation, motion prediction for self-driving cars, and the democratization of fine-tuning large models further emphasize the progress we are making.   Moreover, the introduction of FinGPT as an open-source framework for FinLLMs opens up new possibilities in the financial domain, fostering innovation and accessibility. It holds the potential to revolutionize financial research and decision-making processes, making them more inclusive and transparent.   By staying up-to-date with these developments, I can actively participate in shaping the future of AI.   ","categories": ["blog"],
        "tags": ["pytorch","news"],
        "url": "http://localhost:4000/blog/AI-Weeky/",
        "teaser": null
      },{
        "title": "Stock Price Prediction of Apple with PyTorch",
        "excerpt":"LSTM and GRU   Time Series  Machine Learning’s captivating domain of time series forecasting commands attention, offering potentially significant benefits when integrated with advanced subjects like stock price prediction. Essentially, time series forecasting employs a specific model to anticipate future data points by utilizing the patterns found in previously observed value.   A time series, by definition, is a sequence of data points arranged in chronological order. This kind of problem holds significant relevance because numerous prediction challenges incorporate a temporal aspect. Uncovering the relationship between data and time is crucial to these analyses, such as weather forecasting or earthquake prediction. However, these issues are occasionally overlooked due to the non-trivial complexities involved in modeling these temporal relationships.   Stock market prediction entails efforts to estimate the future value of a company’s stock. The accurate prognostication of a stock’s future price can result in substantial gains, fitting this scenario into the realm of time series problems.   Over time, numerous methods have been developed to predict stock prices accurately, given their volatile and complex fluctuations. Neural networks, particularly Recurrent Neural Networks (RNNs), have exhibited significant applicability in this field. In this context, we will construct two distinct RNN models — Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) — using PyTorch. We aim to forecast Apples’s stock market price and compare these models’ performance in aspects of time and efficiency.   Recurrent Neural Network (RNN)   A Recurrent Neural Network (RNN) is a particular breed of artificial neural network crafted to discern patterns in sequential data to anticipate ensuing events. The power of this architecture lies in its interconnected nodes, enabling it to demonstrate dynamic behavior over time. Another notable attribute of this structure is the utilization of feedback loops for sequence processing. This feature facilitates the persistence of information, often likened to memory, rendering RNNs ideal for Natural Language Processing (NLP) and time series problems. This foundational structure gave rise to advanced architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU).   An LSTM unit comprises a cell and three gates: an input gate, an output gate, and a forget gate. The cell retains values over arbitrary time intervals, while the trio of gates control the influx and efflux of information from the cell.      Conversely, a GRU possesses fewer parameters compared to an LSTM as it lacks an output gate. However, both configurations are capable of resolving the “short-term memory” problem typically associated with basic RNNs, and successfully maintain long-term correlations in sequential data.      While LSTM enjoys greater popularity at present, it’s anticipated that GRU will ultimately surpass it due to its enhanced speed while maintaining comparable accuracy and effectiveness. It’s likely that we’ll observe a similar outcome in this case, with the GRU model demonstrating superior performance under these conditions.   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory  import os for dirname, _, filenames in os.walk('./archive'):     for filename in filenames:         print(os.path.join(dirname, filename))    ./archive/JPM_2006-01-01_to_2018-01-01.csv ./archive/MSFT_2006-01-01_to_2018-01-01.csv ./archive/JNJ_2006-01-01_to_2018-01-01.csv ./archive/UNH_2006-01-01_to_2018-01-01.csv ./archive/CAT_2006-01-01_to_2018-01-01.csv ./archive/AABA_2006-01-01_to_2018-01-01.csv ./archive/HD_2006-01-01_to_2018-01-01.csv ./archive/CVX_2006-01-01_to_2018-01-01.csv ./archive/MMM_2006-01-01_to_2018-01-01.csv ./archive/AMZN_2006-01-01_to_2018-01-01.csv ./archive/CSCO_2006-01-01_to_2018-01-01.csv ./archive/XOM_2006-01-01_to_2018-01-01.csv ./archive/all_stocks_2017-01-01_to_2018-01-01.csv ./archive/VZ_2006-01-01_to_2018-01-01.csv ./archive/WMT_2006-01-01_to_2018-01-01.csv ./archive/GS_2006-01-01_to_2018-01-01.csv ./archive/AAPL_2006-01-01_to_2018-01-01.csv ./archive/AXP_2006-01-01_to_2018-01-01.csv ./archive/all_stocks_2006-01-01_to_2018-01-01.csv ./archive/GOOGL_2006-01-01_to_2018-01-01.csv ./archive/UTX_2006-01-01_to_2018-01-01.csv ./archive/KO_2006-01-01_to_2018-01-01.csv ./archive/MRK_2006-01-01_to_2018-01-01.csv ./archive/TRV_2006-01-01_to_2018-01-01.csv ./archive/IBM_2006-01-01_to_2018-01-01.csv ./archive/INTC_2006-01-01_to_2018-01-01.csv ./archive/PFE_2006-01-01_to_2018-01-01.csv ./archive/GE_2006-01-01_to_2018-01-01.csv ./archive/DIS_2006-01-01_to_2018-01-01.csv ./archive/PG_2006-01-01_to_2018-01-01.csv ./archive/BA_2006-01-01_to_2018-01-01.csv ./archive/MCD_2006-01-01_to_2018-01-01.csv ./archive/NKE_2006-01-01_to_2018-01-01.csv   Implementation   The dataset contains historical stock prices. We are going to predict the Close price of the stock, and the following is the data behavior over the years.   filepath = './archive/AAPL_2006-01-01_to_2018-01-01.csv' data = pd.read_csv(filepath) data = data.sort_values('Date') data.head()                               Date       Open       High       Low       Close       Volume       Name                       0       2006-01-03       10.34       10.68       10.32       10.68       201853036       AAPL                 1       2006-01-04       10.73       10.85       10.64       10.71       155225609       AAPL                 2       2006-01-05       10.69       10.70       10.54       10.63       112396081       AAPL                 3       2006-01-06       10.75       10.96       10.65       10.90       176139334       AAPL                 4       2006-01-09       10.96       11.03       10.82       10.86       168861224       AAPL               import matplotlib.pyplot as plt import seaborn as sns  sns.set_style(\"darkgrid\") plt.figure(figsize = (15,9)) plt.plot(data[['Close']]) plt.xticks(range(0,data.shape[0],500),data['Date'].loc[::500],rotation=45) plt.title(\"Apple Stock Price\",fontsize=18, fontweight='bold') plt.xlabel('Date',fontsize=18) plt.ylabel('Close Price (USD)',fontsize=18) plt.show()   price = data[['Close']] price.info()   &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 3019 entries, 0 to 3018 Data columns (total 1 columns):  #   Column  Non-Null Count  Dtype   ---  ------  --------------  -----    0   Close   3019 non-null   float64 dtypes: float64(1) memory usage: 47.2 KB   I slice the data frame to get the column we want and normalize the data.   from sklearn.preprocessing import MinMaxScaler  scaler = MinMaxScaler(feature_range=(-1, 1)) price['Close'] = scaler.fit_transform(price['Close'].values.reshape(-1,1))   /var/folders/8_/k8h_gshs3_qdkr8glv1ff08h0000gn/T/ipykernel_43516/68737012.py:4: SettingWithCopyWarning:  A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead  See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy   price['Close'] = scaler.fit_transform(price['Close'].values.reshape(-1,1))   We’re now ready to partition the data into training and test sets. But prior to that, it’s necessary to determine the width of the analysis window. This technique of employing previous time steps to forecast the subsequent time step is referred to as the sliding window approach.   def split_data(stock, lookback):     data_raw = stock.to_numpy() # convert to numpy array     data = []          # create all possible sequences of length seq_len     for index in range(len(data_raw) - lookback):          data.append(data_raw[index: index + lookback])          data = np.array(data);     test_set_size = int(np.round(0.2*data.shape[0]));     train_set_size = data.shape[0] - (test_set_size);          x_train = data[:train_set_size,:-1,:]     y_train = data[:train_set_size,-1,:]          x_test = data[train_set_size:,:-1]     y_test = data[train_set_size:,-1,:]          return [x_train, y_train, x_test, y_test]   lookback = 20 # choose sequence length x_train, y_train, x_test, y_test = split_data(price, lookback) print('x_train.shape = ',x_train.shape) print('y_train.shape = ',y_train.shape) print('x_test.shape = ',x_test.shape) print('y_test.shape = ',y_test.shape)   x_train.shape =  (2399, 19, 1) y_train.shape =  (2399, 1) x_test.shape =  (600, 19, 1) y_test.shape =  (600, 1)   Next, we convert them into tensors, the foundational data structure required for constructing a model in PyTorch.   import torch import torch.nn as nn  x_train = torch.from_numpy(x_train).type(torch.Tensor) x_test = torch.from_numpy(x_test).type(torch.Tensor) y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor) y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor) y_train_gru = torch.from_numpy(y_train).type(torch.Tensor) y_test_gru = torch.from_numpy(y_test).type(torch.Tensor)   input_dim = 1 hidden_dim = 32 num_layers = 2 output_dim = 1 num_epochs = 100   LSTM   class LSTM(nn.Module):     def __init__(self, input_dim, hidden_dim, num_layers, output_dim):         super(LSTM, self).__init__()         self.hidden_dim = hidden_dim         self.num_layers = num_layers                  self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)         self.fc = nn.Linear(hidden_dim, output_dim)      def forward(self, x):         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()         out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))         out = self.fc(out[:, -1, :])          return out   model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers) criterion = torch.nn.MSELoss(reduction='mean') optimiser = torch.optim.Adam(model.parameters(), lr=0.01)   import time  hist = np.zeros(num_epochs) start_time = time.time() lstm = []  for t in range(num_epochs):     y_train_pred = model(x_train)      loss = criterion(y_train_pred, y_train_lstm)     print(\"Epoch \", t, \"MSE: \", loss.item())     hist[t] = loss.item()      optimiser.zero_grad()     loss.backward()     optimiser.step()      training_time = time.time()-start_time print(\"Training time: {}\".format(training_time))   Epoch  0 MSE:  0.25819653272628784 Epoch  1 MSE:  0.15166231989860535 Epoch  2 MSE:  0.20903076231479645 Epoch  3 MSE:  0.13991586863994598 Epoch  4 MSE:  0.1287676990032196 Epoch  5 MSE:  0.13563665747642517 Epoch  6 MSE:  0.13573406636714935 Epoch  7 MSE:  0.12434989213943481 Epoch  8 MSE:  0.1031389907002449 Epoch  9 MSE:  0.0779111385345459 Epoch  10 MSE:  0.06311900913715363 Epoch  11 MSE:  0.07091287523508072 Epoch  12 MSE:  0.052488457411527634 Epoch  13 MSE:  0.023740172386169434 Epoch  14 MSE:  0.01886197179555893 Epoch  15 MSE:  0.023339685052633286 Epoch  16 MSE:  0.017914501950144768 Epoch  17 MSE:  0.010528038255870342 Epoch  18 MSE:  0.020709635689854622 Epoch  19 MSE:  0.022142166271805763 Epoch  20 MSE:  0.00913378968834877 Epoch  21 MSE:  0.0032837125472724438 Epoch  22 MSE:  0.006005624774843454 Epoch  23 MSE:  0.009221922606229782 Epoch  24 MSE:  0.009343614801764488 Epoch  25 MSE:  0.007402882911264896 Epoch  26 MSE:  0.006014988292008638 Epoch  27 MSE:  0.006637887097895145 Epoch  28 MSE:  0.007978024892508984 Epoch  29 MSE:  0.007520338520407677 Epoch  30 MSE:  0.005026531405746937 Epoch  31 MSE:  0.002815255429595709 Epoch  32 MSE:  0.002627915469929576 Epoch  33 MSE:  0.0038487249985337257 Epoch  34 MSE:  0.004618681501597166 Epoch  35 MSE:  0.0039092665538191795 Epoch  36 MSE:  0.002484086435288191 Epoch  37 MSE:  0.0018542427569627762 Epoch  38 MSE:  0.002269477816298604 Epoch  39 MSE:  0.002432651352137327 Epoch  40 MSE:  0.0017461515963077545 Epoch  41 MSE:  0.001250344910658896 Epoch  42 MSE:  0.0016412724507972598 Epoch  43 MSE:  0.0022176974453032017 Epoch  44 MSE:  0.002139537362381816 Epoch  45 MSE:  0.0015903041930869222 Epoch  46 MSE:  0.0013555067125707865 Epoch  47 MSE:  0.0015729618025943637 Epoch  48 MSE:  0.0015560296596959233 Epoch  49 MSE:  0.0010814378038048744 Epoch  50 MSE:  0.0007583849364891648 Epoch  51 MSE:  0.0008783523226156831 Epoch  52 MSE:  0.0010345984483137727 Epoch  53 MSE:  0.0009140677284449339 Epoch  54 MSE:  0.0007315980619750917 Epoch  55 MSE:  0.0007604123093187809 Epoch  56 MSE:  0.0008461487013846636 Epoch  57 MSE:  0.0007311741355806589 Epoch  58 MSE:  0.0005497800884768367 Epoch  59 MSE:  0.0005577158881351352 Epoch  60 MSE:  0.0006879133288748562 Epoch  61 MSE:  0.0007144041010178626 Epoch  62 MSE:  0.0006207934347912669 Epoch  63 MSE:  0.0005688412929885089 Epoch  64 MSE:  0.0005930980551056564 Epoch  65 MSE:  0.0005647227517329156 Epoch  66 MSE:  0.00047004505177028477 Epoch  67 MSE:  0.0004422623314894736 Epoch  68 MSE:  0.0004969367873854935 Epoch  69 MSE:  0.0005126534379087389 Epoch  70 MSE:  0.00046223439858295023 Epoch  71 MSE:  0.00043931364780291915 Epoch  72 MSE:  0.0004609820316545665 Epoch  73 MSE:  0.00045188714284449816 Epoch  74 MSE:  0.0004201307019684464 Epoch  75 MSE:  0.00043058270239271224 Epoch  76 MSE:  0.0004576134087983519 Epoch  77 MSE:  0.0004446248640306294 Epoch  78 MSE:  0.0004175296053290367 Epoch  79 MSE:  0.00041787829832173884 Epoch  80 MSE:  0.00041931969462893903 Epoch  81 MSE:  0.0004008542455267161 Epoch  82 MSE:  0.0003951751277782023 Epoch  83 MSE:  0.0004096981429029256 Epoch  84 MSE:  0.00041126698488369584 Epoch  85 MSE:  0.0003979630710091442 Epoch  86 MSE:  0.0003955823485739529 Epoch  87 MSE:  0.0004000987682957202 Epoch  88 MSE:  0.0003947264631278813 Epoch  89 MSE:  0.00039027887396514416 Epoch  90 MSE:  0.00039583834586665034 Epoch  91 MSE:  0.00039670622209087014 Epoch  92 MSE:  0.0003883809840772301 Epoch  93 MSE:  0.00038467306876555085 Epoch  94 MSE:  0.00038626548484899104 Epoch  95 MSE:  0.00038355711149051785 Epoch  96 MSE:  0.0003808493784163147 Epoch  97 MSE:  0.00038343065534718335 Epoch  98 MSE:  0.0003837483818642795 Epoch  99 MSE:  0.00037956441519781947 Training time: 8.352406024932861   predict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy())) original = pd.DataFrame(scaler.inverse_transform(y_train_lstm.detach().numpy()))   import seaborn as sns sns.set_style(\"darkgrid\")      fig = plt.figure() fig.subplots_adjust(hspace=0.2, wspace=0.2)  plt.subplot(1, 2, 1) ax = sns.lineplot(x = original.index, y = original[0], label=\"Data\", color='royalblue') ax = sns.lineplot(x = predict.index, y = predict[0], label=\"Training Prediction (LSTM)\", color='tomato') ax.set_title('Stock price', size = 14, fontweight='bold') ax.set_xlabel(\"Days\", size = 14) ax.set_ylabel(\"Cost (USD)\", size = 14) ax.set_xticklabels('', size=10)   plt.subplot(1, 2, 2) ax = sns.lineplot(data=hist, color='royalblue') ax.set_xlabel(\"Epoch\", size = 14) ax.set_ylabel(\"Loss\", size = 14) ax.set_title(\"Training Loss\", size = 14, fontweight='bold') fig.set_figheight(6) fig.set_figwidth(16)      import math, time from sklearn.metrics import mean_squared_error  # make predictions y_test_pred = model(x_test)  # invert predictions y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy()) y_train = scaler.inverse_transform(y_train_lstm.detach().numpy()) y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy()) y_test = scaler.inverse_transform(y_test_lstm.detach().numpy())  # calculate root mean squared error trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0])) print('Train Score: %.2f RMSE' % (trainScore)) testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0])) print('Test Score: %.2f RMSE' % (testScore)) lstm.append(trainScore) lstm.append(testScore) lstm.append(training_time)   Train Score: 1.65 RMSE Test Score: 5.56 RMSE   # shift train predictions for plotting trainPredictPlot = np.empty_like(price) trainPredictPlot[:, :] = np.nan trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred  # shift test predictions for plotting testPredictPlot = np.empty_like(price) testPredictPlot[:, :] = np.nan testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred  original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))  predictions = np.append(trainPredictPlot, testPredictPlot, axis=1) predictions = np.append(predictions, original, axis=1) result = pd.DataFrame(predictions)   import plotly.express as px import plotly.graph_objects as go  fig = go.Figure() fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],                     mode='lines',                     name='Train prediction'))) fig.add_trace(go.Scatter(x=result.index, y=result[1],                     mode='lines',                     name='Test prediction')) fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],                     mode='lines',                     name='Actual Value'))) fig.update_layout(     xaxis=dict(         showline=True,         showgrid=True,         showticklabels=False,         linecolor='white',         linewidth=2     ),     yaxis=dict(         title_text='Close (USD)',         titlefont=dict(             family='Rockwell',             size=12,             color='white',         ),         showline=True,         showgrid=True,         showticklabels=True,         linecolor='white',         linewidth=2,         ticks='outside',         tickfont=dict(             family='Rockwell',             size=12,             color='white',         ),     ),     showlegend=True,     template = 'plotly_dark'  )    annotations = [] annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,                               xanchor='left', yanchor='bottom',                               text='Results (LSTM)',                               font=dict(family='Rockwell',                                         size=26,                                         color='white'),                               showarrow=False)) fig.update_layout(annotations=annotations)  fig.show()     The model behaves well with the training set, and it also has solid performace with the test set. The model is probably overfitting, especially taking into consideration that the loss is minimal after the 40th epoch.   GRU   class GRU(nn.Module):     def __init__(self, input_dim, hidden_dim, num_layers, output_dim):         super(GRU, self).__init__()         self.hidden_dim = hidden_dim         self.num_layers = num_layers                  self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)         self.fc = nn.Linear(hidden_dim, output_dim)      def forward(self, x):         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()         out, (hn) = self.gru(x, (h0.detach()))         out = self.fc(out[:, -1, :])          return out   model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers) criterion = torch.nn.MSELoss(reduction='mean') optimiser = torch.optim.Adam(model.parameters(), lr=0.01)   hist = np.zeros(num_epochs) start_time = time.time() gru = []  for t in range(num_epochs):     y_train_pred = model(x_train)      loss = criterion(y_train_pred, y_train_gru)     print(\"Epoch \", t, \"MSE: \", loss.item())     hist[t] = loss.item()      optimiser.zero_grad()     loss.backward()     optimiser.step()  training_time = time.time()-start_time     print(\"Training time: {}\".format(training_time))   Epoch  0 MSE:  0.3607046902179718 Epoch  1 MSE:  0.13102510571479797 Epoch  2 MSE:  0.19682788848876953 Epoch  3 MSE:  0.12917079031467438 Epoch  4 MSE:  0.075187087059021 Epoch  5 MSE:  0.07155207544565201 Epoch  6 MSE:  0.06979498267173767 Epoch  7 MSE:  0.045164529234170914 Epoch  8 MSE:  0.010227248072624207 Epoch  9 MSE:  0.007098929025232792 Epoch  10 MSE:  0.03696290776133537 Epoch  11 MSE:  0.023822534829378128 Epoch  12 MSE:  0.007158784195780754 Epoch  13 MSE:  0.011737179011106491 Epoch  14 MSE:  0.018340380862355232 Epoch  15 MSE:  0.01571972481906414 Epoch  16 MSE:  0.007737305015325546 Epoch  17 MSE:  0.0022043471690267324 Epoch  18 MSE:  0.003535511204972863 Epoch  19 MSE:  0.009062006138265133 Epoch  20 MSE:  0.011892840266227722 Epoch  21 MSE:  0.009278996847569942 Epoch  22 MSE:  0.004890399053692818 Epoch  23 MSE:  0.00290724472142756 Epoch  24 MSE:  0.0037367662880569696 Epoch  25 MSE:  0.005075107794255018 Epoch  26 MSE:  0.0048552630469202995 Epoch  27 MSE:  0.0029767893720418215 Epoch  28 MSE:  0.0011302019702270627 Epoch  29 MSE:  0.0010998218785971403 Epoch  30 MSE:  0.0027486730832606554 Epoch  31 MSE:  0.003973710350692272 Epoch  32 MSE:  0.0033208823297172785 Epoch  33 MSE:  0.001725937006995082 Epoch  34 MSE:  0.0009095442364923656 Epoch  35 MSE:  0.0012437441619113088 Epoch  36 MSE:  0.001808099914342165 Epoch  37 MSE:  0.001749989576637745 Epoch  38 MSE:  0.0011168664786964655 Epoch  39 MSE:  0.0006078396691009402 Epoch  40 MSE:  0.0007392823463305831 Epoch  41 MSE:  0.0012739860685542226 Epoch  42 MSE:  0.001527499407529831 Epoch  43 MSE:  0.0012058777501806617 Epoch  44 MSE:  0.0006985657382756472 Epoch  45 MSE:  0.0005090595805086195 Epoch  46 MSE:  0.0006709578447043896 Epoch  47 MSE:  0.0008276336011476815 Epoch  48 MSE:  0.0007188778254203498 Epoch  49 MSE:  0.00046144291991367936 Epoch  50 MSE:  0.00036167059442959726 Epoch  51 MSE:  0.0005161706358194351 Epoch  52 MSE:  0.0006932021933607757 Epoch  53 MSE:  0.0006540967733599246 Epoch  54 MSE:  0.00046293693594634533 Epoch  55 MSE:  0.0003537530137691647 Epoch  56 MSE:  0.0004015856538899243 Epoch  57 MSE:  0.0004694756935350597 Epoch  58 MSE:  0.0004315198748372495 Epoch  59 MSE:  0.00033121410524472594 Epoch  60 MSE:  0.0002968600601889193 Epoch  61 MSE:  0.00035973641206510365 Epoch  62 MSE:  0.00042053856304846704 Epoch  63 MSE:  0.00039580956217832863 Epoch  64 MSE:  0.00032569453469477594 Epoch  65 MSE:  0.000298373750410974 Epoch  66 MSE:  0.0003244410618208349 Epoch  67 MSE:  0.00034042992047034204 Epoch  68 MSE:  0.00031064936774782836 Epoch  69 MSE:  0.0002729821135289967 Epoch  70 MSE:  0.00027573812985792756 Epoch  71 MSE:  0.0003066273347940296 Epoch  72 MSE:  0.0003163278743159026 Epoch  73 MSE:  0.0002932495262939483 Epoch  74 MSE:  0.00027294218307361007 Epoch  75 MSE:  0.0002778717316687107 Epoch  76 MSE:  0.0002882281842175871 Epoch  77 MSE:  0.0002800409565679729 Epoch  78 MSE:  0.0002621126768644899 Epoch  79 MSE:  0.0002582546148914844 Epoch  80 MSE:  0.000269515992840752 Epoch  81 MSE:  0.0002754448796622455 Epoch  82 MSE:  0.0002673306444194168 Epoch  83 MSE:  0.0002585184993222356 Epoch  84 MSE:  0.0002598936844151467 Epoch  85 MSE:  0.0002639705198816955 Epoch  86 MSE:  0.00026010669535025954 Epoch  87 MSE:  0.0002519670524634421 Epoch  88 MSE:  0.0002499922120478004 Epoch  89 MSE:  0.0002542092406656593 Epoch  90 MSE:  0.0002556447288952768 Epoch  91 MSE:  0.0002515747328288853 Epoch  92 MSE:  0.0002483536081854254 Epoch  93 MSE:  0.0002494044601917267 Epoch  94 MSE:  0.0002502511197235435 Epoch  95 MSE:  0.00024729460710659623 Epoch  96 MSE:  0.00024373934138566256 Epoch  97 MSE:  0.00024349824525415897 Epoch  98 MSE:  0.000244816328631714 Epoch  99 MSE:  0.0002440418174955994 Training time: 8.153863906860352   predict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy())) original = pd.DataFrame(scaler.inverse_transform(y_train_gru.detach().numpy()))   import seaborn as sns sns.set_style(\"darkgrid\")      fig = plt.figure() fig.subplots_adjust(hspace=0.2, wspace=0.2)  plt.subplot(1, 2, 1) ax = sns.lineplot(x = original.index, y = original[0], label=\"Data\", color='royalblue') ax = sns.lineplot(x = predict.index, y = predict[0], label=\"Training Prediction (GRU)\", color='tomato') ax.set_title('Stock price', size = 14, fontweight='bold') ax.set_xlabel(\"Days\", size = 14) ax.set_ylabel(\"Cost (USD)\", size = 14) ax.set_xticklabels('', size=10)   plt.subplot(1, 2, 2) ax = sns.lineplot(data=hist, color='royalblue') ax.set_xlabel(\"Epoch\", size = 14) ax.set_ylabel(\"Loss\", size = 14) ax.set_title(\"Training Loss\", size = 14, fontweight='bold') fig.set_figheight(6) fig.set_figwidth(16)      import math, time from sklearn.metrics import mean_squared_error  # make predictions y_test_pred = model(x_test)  # invert predictions y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy()) y_train = scaler.inverse_transform(y_train_gru.detach().numpy()) y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy()) y_test = scaler.inverse_transform(y_test_gru.detach().numpy())  # calculate root mean squared error trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0])) print('Train Score: %.2f RMSE' % (trainScore)) testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0])) print('Test Score: %.2f RMSE' % (testScore)) gru.append(trainScore) gru.append(testScore) gru.append(training_time)   Train Score: 1.32 RMSE Test Score: 4.90 RMSE   # shift train predictions for plotting trainPredictPlot = np.empty_like(price) trainPredictPlot[:, :] = np.nan trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred  # shift test predictions for plotting testPredictPlot = np.empty_like(price) testPredictPlot[:, :] = np.nan testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred  original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))  predictions = np.append(trainPredictPlot, testPredictPlot, axis=1) predictions = np.append(predictions, original, axis=1) result = pd.DataFrame(predictions)   import plotly.express as px import plotly.graph_objects as go  fig = go.Figure() fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],                     mode='lines',                     name='Train prediction'))) fig.add_trace(go.Scatter(x=result.index, y=result[1],                     mode='lines',                     name='Test prediction')) fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],                     mode='lines',                     name='Actual Value'))) fig.update_layout(     xaxis=dict(         showline=True,         showgrid=True,         showticklabels=False,         linecolor='white',         linewidth=2     ),     yaxis=dict(         title_text='Close (USD)',         titlefont=dict(             family='Rockwell',             size=12,             color='white',         ),         showline=True,         showgrid=True,         showticklabels=True,         linecolor='white',         linewidth=2,         ticks='outside',         tickfont=dict(             family='Rockwell',             size=12,             color='white',         ),     ),     showlegend=True,     template = 'plotly_dark'  )    annotations = [] annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,                               xanchor='left', yanchor='bottom',                               text='Results (GRU)',                               font=dict(family='Rockwell',                                         size=26,                                         color='white'),                               showarrow=False)) fig.update_layout(annotations=annotations)  fig.show()   lstm = pd.DataFrame(lstm, columns=['LSTM']) gru = pd.DataFrame(gru, columns=['GRU']) result = pd.concat([lstm, gru], axis=1, join='inner') result.index = ['Train RMSE', 'Test RMSE', 'Train Time'] result                               LSTM       GRU                       Train RMSE       1.648017       1.321451                 Test RMSE       5.562765       4.904847                 Train Time       8.352406       8.153864               Conclusion  Both models demonstrate commendable performance during the training phase, but their progress seems to plateau around the 40th epoch, indicating that the predefined 100 epochs may not be necessary.   In line with our expectations, the GRU neural network excelled over the LSTM in terms of accuracy, achieving a lower mean square error (in both training and, crucially, in the test set) and faster processing speed.   ","categories": ["blog"],
        "tags": ["pytorch","news"],
        "url": "http://localhost:4000/blog/Stock-Predcition/",
        "teaser": null
      }]
