var store = [{
        "title": "Live Corona Map",
        "excerpt":"Live Corona Map is a COVID-19 data visuaization and dashboard project I joined in the April of 2020 as a Python developer.   About the Project   When reported cases of COVID-19 in Korea started to erupt in late Febuary and early March, there existed no centrailzed platform that provided up-to-date, reliable information to the general public. As a matter of fact, even government entities were poorly equipped to flexibly respond to the pandemic. Live Corona Map was created at this critical point in time to provide a trustworthy channel of data to reduce misinformation and the asymmetry of information. It was also used by government agencies—most notably the Jeju Self-governing Provincial Government—to monitor the status quo. Even today, the link redirecting users to Live Corona Map is available on the website of the Jeju Provincial Government.   Given the public nature of Live Corona Map, perhaps it is unsurprising that te code base for this project is entirely open-source. Until recently, the web server for the website was sponsored by Naver. It is also worth mentioning that the project is run entirely by volunteers and civic hackers who commit their time without any form of renumeration.   Contributions   When I joined the team in April, the project was at a transition phase: reported cases were somewhat starting to drop, as was the volume of user traffic to the website. However, the number of COVID-19 cases started to spike up again in May and June, making it necessary to maintain and improve the web application.   Summarized below is a list of my contributions as a Python developer, which would not have been possible without the support and help of my team members.   Web Crawlers   The bulk of my work revolved around writing and debugging Python web crawlers. Although there were scripts for different web crawlers, a lot of them were either missing or broken. When I joined the team, people were hand-scraping data from websites instead of relying on automation, saying that web crawlers were highly unreliable.   Using bs4, pandas, and other libraries, I rewrote and engineered 8 web crawlers to make them fully functional and more robust. We were then able to use GitHub Actions CI to schedule automatic web scraping. This vastly improved the reliability and timeliness of the data and graphics posted on the website.   Refactoring   Instead of the more conventional SQL-flavored database, the website directly saves web craped data as .js files, hosted directly on GitHub. The reason behind this seemingly atypical choice is that we want to share the data with other groups and developers—after all, this is the very spirit of open-source.   The issue I saw was that a lot of the code was repeated. For instance, all Python crawler scripts basically used the same routine to create a bs4.BeautifulSoup object using requests. The same logic was also used for other operations, most notably encoding Python dictionaries as JSON files for use later on the client-side.   I created a utils.py file to reduce unnecessary boilerplate and performed some micro-optimizations to streamline the codebase. This vastly improved code readability across the Python portion of the repository.   News Scraping   A new feature I added to the website was the “Major Headlines” widget. Previously, the website only displayed numerical, qualitative data. However, I thought it would be informative and helpful to dispay the hottest news headlines regarding COVID-19 so that interested users could be redirected to media websites.   Adding this functionality required use of not only Python for scraping and saving data, but also Javascript for rendering it on the website. It was a challenging, yet also an incrediby rewarding experience.      Daily Graph   I added a graph to the website that displays per-day increases in reported cases of COVID-19. This was not an entirely new graph; however, the graph was somewhat ineffective in presenting information since the x-axis was fixed to start from Febuary 10th. Due to the huge spikes in March—at one point, the estimate is over 400 on a single day—more recent cases of 40 to 50 patients on the daily were completely overshadowed in the visualization.   Using simple Javascript and chart.js, a team member and I added a new visualization that displays displays this information for only the last 10 days. I also plan on adding an exponential moving average estimate to smoothen out the graphs we already have.   More   You can visit the GitHub repository for the website through this link. Feel free to submit a pull request if you find any bugs if there are improvements you want to make yourself!   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/coronamap/",
        "teaser": "http://localhost:4000/assets/images/portfolio/coronamap-th.png"
      },{
        "title": "ML from Scratch",
        "excerpt":"For more information, visit this link.   (This is an unfinished page.)  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/ml-from-scratch/",
        "teaser": null
      },{
        "title": "PPETrackr",
        "excerpt":"For more information, visit this link.   (This is an unfinished page.)   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/ppetrackr/",
        "teaser": null
      },{
        "title": "PX Chatbot",
        "excerpt":"For more information, visit this link.   (This is an unfinished page.)  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/px-chatbot/",
        "teaser": null
      },{
        "title": "Shape of You",
        "excerpt":"For more information, visit this link.   (This is an unfinished page.)   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/shape-of-you/",
        "teaser": null
      },{
        "title": "A Self-Created Curriculum",
        "excerpt":"My Machine Learning journey  Welcome to my blog! I’m embarking on a journey to learn Machine Learning, and I’m excited to share my experiences here. Follow me as I explore the world of Machine Learning algorithms and technologies!   My First Steps  I’m starting my journey by exploring the fundamentals of Machine Learning. I’m brushing up on my math and programming skills, and I’m getting familiar with the different types of algorithms and techniques. I’m also reading up on the latest Machine Learning research.   My Next Steps  My next steps involve diving deeper into the various techniques and algorithms. I’m also exploring different types of Machine Learning tools and libraries, and I’m working on some basic projects to get hands-on experience.   Update 01/01/2023:  My first update: I have started this blog a little bit late as I am in the Sequential Models Course of Andrew Ngâ€™s Deep Learning Specialization. Today I got bored of watching lectures and wanted to code so I implemented MNIST with TensorFlow. I watched the first Stanford CS244N Lecuture on NLP and realized that I just need to write code so I went through a build a house price prediction model for a Kaggle Competition.  ","categories": ["blog"],
        "tags": ["update"],
        "url": "http://localhost:4000/blog/studying-deep-learning/",
        "teaser": null
      },{
        "title": "Math For Machine Learning Notes",
        "excerpt":"Linear Algebra Review   1. Vectors and Matrices  Vectors and matrices are fundamental objects in linear algebra. A vector can be thought of as a list of numbers, and a matrix as a grid of numbers arranged in rows and columns. For example:   Vector: v = [1, 2, 3] Matrix: M = [1 2              3 4              5 6]   2. Matrix Operations   2.1 Addition  We add two matrices by adding their corresponding entries:   [1 2]     [4 5]     [1+4 2+5]     [5 7] [3 4]  +  [6 7]  =  [3+6 4+7]  =  [9 11]   2.2 Subtraction  Similar to addition, we subtract two matrices by subtracting their corresponding entries:   [5 7]     [4 5]     [5-4 7-5]     [1 2] [9 11]  - [6 7]  =  [9-6 11-7] =  [3 4]   2.3 Multiplication  Multiplication is a bit more complex. You multiply the elements of the rows of the first matrix by the elements of the columns of the second, and then add them up.   [1 2]     [5 6]     [(1*5+2*7) (1*6+2*8)]     [19 22] [3 4]  x  [7 8]  =  [(3*5+4*7) (3*6+4*8)]  =  [43 50]   3. Vector Spaces   A vector space is a set of vectors that can be added together and multiplied by scalars (real or complex numbers) in such a way that these operations obey certain axioms. For example, the set of all 2D vectors forms a vector space.   4. Linear Transformations and Matrices   Linear transformations are a way of ‘transforming’ one vector into another while preserving the operations of addition and scalar multiplication. For example, scaling and rotating vectors are both linear transformations. Every linear transformation can be represented by a matrix. If v is a vector and A is the matrix representing a linear transformation, the transformed vector is obtained by the matrix-vector product Av.   5. Eigenvalues and Eigenvectors   Given a square matrix A, if there is a non-zero vector v such that multiplying A by v gives a vector that’s a scaled version of v, then v is an eigenvector of A and the scaling factor is the corresponding eigenvalue.   If Av = λv, then v is an eigenvector of A and λ is the corresponding eigenvalue. For instance:   Let A = [4 1]          [2 3]  Then, for v = [1]                [2]  We have Av = [4*1 + 1*2]              [2*1 + 3*2]            = [6]             [8]  which is 2*v, so λ = 2.   6. Diagonalization   A matrix A is diagonalizable if we can   write it as A = PDP^(-1) where D is a diagonal matrix, and P is a matrix whose columns are the eigenvectors of A.   7. Orthogonal and Unitary Matrices   An orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors). Orthogonal matrices have a lovely property: the inverse of an orthogonal matrix is its transpose.   8. Dot Product and Norms   The dot product of two vectors a = [a1, a2, ..., an] and b = [b1, b2, ..., bn] is defined as a1*b1 + a2*b2 + ... + an*bn.                  The norm of a vector v (often interpreted as the length of the vector) is the square root of the dot product of the vector with itself, and is denoted as               v               .           9. Projections   The projection of a vector y onto another vector x is the vector in the direction of x that best approximates y. If y is projected onto x to get the projection vector p, then y - p is orthogonal to x.   10. Singular Value Decomposition (SVD)   Every matrix A can be decomposed into the product of three matrices U, Σ, and V^T, where U and V are orthogonal matrices and Σ is a diagonal matrix. The diagonal entries of Σ are the singular values of A.   11. Linear Systems of Equations   Many problems in linear algebra boil down to solving a system of linear equations, which can often be represented in matrix form as Ax = b. Gaussian elimination and similar methods can be used to solve these systems.   Probability and Statistics Review   1. Probability Basics   1.1 Sample Space and Events  A sample space is the set of all possible outcomes of an experiment. An event is a subset of the sample space.   1.2 Probability of an Event  The probability of an event A, denoted by P(A), is a number between 0 and 1 inclusive that measures the likelihood of the occurrence of the event.   1.3 Probability Axioms     For any event A, 0 &lt;= P(A) &lt;= 1.   P(S) = 1, where S is the sample space.   If A1, A2, A3, … are disjoint events, then P(A1 ∪ A2 ∪ A3 ∪ …) = P(A1) + P(A2) + P(A3) + …   2. Conditional Probability                  Conditional probability is the probability of an event given that another event has occurred. If we denote A and B as two events, the conditional probability of A given that B has occurred is written as P(A       B).           3. Random Variables   A random variable is a function that assigns a real number to each outcome in a sample space.   4. Probability Distributions   A probability distribution assigns a probability to each possible value of the random variable. A probability distribution is described by a probability density function (pdf) for continuous variables or a probability mass function (pmf) for discrete variables.   4.1 Common Distributions     Uniform Distribution   Normal (Gaussian) Distribution   Binomial Distribution   Poisson Distribution   Exponential Distribution   5. Expected Value and Variance   The expected value (mean) of a random variable is the long-run average value of the outcomes. The variance measures how spread out the values are around the expected value.   6. Covariance and Correlation   Covariance and correlation are measures of the relationship between two random variables. Correlation is a normalized form of covariance and provides a measure between -1 and 1 of how related two variables are.   7. Hypothesis Testing   Hypothesis testing is a method for testing a claim or hypothesis about a parameter in a population, using data measured in a sample.   7.1 Null and Alternative Hypotheses  The null hypothesis (H0) is a statement about the population parameter that implies no effect or difference, while the alternative hypothesis (Ha) is the statement being tested.   7.2 p-value  The p-value is the probability of obtaining the observed data (or data more extreme) if the null hypothesis is true. A smaller p-value provides stronger evidence against the null hypothesis.   7.3 Significance Level  The significance level (often denoted by α) is the probability of rejecting the null hypothesis when it is true. It’s a threshold used to determine when the null hypothesis can be rejected.   8. Confidence Intervals   A confidence interval provides an estimated range of values which is likely to include an unknown population parameter. The confidence level describes the uncertainty of this range.   ","categories": ["blog"],
        "tags": ["study","cs229"],
        "url": "http://localhost:4000/blog/2023-Machine-Learning-Math-Review/",
        "teaser": null
      },{
        "title": "MNIST From Scratch Using NumPy",
        "excerpt":"To gain a deeper understanding of what is happening in Neural Networks I decided I wanted to complete a project froms scratch without the help of Pytorch or Tensorflow. The best part about machine learning is the easiest way to learn is by training models! Therefore I am going to train as many as I possibly can throughout this journey to master Artifical Intelligence!   This is the training process I will be going through:      This is the dataset I will be using:      Code Implementation   Prep Data   import numpy as np import matplotlib.pyplot as plt  def load_data(path):     def one_hot(y):         table = np.zeros((y.shape[0], 10))         for i in range(y.shape[0]):             table[i][int(y[i][0])] = 1          return table      def normalize(x):          x = x / 255         return x       data = np.loadtxt('{}'.format(path), delimiter = ',', skiprows=1)     return normalize(data[:,1:]),one_hot(data[:,:1])   X_train, y_train = load_data('mnist_train.csv') X_test, y_test = load_data('mnist_test.csv')   Setup Neural Network   import seaborn as sns from sklearn.metrics import confusion_matrix  class NeuralNetwork:     def __init__(self, X, y, hidden_sizes=(256, 128), batch=64, lr=.008, epochs=75):         self.X_train, self.y_train = X, y  # training data          # Parameters         self.batch = batch         self.lr = lr         self.epochs = epochs          # Initialize weights and biases         input_dim = X.shape[1]         output_dim = y.shape[1]         self.sizes = [input_dim, *hidden_sizes, output_dim]          self.weights = []         self.biases = []         for i in range(len(self.sizes) - 1):             # Xavier/Glorot Initialization for weights             bound = np.sqrt(6. / (self.sizes[i] + self.sizes[i+1]))             self.weights.append(np.random.uniform(-bound, bound, (self.sizes[i], self.sizes[i+1])))             self.biases.append(np.zeros((1, self.sizes[i+1])))          # List to store loss and accuracy history         self.train_loss, self.train_acc = [], []      def ReLU(self, x):         return np.maximum(0, x)      def dReLU(self, x):         return (x &gt; 0).astype(float)      def softmax(self, x):         e_x = np.exp(x - np.max(x, axis=1, keepdims=True))         return e_x / np.sum(e_x, axis=1, keepdims=True)      def cross_entropy(self, pred, real):         n_samples = real.shape[0]         res = pred - real         return res/n_samples      def error(self, y_pred, y_real):         return -np.sum(y_real * np.log(y_pred))      def feedforward(self, X):         self.a = [X]         self.z = []         for i in range(len(self.sizes) - 2):             self.z.append(np.dot(self.a[-1], self.weights[i]) + self.biases[i])             self.a.append(self.ReLU(self.z[-1]))         self.z.append(np.dot(self.a[-1], self.weights[-1]) + self.biases[-1])         self.a.append(self.softmax(self.z[-1]))         return self.a[-1]          def backprop(self, y):         m = y.shape[0]         dw = []  # dC/dW         db = []  # dC/dB         dz = [self.cross_entropy(self.a[-1], y)]  # dC/dz           # loop through each layer in reverse order         for i in reversed(range(len(self.sizes) - 1)):             dw.append(np.dot(self.a[i].T, dz[-1]) / m)             db.append(np.sum(dz[-1], axis=0, keepdims=True) / m)             if i &gt; 0:  # Skip dz for input layer                 da = np.dot(dz[-1], self.weights[i].T)                 dz.append(da * self.dReLU(self.z[i-1]))          # Reverse lists since we computed in reverse         self.dw = dw[::-1]         self.db = db[::-1]      def update_weights(self):         for i in range(len(self.sizes) - 1):             self.weights[i] -= self.lr * self.dw[i]             self.biases[i] -= self.lr * self.db[i]      def train(self):         for epoch in range(self.epochs):             # Forward and Backward pass for each batch             for i in range(0, self.X_train.shape[0], self.batch):                 X = self.X_train[i:i+self.batch]                 y = self.y_train[i:i+self.batch]                 self.feedforward(X)                 self.backprop(y)                 self.update_weights()              # Save and print loss and accuracy at the end of each epoch             train_pred = self.feedforward(self.X_train)             train_loss = self.error(train_pred, self.y_train)             self.train_loss.append(train_loss)             train_acc = np.mean(np.argmax(self.y_train, axis=1) == np.argmax(train_pred, axis=1))             self.train_acc.append(train_acc)             print(f\"Epoch {epoch+1}/{self.epochs} - loss: {train_loss:.4f} - acc: {train_acc:.4f}\")       def plot_loss(self):         plt.figure(figsize=(10, 6))         plt.plot(self.train_loss, label='Train Loss')         plt.title(\"Loss vs. Epochs\")         plt.xlabel(\"Epochs\")         plt.ylabel(\"Loss\")         plt.legend()         plt.grid(True)         plt.show()      def plot_acc(self):         plt.figure(figsize=(10, 6))         plt.plot(self.train_acc, label='Train Accuracy')         plt.title(\"Accuracy vs. Epochs\")         plt.xlabel(\"Epochs\")         plt.ylabel(\"Accuracy\")         plt.legend()         plt.grid(True)         plt.show()      def plot_confusion_matrix(self, X_test, y_test):         y_pred = self.feedforward(X_test)         y_pred = np.argmax(y_pred, axis=1)         y_true = np.argmax(y_test, axis=1)          plt.figure(figsize=(10, 6))         cm = confusion_matrix(y_true, y_pred)         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')         plt.title(\"Confusion Matrix\")         plt.xlabel(\"Predicted Label\")         plt.ylabel(\"True Label\")         plt.show()      def test(self, X_test, y_test):         y_pred = self.feedforward(X_test)         acc = np.mean(np.argmax(y_test, axis=1) == np.argmax(y_pred, axis=1))         print(\"Test Accuracy: {:.2f}%\".format(100 * acc))  # Assume you have training data X_train, y_train and test data X_test, y_test. NN = NeuralNetwork(X_train, y_train) NN.train() NN.plot_loss() NN.plot_acc() NN.test(X_test, y_test)    Epoch 1/75 - loss: 137129.5682 - acc: 0.1268 Epoch 2/75 - loss: 131418.5482 - acc: 0.2510 Epoch 3/75 - loss: 126235.7899 - acc: 0.3750 Epoch 4/75 - loss: 121282.3307 - acc: 0.4682 Epoch 5/75 - loss: 116405.4435 - acc: 0.5308 Epoch 6/75 - loss: 111541.0515 - acc: 0.5762 Epoch 7/75 - loss: 106677.8296 - acc: 0.6109 Epoch 8/75 - loss: 101828.8328 - acc: 0.6406 Epoch 9/75 - loss: 97028.6175 - acc: 0.6657 Epoch 10/75 - loss: 92318.4315 - acc: 0.6870 Epoch 11/75 - loss: 87754.4193 - acc: 0.7048 Epoch 12/75 - loss: 83385.3926 - acc: 0.7205 Epoch 13/75 - loss: 79251.8938 - acc: 0.7342 Epoch 14/75 - loss: 75378.7727 - acc: 0.7472 Epoch 15/75 - loss: 71773.3357 - acc: 0.7584 Epoch 16/75 - loss: 68437.6353 - acc: 0.7689 Epoch 17/75 - loss: 65362.9476 - acc: 0.7790 Epoch 18/75 - loss: 62536.8781 - acc: 0.7874 Epoch 19/75 - loss: 59943.4454 - acc: 0.7947 Epoch 20/75 - loss: 57565.2254 - acc: 0.8012 Epoch 21/75 - loss: 55384.5291 - acc: 0.8075 Epoch 22/75 - loss: 53382.6301 - acc: 0.8124 Epoch 23/75 - loss: 51543.0992 - acc: 0.8176 Epoch 24/75 - loss: 49850.6227 - acc: 0.8219 Epoch 25/75 - loss: 48290.0803 - acc: 0.8257 Epoch 26/75 - loss: 46848.8197 - acc: 0.8291 Epoch 27/75 - loss: 45515.0078 - acc: 0.8325 Epoch 28/75 - loss: 44278.3199 - acc: 0.8354 Epoch 29/75 - loss: 43129.9729 - acc: 0.8380 Epoch 30/75 - loss: 42061.1922 - acc: 0.8404 Epoch 31/75 - loss: 41065.1078 - acc: 0.8430 Epoch 32/75 - loss: 40135.0729 - acc: 0.8456 Epoch 33/75 - loss: 39265.3970 - acc: 0.8477 Epoch 34/75 - loss: 38450.5495 - acc: 0.8498 Epoch 35/75 - loss: 37685.9770 - acc: 0.8516 Epoch 36/75 - loss: 36967.8159 - acc: 0.8534 Epoch 37/75 - loss: 36291.8714 - acc: 0.8550 Epoch 38/75 - loss: 35654.2947 - acc: 0.8566 Epoch 39/75 - loss: 35052.2173 - acc: 0.8580 Epoch 40/75 - loss: 34482.9341 - acc: 0.8598 Epoch 41/75 - loss: 33943.8488 - acc: 0.8615 Epoch 42/75 - loss: 33432.6360 - acc: 0.8630 Epoch 43/75 - loss: 32947.0680 - acc: 0.8644 Epoch 44/75 - loss: 32485.4873 - acc: 0.8657 Epoch 45/75 - loss: 32046.1996 - acc: 0.8672 Epoch 46/75 - loss: 31627.5566 - acc: 0.8687 Epoch 47/75 - loss: 31228.1234 - acc: 0.8698 Epoch 48/75 - loss: 30846.5072 - acc: 0.8710 Epoch 49/75 - loss: 30481.5252 - acc: 0.8721 Epoch 50/75 - loss: 30132.1861 - acc: 0.8730 Epoch 51/75 - loss: 29797.4417 - acc: 0.8741 Epoch 52/75 - loss: 29476.4986 - acc: 0.8750 Epoch 53/75 - loss: 29168.3811 - acc: 0.8758 Epoch 54/75 - loss: 28872.4349 - acc: 0.8767 Epoch 55/75 - loss: 28587.9371 - acc: 0.8776 Epoch 56/75 - loss: 28314.2709 - acc: 0.8784 Epoch 57/75 - loss: 28050.7225 - acc: 0.8793 Epoch 58/75 - loss: 27796.8310 - acc: 0.8801 Epoch 59/75 - loss: 27552.0479 - acc: 0.8808 Epoch 60/75 - loss: 27315.8496 - acc: 0.8816 Epoch 61/75 - loss: 27087.7822 - acc: 0.8822 Epoch 62/75 - loss: 26867.4438 - acc: 0.8829 Epoch 63/75 - loss: 26654.5043 - acc: 0.8834 Epoch 64/75 - loss: 26448.5684 - acc: 0.8840 Epoch 65/75 - loss: 26249.2325 - acc: 0.8849 Epoch 66/75 - loss: 26056.2143 - acc: 0.8855 Epoch 67/75 - loss: 25869.1442 - acc: 0.8863 Epoch 68/75 - loss: 25687.7101 - acc: 0.8869 Epoch 69/75 - loss: 25511.7632 - acc: 0.8876 Epoch 70/75 - loss: 25340.9962 - acc: 0.8881 Epoch 71/75 - loss: 25175.1319 - acc: 0.8885 Epoch 72/75 - loss: 25014.0458 - acc: 0.8890 Epoch 73/75 - loss: 24857.4101 - acc: 0.8895 Epoch 74/75 - loss: 24704.9976 - acc: 0.8898 Epoch 75/75 - loss: 24556.6116 - acc: 0.8904         Test Accuracy: 89.78%   NN.plot_confusion_matrix(X_test, y_test)       Conclusion   This was a fun project where I created a 3-layer Neural-Net only using Numpy! The model performed very well. It finished with 89.78% accuracy! I found it very intresting that the model’s most common mistake was confusing 5’s for 3’s! The confusion matrix does confirm that model was very consistent on all numbers!   Being able to see the model at such a low level allowed me to look at Neural Nets from a mathmatical standpoint which helped me understand the “Black-Box” in an insightful way! Matrix Multiplcations truly are incredible!   ","categories": ["blog"],
        "tags": ["Numpy","project"],
        "url": "http://localhost:4000/blog/MNIST-WITH-NUMPY/",
        "teaser": null
      },{
        "title": "Fake News Detection",
        "excerpt":"The problem at hand is called fake news detection. In the context of information technology and AI, “fake news” is misleading or false information presented as true news. With the rise of social media and online platforms, the spread of fake news has been prevalent. It can be harmful in many ways, like influencing public opinion based on false information or causing unnecessary panic and confusion among people.   The AI technique used to solve this problem falls under the domain of Natural Language Processing (NLP) which is a subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. The goal of NLP is to read, decipher, understand, and make sense of human language in a valuable way.   Specifically, the machine learning model used here is called the PassiveAggressiveClassifier. This is a type of online learning algorithm. The online learning model is very suitable for large scale learning problems, and it’s quite useful when we have a large stream of incoming data, where it’s not feasible to train over the entire data set.   The PassiveAggressiveClassifier is part of a family of algorithms for large-scale learning. It’s very similar to the Perceptron in that it does not require a learning rate. However, it does include a regularization parameter.   In layman terms, this is an algorithm that remains ‘passive’ when dealing with an outcome that has been correctly classified but turns ‘aggressive’ in the event of a miscalculation, updating and adjusting itself to avoid the mistake in the future.   The specific tasks it’s used for here include:   Text Feature Extraction: Before we feed the text into a machine learning model, we have to convert it into some kind of numeric representation that the model can understand. This is where CountVectorizer comes in. It’s a method used to convert the text data into a matrix of token counts.   Text Classification: This is the task of predicting the class (i.e., category) of a given piece of text. Here, we use it to predict whether a given piece of news is “real” or “fake”. The PassiveAggressiveClassifier is particularly well-suited to this task because it can efficiently handle large amounts of data and provide accurate predictions.   Step 1: Import Necessary Libraries   import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import PassiveAggressiveClassifier from sklearn.metrics import accuracy_score, confusion_matrix import matplotlib.pyplot as plt import seaborn as sns   Step 2: Load the Data   # Read the data df = pd.read_csv('train.csv')   Step 3: Inspect the Data   # Display the first few records print(df.head())  # Summary of the dataset print(df.info())      id                                              title              author  \\ 0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus    1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn    2   2                  Why the Truth Might Get You Fired  Consortiumnews.com    3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss    4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy                                                     text  label   0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   1  Ever get the feeling your life circles the rou...      0   2  Why the Truth Might Get You Fired October 29, ...      1   3  Videos 15 Civilians Killed In Single US Airstr...      1   4  Print \\nAn Iranian woman has been sentenced to...      1   &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 20800 entries, 0 to 20799 Data columns (total 5 columns):  #   Column  Non-Null Count  Dtype  ---  ------  --------------  -----   0   id      20800 non-null  int64   1   title   20242 non-null  object  2   author  18843 non-null  object  3   text    20761 non-null  object  4   label   20800 non-null  int64  dtypes: int64(2), object(3) memory usage: 812.6+ KB None   Before we begin pre-processing, we are inspecting our data. This gives us a rough idea about the dataset’s structure and any potential issues it might have such as missing values.   Step 4: Prepare the Labels   # Get the labels labels = df.label   Step 5: Split the Data   # Split the dataset x_train, x_test, y_train, y_test = train_test_split(df['text'], labels, test_size=0.2, random_state=7)   We split our dataset into a training set and a test set. This is to ensure that we have a fair evaluation of our model, by testing it on unseen data.   Step 6: Handle Missing Values   # Fill NaN values with empty string x_train = x_train.fillna('') x_test = x_test.fillna('')   We’re handling any potential missing values in our dataset. Since our feature is text, we can fill missing values with an empty string.   Step 7: Initialize and Apply Count Vectorizer   # Initialize a CountVectorizer count_vectorizer = CountVectorizer(stop_words='english')  # Fit and transform the training data  count_train = count_vectorizer.fit_transform(x_train.values) # Transform the test data count_test = count_vectorizer.transform(x_test.values)   We’re initializing our CountVectorizer and fitting it to our data. This converts our text data into a format that our model can understand.   Step 8: Train the Model   # Initialize a PassiveAggressiveClassifier pac = PassiveAggressiveClassifier(max_iter=50) pac.fit(count_train, y_train)            PassiveAggressiveClassifier(max_iter=50)     In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.  On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.                   PassiveAggressiveClassifier                   PassiveAggressiveClassifier(max_iter=50)                                     Here we’re initializing our PassiveAggressiveClassifier and fitting it to our training data.   Step 9: Make Predictions and Evaluate the Model   # Predict on the test set and calculate accuracy y_pred = pac.predict(count_test) score = accuracy_score(y_test, y_pred) print(f'Accuracy: {round(score*100,2)}%')  # Confusion matrix confusion_mat = confusion_matrix(y_test, y_pred) print('Confusion Matrix:\\n', confusion_mat)   Accuracy: 94.18% Confusion Matrix:  [[1930  130]  [ 112 1988]]   We are making predictions on our test set and evaluating our model’s performance. In this case, we’re using accuracy as our metric.   Step 10: Visualize Results   import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import confusion_matrix  # Get the confusion matrix cm = confusion_matrix(y_test,y_pred)  # Plot the confusion matrix in a heat map plt.figure(figsize=(7,7)) sns.heatmap(cm, annot=True, fmt=\"d\") plt.title('Confusion matrix of the classifier') plt.xlabel('Predicted') plt.ylabel('True')    Text(58.222222222222214, 0.5, 'True')      from sklearn.feature_extraction.text import CountVectorizer import matplotlib.pyplot as plt import seaborn as sns  # We'll use CountVectorizer to count the word frequencies vectorizer = CountVectorizer(stop_words='english')  # Fit and transform the training data train_matrix = vectorizer.fit_transform(x_train)  # Get the word frequencies word_freq_df = pd.DataFrame(train_matrix.toarray(), columns=vectorizer.get_feature_names_out()) word_freq = word_freq_df.sum(axis=0)  # Get the 20 most common words top_words = word_freq.sort_values(ascending=False).head(20)  plt.figure(figsize=(10, 8)) sns.barplot(x=top_words.values, y=top_words.index) plt.title('Top 20 words in fake news texts') plt.xlabel('Frequency') plt.ylabel('Word') plt.show()       ","categories": ["blog"],
        "tags": ["Numpy","project"],
        "url": "http://localhost:4000/blog/Fake-News-Detection/",
        "teaser": null
      },{
        "title": "Credit Card Fraud Detection Using Random Forest",
        "excerpt":"Introduction:   Credit card fraud is a significant concern in today’s digital world, with substantial financial and security implications for individuals and financial institutions. Machine learning techniques provide a promising solution to detect fraudulent transactions and mitigate the risks associated with them. In this blog post, we explore the application of machine learning algorithms to enhance credit card fraud detection.   Random Forest   Here’s how Random Forest works:   Decision Trees:  A decision tree is a flowchart-like structure where each internal node represents a test on a feature, each branch represents the outcome of the test, and each leaf node represents a class label or a decision. Decision trees can be prone to overfitting, meaning they might learn too much from the training data and perform poorly on new, unseen data.   Ensemble Learning:  Random Forest overcomes the limitations of a single decision tree by combining the predictions of multiple decision trees. It creates an ensemble of decision trees and makes predictions by taking a majority vote or averaging the predictions of the individual trees.   Random Subsets:  Random Forest introduces randomness by using random subsets of the original dataset for building each decision tree. This process is known as bagging (bootstrap aggregating). The subsets are created by sampling the data with replacement, meaning that some instances may be repeated in the subsets, while others may be left out. This approach helps to introduce diversity among the trees.   Random Feature Selection:  In addition to using random subsets of data, Random Forest also randomly selects a subset of features at each split of a decision tree. This process helps to decorrelate the trees and ensures that different trees consider different subsets of features. It prevents a single feature from dominating the decision-making process and promotes more robust predictions.   Voting or Averaging:  Once the individual decision trees are built, predictions are made by taking a majority vote (for classification problems) or averaging (for regression problems) of the predictions from each tree. This aggregation of predictions reduces the variance and improves the overall performance of the model.   Advantages of Random Forest:  Random Forest is highly accurate and performs well on a wide range of datasets. It can handle large datasets with high dimensionality. Random Forest can effectively handle imbalanced datasets and is less prone to overfitting compared to individual decision trees. It provides a measure of feature importance, allowing for feature selection and interpretation of the model.      Step 1: Import necessary libraries and load the data   The first step in any machine learning problem is to import the necessary libraries and load the data we are going to use.   # Importing necessary libraries import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, accuracy_score from sklearn.manifold import TSNE from sklearn import preprocessing import matplotlib.pyplot as plt import seaborn as sns  # Load the dataset from the csv file data = pd.read_csv('creditcard.csv')   Step 2: Data Exploration   Before proceeding to model training, we should first explore our data a bit.   # Understanding the columns print(data.columns)  # Understanding the shape print(data.shape)  # Understanding the data types print(data.dtypes)  # Check for missing values print(data.isnull().sum())    Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',        'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',        'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',        'Class'],       dtype='object') (284807, 31) Time      float64 V1        float64 V2        float64 V3        float64 V4        float64 V5        float64 V6        float64 V7        float64 V8        float64 V9        float64 V10       float64 V11       float64 V12       float64 V13       float64 V14       float64 V15       float64 V16       float64 V17       float64 V18       float64 V19       float64 V20       float64 V21       float64 V22       float64 V23       float64 V24       float64 V25       float64 V26       float64 V27       float64 V28       float64 Amount    float64 Class       int64 dtype: object Time      0 V1        0 V2        0 V3        0 V4        0 V5        0 V6        0 V7        0 V8        0 V9        0 V10       0 V11       0 V12       0 V13       0 V14       0 V15       0 V16       0 V17       0 V18       0 V19       0 V20       0 V21       0 V22       0 V23       0 V24       0 V25       0 V26       0 V27       0 V28       0 Amount    0 Class     0 dtype: int64   The above code provides information about the dataset like the column names, the number of records (shape), the data types of the columns, and checks if there are any missing values in the data.   Step 3: Visualizing the Data   Visualization of data is an essential aspect of any data analysis. Here, we are visualizing the distribution of the classes (fraudulent vs. non-fraudulent transactions)   # Visualizing the classes count_classes = pd.value_counts(data['Class'], sort = True) count_classes.plot(kind = 'bar', rot=0) plt.title(\"Transaction class distribution\") plt.xticks(range(2), [\"Normal\", \"Fraud\"]) plt.xlabel(\"Class\") plt.ylabel(\"Frequency\") plt.show()      This code creates a bar chart showing the frequency of fraudulent vs. non-fraudulent transactions. This can be useful in understanding the balance (or imbalance) between the classes in our dataset.   Step 4: Data Preprocessing   Data preprocessing is an important step in a machine learning project. It transforms raw data into a format that will be more easily and effectively processed for the purpose of the user.   In the given dataset, features are mostly scaled except for Time and Amount. So let’s scale these features too.   from sklearn.preprocessing import StandardScaler  # Scaling Time and Amount scaler = StandardScaler() data['scaled_amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1,1)) data['scaled_time'] = scaler.fit_transform(data['Time'].values.reshape(-1,1))  # Dropping old Time and Amount data = data.drop(['Time','Amount'], axis=1)   Step 5: Splitting the Data into Train and Test Sets   The data is divided into two parts: a training set and a test set. The model is trained on the training data and then tested on the unseen test data to evaluate its performance.   from sklearn.model_selection import train_test_split  # Defining the features and target X = data.drop('Class', axis=1) y = data['Class']  # Splitting the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)   Step 6: Training the Machine Learning Model   The Random Forest model has a good performance in general because it runs multiple decision trees on various subsets of the dataset and makes the final decision based on the majority voting from all the individual decision trees. It has the ability to limit overfitting without substantially increasing error due to bias.      # Importing necessary libraries from sklearn.ensemble import RandomForestClassifier  # Define the model as the random forest rf_model = RandomForestClassifier(n_estimators=100)  # Train the model rf_model.fit(X_train, y_train)  # Use the model to make predictions rf_predicted = rf_model.predict(X_test)            LogisticRegression()     In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.  On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.                   LogisticRegression                   LogisticRegression()                                     Step 7: Evaluating the Model   Now, we will predict the labels for our test set and evaluate the model’s performance by comparing these predictions to the actual labels.   from sklearn.metrics import classification_report  # Making predictions on the test data y_pred = rf_model.predict(X_test)  # Printing the classification report print(classification_report(y_test, y_pred))                 precision    recall  f1-score   support             0       1.00      1.00      1.00     56864            1       0.97      0.78      0.86        98      accuracy                           1.00     56962    macro avg       0.99      0.89      0.93     56962 weighted avg       1.00      1.00      1.00     56962   import seaborn as sns from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt  # Predicting the test set results y_pred = rf_model.predict(X_test)  # Making the confusion matrix cm = confusion_matrix(y_test, y_pred)  # Visualizing the confusion matrix plt.figure(figsize=(10,7)) sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\") plt.title('Confusion matrix of the classifier') plt.xlabel('Predicted') plt.ylabel('True') plt.show()      from sklearn.metrics import roc_curve, auc  # Compute ROC curve and ROC area for predictions on validation set fpr, tpr, _ = roc_curve(y_test, y_pred) roc_auc = auc(fpr, tpr)  # Plot plt.figure() lw = 2 plt.plot(fpr, tpr, color='darkorange',          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic') plt.legend(loc=\"lower right\") plt.show()       Conclusion   The Random Forest model demonstrates excellent performance in classifying non-fraudulent transactions (Class 0). It achieves perfect precision, recall, and f1-score of 1.00, indicating that it accurately predicts all non-fraudulent transactions without any false positives or false negatives. This suggests that the model is highly reliable in identifying genuine transactions.   For fraudulent transactions (Class 1), the Random Forest model exhibits strong performance as well. It achieves a high precision of 0.97, indicating that out of all transactions predicted as fraudulent, 97% are truly fraudulent. The recall of 0.78 suggests that the model correctly identifies 78% of all actual fraudulent transactions. The f1-score of 0.86 represents a harmonious balance between precision and recall for the fraudulent class.   The overall accuracy of the model is 1.00, which means it correctly classifies transactions in both classes with high accuracy.   In summary, the Random Forest model demonstrates exceptional performance in detecting fraudulent transactions while maintaining a high accuracy rate for non-fraudulent transactions. It achieves a good balance between precision and recall for the fraudulent class, providing a reliable and robust solution for credit card fraud detection.   ","categories": ["blog"],
        "tags": ["Numpy","project"],
        "url": "http://localhost:4000/blog/Fraud-Creditcard-Detection/",
        "teaser": null
      },{
        "title": "Stanford CS229:  Linear Regression and Gradient Descent",
        "excerpt":"Introduction   I have recently starting taking CS229 with Andrew Ng through Stanford. This is a course that can take you to the moon if done correctly. With that being said I want to document everything I learn and take it to the next level by doing further research and gaining deeper understanding.   Linear Regression:   In the context of machine learning, linear regression uses a set of input features (X1, X2, …, Xn) to predict a continuous output variable Y.   The equation for multiple linear regression is:   Y = θ0 + θ1X1 + θ2X2 + … + θn*Xn + e   Y = Output variable θ0, θ1, …, θn = Parameters of the model X1, X2, …, Xn = Input features e = error term   Cost Function:   Linear regression typically uses a cost function called Mean Squared Error (MSE) to measure the error in the model’s predictions. The goal of linear regression is to find the parameters θ that minimize the cost function.   The cost function J(θ) for linear regression is:   J(θ) = 1/2m Σ (hθ(xi) - yi)^2   where:   m is the number of training examples hθ(xi) is the predicted output for the ith training example using the current parameters θ yi is the actual output for the ith training example   Gradient Descent:   Gradient Descent is an optimization algorithm used to minimize the cost function. It iteratively adjusts the parameters θ in the direction that reduces the cost function the most, until the cost function converges to a minimum.   The update rule for the gradient descent is:   θj := θj - α * ∂/∂θj J(θ)   where:   α is the learning rate, determining the step size in each iteration ∂/∂θj J(θ) is the partial derivative of the cost function with respect to the parameter θj The partial derivative of the cost function with respect to a parameter θj is:   ∂/∂θj J(θ) = 1/m * Σ (hθ(xi) - yi) * xij   Therefore, the update rule becomes:   θj := θj - α * 1/m * Σ (hθ(xi) - yi) * xij   Here, α is the learning rate, which controls how large a step we take in each iteration.   Normal Equation:   The Normal Equation is an analytical approach to linear regression that can be used to find the values of the parameters θ that minimize the cost function. Unlike gradient descent, this method does not require choosing a learning rate α, and it does not require many iterations since it directly computes the solution.   The normal equation is:   θ = (X^T * X)^-1 * X^T * y   where:   X is the matrix of input features y is the vector of output variables The cost function in matrix form is:   J(θ) = 1/2 * (Xθ - y)^T * (Xθ - y)   We can differentiate this with respect to θ:   ∂/∂θ J(θ) = X^T * (Xθ - y)   Setting this equal to zero, we get:   `X^T * X * θ = X   ","categories": ["blog"],
        "tags": ["study","cs229"],
        "url": "http://localhost:4000/blog/CS229-Lecture1/",
        "teaser": null
      },{
        "title": "CS229 Problem Set #1: Supervised Learning",
        "excerpt":"Introduction  In this blog post, I will delve into the first problem set of the CS229 course on Supervised Learning. This problem set focuses on linear classifiers, specifically logistic regression and Gaussian discriminant analysis (GDA). I will explore the concepts, assumptions, and strengths and Iaknesses of these two algorithms. Additionally, I will discuss the Poisson regression and locally Iighted linear regression.   Problem 1: Linear Classifiers (Logistic Regression and GDA)  The first problem in the CS229 problem set deals with logistic regression and Gaussian discriminant analysis (GDA) as linear classifiers. I are given two datasets, and our task is to perform binary classification on these datasets using logistic regression and GDA.   Part (a): Logistic Regression  I are asked to find the Hessian of the average empirical loss function for logistic regression and show that it is positive semi-definite. The average empirical loss for logistic regression is defined as:   J(θ) = -1/m * Σ [y(i)log(hθ(x(i))) + (1-y(i))log(1-hθ(x(i)))],   where y(i) ∈ {0, 1}, hθ(x) = g(θ^T * x), and g(z) = 1/(1 + exp(-z)).   To show that the Hessian is positive semi-definite, I need to prove that z^T * Hz ≥ 0 for any vector z. I can start by showing that PΣΣz_i * x_i * x_j * z_j = (x^T * z)^2 ≥ 0. I can also use the fact that g’(z) = g(z)(1 - g(z)).   Part (b): Logistic Regression Implementation  The next part of the problem involves implementing logistic regression using Newton’s Method. I need to train the logistic regression classifier using the provided training dataset and write the model’s predictions to a file. I continue training until the updates to θ become small.   import numpy as np import util from linear_model import LinearModel  def main(train_path, eval_path, pred_path):     \"\"\"Problem 1(b): Logistic regression with Newton's Method.          Args:         train_path: Path to CSV file containing dataset for training.         eval_path: Path to CSV file containing dataset for evaluation.         pred_path: Path to save predictions.     \"\"\"     x_train, y_train = util.load_dataset(train_path, add_intercept=True)      # Train logistic regression     model = LogisticRegression(eps=1e-5)     model.fit(x_train, y_train)      # Plot data and decision boundary     util.plot(x_train, y_train, model.theta, 'output/p01b_{}.png'.format(pred_path[-5]))      # Save predictions     x_eval, y_eval = util.load_dataset(eval_path, add_intercept=True)     y_pred = model.predict(x_eval)     np.savetxt(pred_path, y_pred &gt; 0.5, fmt='%d')  class LogisticRegression(LinearModel):     \"\"\"Logistic regression with Newton's Method as the solver.\"\"\"      def fit(self, x, y):         \"\"\"Run Newton's Method to minimize J(theta) for logistic regression.                  Args:             x: Training example inputs. Shape (m, n).             y: Training example labels. Shape (m,).         \"\"\"         # Init theta         m, n = x.shape         self.theta = np.zeros(n)          # Newton's method         while True:             # Save old theta             theta_old = np.copy(self.theta)                          # Compute Hessian Matrix             h_x = 1 / (1 + np.exp(-x.dot(self.theta)))             H = (x.T * h_x * (1 - h_x)).dot(x) / m             gradient_J_theta = x.T.dot(h_x - y) / m              # Update theta             self.theta -= np.linalg.inv(H).dot(gradient_J_theta)              # Check for convergence             if np.linalg.norm(self.theta - theta_old, ord=1) &lt; self.eps:                 break      def predict(self, x):         \"\"\"Make predictions given new inputs x.                  Args:             x: Inputs of shape (m, n).                      Returns:             Predicted outputs of shape (m,).         \"\"\"         return self.predict_proba(x) &gt;= 0.5      def predict_proba(self, x):         \"\"\"Compute the predicted probabilities of class 1 given new inputs x.                  Args:             x: Inputs of shape (m, n).                      Returns:             Predicted probabilities of class 1 of shape (m,).         \"\"\"         return 1 / (1 + np.exp(-x.dot(self.theta)))    Part (c): Gaussian Discriminant Analysis (GDA)  In this part, I revisit Gaussian discriminant analysis (GDA). I are given the joint distribution of (x, y) and asked to show that the posterior distribution can be written as p(y=1|x; φ, µ0, µ1, Σ) = 1/(1 + exp(-(θ^T * x + θ0))), where θ ∈ R^n and θ0 ∈ R are functions of φ, Σ, µ0, and µ1.   Part (d): Maximum Likelihood Estimation  For this part, I assume that n (the dimension of x) is 1, and I are asked to derive the maximum likelihood estimates of the parameters φ, µ0, µ1, and Σ. I are given the dataset and need to calculate the maximum likelihood estimates based on the formulas provided.   Part (e): GDA Implementation  I need to implement GDA using the provided dataset and calculate the parameters φ, µ0, µ1, and Σ. I then derive θ based on these parameters and use the resulting GDA model to make predictions on the validation set.   import numpy as np import util  from linear_model import LinearModel   def main(train_path, eval_path, pred_path):     \"\"\"Problem 1(e): Gaussian discriminant analysis (GDA)      Args:         train_path: Path to CSV file containing dataset for training.         eval_path: Path to CSV file containing dataset for evaluation.         pred_path: Path to save predictions.     \"\"\"     # Load dataset     x_train, y_train = util.load_dataset(train_path, add_intercept=False)      # *** START CODE HERE ***          # Train GDA     model = GDA()     model.fit(x_train, y_train)      # Plot data and decision boundary     util.plot(x_train, y_train, model.theta, 'output/p01e_{}.png'.format(pred_path[-5]))      # Save predictions     x_eval, y_eval = util.load_dataset(eval_path, add_intercept=True)     y_pred = model.predict(x_eval)     np.savetxt(pred_path, y_pred &gt; 0.5, fmt='%d')      # *** END CODE HERE ***   class GDA(LinearModel):     def fit(self, x, y):         \"\"\"Fit a GDA model to training set given by x and y.          Args:             x: Training example inputs. Shape (m, n).             y: Training example labels. Shape (m,).          Returns:             theta: GDA model parameters.         \"\"\"         # *** START CODE HERE ***                  # Init theta         m, n = x.shape         self.theta = np.zeros(n+1)          # Compute phi, mu_0, mu_1, sigma         y_1 = sum(y == 1)         phi = y_1 / m         mu_0 = np.sum(x[y == 0], axis=0) / (m - y_1)         mu_1 = np.sum(x[y == 1], axis=0) / y_1         sigma = ((x[y == 0] - mu_0).T.dot(x[y == 0] - mu_0) + (x[y == 1] - mu_1).T.dot(x[y == 1] - mu_1)) / m          # Compute theta         sigma_inv = np.linalg.inv(sigma)         self.theta[0] = 0.5 * (mu_0 + mu_1).dot(sigma_inv).dot(mu_0 - mu_1) - np.log((1 - phi) / phi)         self.theta[1:] = sigma_inv.dot(mu_1 - mu_0)                  # Return theta         return self.theta          # *** END CODE HERE ***      def predict(self, x):         \"\"\"Make a prediction given new inputs x.          Args:             x: Inputs of shape (m, n).          Returns:             Outputs of shape (m,).         \"\"\"         # *** START CODE HERE ***                  return 1 / (1 + np.exp(-x.dot(self.theta)))          # *** END CODE HERE   Part (f): Visualization of Decision Boundaries  In this part, I visualize the training data for Dataset 1 and plot the decision boundary found by logistic regression and GDA on the same figure. I use different symbols to represent examples with y=0 and y=1.   Part (g): Comparison of Logistic Regression and GDA  I repeat the visualization and comparison process for Dataset 2. I analyze which algorithm performs better and discuss the reasons for the observed performance.   Part (h): Transformation for Improved GDA Performance  As an extra credit task, I explore if a transformation of the x’s can significantly improve the performance of GDA on the dataset where it initially performed worse. I discuss the transformation and its impact on GDA’s performance.   Problem 2: Incomplete, Positive-Only Labels  The second problem focuses on training binary classifiers in situations where I have labels only for a subset of the positive examples. I are given a dataset with true labels t(i), partial labels y(i), and input features x(i). Our task is to construct a binary classifier h for the true labels t using only the partial labels y and the input features x.   Part (a): Probability of Being Labeled  I are asked to show that the probability of an example being labeled differs by a constant factor from the probability of an example being positive. I need to prove that p(t=1|x) = p(y=1|x)/α for some α ∈ R.   Part (b): Estimating α  In this part, I derive an expression for α using a trained classifier h and a held-out validation set V. I show that h(x(i)) ≈ α for all x(i) ∈ V+ (labeled examples). I assume that p(t=1|x) ≈ 1 when x(i) ∈ V+.   Part (c): Partial Label Classification Implementation  I are provided with a dataset and asked to implement logistic regression using the partial labels y. I train the classifier, rescale the predictions using the estimated value of α, and visualize the decision boundaries on the test set.   import numpy as np import util  from p01b_logreg import LogisticRegression  # Character to replace with sub-problem letter in plot_path/pred_path WILDCARD = 'X'   def main(train_path, valid_path, test_path, pred_path):     \"\"\"Problem 2: Logistic regression for incomplete, positive-only labels.      Run under the following conditions:         1. on y-labels,         2. on l-labels,         3. on l-labels with correction factor alpha.      Args:         train_path: Path to CSV file containing training set.         valid_path: Path to CSV file containing validation set.         test_path: Path to CSV file containing test set.         pred_path: Path to save predictions.     \"\"\"     pred_path_c = pred_path.replace(WILDCARD, 'c')     pred_path_d = pred_path.replace(WILDCARD, 'd')     pred_path_e = pred_path.replace(WILDCARD, 'e')      # *** START CODE HERE ***     #######################################################################################     # Problem (c)     x_train, t_train = util.load_dataset(train_path, label_col='t', add_intercept=True)     x_test, t_test = util.load_dataset(test_path, label_col='t', add_intercept=True)      model_t = LogisticRegression()     model_t.fit(x_train, t_train)      util.plot(x_test, t_test, model_t.theta, 'output/p02c.png')      t_pred_c = model_t.predict(x_test)     np.savetxt(pred_path_c, t_pred_c &gt; 0.5, fmt='%d')     #######################################################################################     # Problem (d)     x_train, y_train = util.load_dataset(train_path, label_col='y', add_intercept=True)     x_test, y_test = util.load_dataset(test_path, label_col='y', add_intercept=True)      model_y = LogisticRegression()     model_y.fit(x_train, y_train)      util.plot(x_test, y_test, model_y.theta, 'output/p02d.png')      y_pred = model_y.predict(x_test)     np.savetxt(pred_path_d, y_pred &gt; 0.5, fmt='%d')     #######################################################################################       # Problem (e)     x_valid, y_valid = util.load_dataset(valid_path, label_col='y', add_intercept=True)      alpha = np.mean(model_y.predict(x_valid))      correction = 1 + np.log(2 / alpha - 1) / model_y.theta[0]     util.plot(x_test, t_test, model_y.theta, 'output/p02e.png', correction)      t_pred_e = y_pred / alpha     np.savetxt(pred_path_e, t_pred_e &gt; 0.5, fmt='%d')     #######################################################################################     # *** END CODER HERE    Problem 3: Poisson Regression  The third problem focuses on Poisson regression, which is a type of generalized linear model (GLM) used for count data. I are asked to derive the properties of Poisson distribution and Poisson regression.   Part (a): Exponential Family Representation  I need to show that the Poisson distribution is in the exponential family and provide the values for b(y), η, T(y), and a(η). The exponential family representation for the Poisson distribution is given by p(y; η) = b(y)exp(ηT(y) - a(η)).   Part (b): Canonical Response Function  I are asked to determine the canonical response function for Poisson regression. The canonical response function for a GLM with a Poisson response variable is derived by setting the mean of the Poisson distribution (λ) equal to the linear combination of the input features (θ^T * x).   Part (c): Stochastic Gradient Ascent Update Rule  In this part, I derive the stochastic gradient ascent update rule for Poisson regression using the negative log-likelihood loss function. I take the derivative of the log-likelihood with respect to θ and set it to zero to find the optimal θ.   Part (d): Poisson Regression Implementation  I are provided with a dataset and asked to implement Poisson regression using gradient ascent to maximize the log-likelihood of θ. I train the model on the training split and make predictions on the test split.   import matplotlib.pyplot as plt import numpy as np import util  from linear_model import LinearModel   def main(lr, train_path, eval_path, pred_path):     \"\"\"Problem 3(d): Poisson regression with gradient ascent.      Args:         lr: Learning rate for gradient ascent.         train_path: Path to CSV file containing dataset for training.         eval_path: Path to CSV file containing dataset for evaluation.         pred_path: Path to save predictions.     \"\"\"     # Load training set     x_train, y_train = util.load_dataset(train_path, add_intercept=False)      # *** START CODE HERE ***          model = PoissonRegression(step_size=lr, eps=1e-5)     model.fit(x_train, y_train)      x_eval, y_eval = util.load_dataset(eval_path, add_intercept=False)     y_pred = model.predict(x_eval)     np.savetxt(pred_path, y_pred)      plt.figure()     plt.plot(y_eval, y_pred, 'bx')     plt.xlabel('true counts')     plt.ylabel('predict counts')     plt.savefig('output/p03d.png')      # *** END CODE HERE ***   class PoissonRegression(LinearModel):     def fit(self, x, y):         \"\"\"Run gradient ascent to maximize likelihood for Poisson regression.          Args:             x: Training example inputs. Shape (m, n).             y: Training example labels. Shape (m,).         \"\"\"         # *** START CODE HERE ***                  m, n = x.shape         self.theta = np.zeros(n)          while True:             theta = np.copy(self.theta)             self.theta += self.step_size * x.T.dot(y - np.exp(x.dot(self.theta))) / m              if np.linalg.norm(self.theta - theta, ord=1) &lt; self.eps:                 break          # *** END CODE HERE ***      def predict(self, x):         \"\"\"Make a prediction given inputs x.          Args:             x: Inputs of shape (m, n).          Returns:             Floating-point prediction for each input, shape (m,).         \"\"\"         # *** START CODE HERE ***                  return np.exp(x.dot(self.theta))          # *** END CODE HERE ***   Problem 4: Convexity of Generalized Linear Models  The fourth problem explores the convexity of Generalized Linear Models (GLMs) and their use of exponential family distributions to model the output.   Part (a): Mean of the Distribution  I derive an expression for the mean of an exponential family distribution and show that it can be represented as the gradient of the log-partition function with respect to the natural parameter.   Part (b): Variance of the Distribution  I derive an expression for the variance of an exponential family distribution and show that it can be expressed as the derivative of the mean with respect to the   natural parameter.   Part (c): Convexity of NLL Loss  In this part, I write the negative log-likelihood (NLL) loss function as a function of the model parameters and calculate its Hessian. I show that the Hessian is positive semi-definite, indicating that the NLL loss of GLMs is a convex function.   Problem 5: Locally Iighted Linear Regression  The fifth problem introduces locally Iighted linear regression, where different training examples are Iighted differently. I minimize a Iighted sum of squared errors to find the optimal parameters.   Part (a): Iighted Loss Function  I express the Iighted loss function in matrix form and define the appropriate Iight matrix W.   Part (b): Normal Equation in Iighted Setting  I generalize the normal equation for the Iighted setting by finding the derivative of the Iighted loss function and setting it to zero. I derive the new value of θ that minimizes the Iighted loss function.   Part (c): Locally Iighted Linear Regression Implementation  I implement locally Iighted linear regression using the derived normal equation and train the model on the provided dataset. I tune the hyperparameter τ and evaluate the model’s performance on the validation and test sets.   Conclusion  In this blog post, I have covered the CS229 Problem Set #1 on Supervised Learning. I explored logistic regression, Gaussian discriminant analysis (GDA), Poisson regression, and locally Iighted linear regression. I discussed the concepts, assumptions, implementations, and evaluations of these algorithms. Through this problem set, I gained a deeper understanding of linear classifiers, the use of exponential family distributions, and the importance of parameter estimation and model evaluation.  ","categories": ["blog","cs229"],
        "tags": ["problem set"],
        "url": "http://localhost:4000/blog/cs229/CS229-ProblemSet1/",
        "teaser": null
      },{
        "title": "Deep Reinforcement Learning for Autonomous Vehicles: A Path to the Future",
        "excerpt":"Introduction   The domain of self-driving cars is an extraordinary example of how cutting-edge machine learning techniques are being applied to real-world problems. Key players in this landscape like Tesla and Comma.ai, led by the indomitable George Hotz, are transforming our understanding of transport. My profound interest in this field propels me to explore its depths, particularly the application of Deep Reinforcement Learning (DRL) in the design of autonomous vehicles.   Reinforcement Learning (RL) and its advanced variant, DRL, are the cornerstones of modern intelligent systems that learn to make sequences of decisions. From Go-playing champions like AlphaGo to sophisticated robotics, DRL has been instrumental in breaking barriers.   The Power of Deep Q-Learning   Q-Learning, a classic RL algorithm, aims to learn a policy that can tell an agent what action to take under what circumstances. It does this by learning a Q-function, which predicts the expected return (the sum of future rewards) for taking an action in a given state.   Deep Q-Networks (DQN), an offshoot of Q-Learning, leverages deep learning to approximate the Q-function. With DQN, we can process high-dimensional inputs and handle large action spaces, which is essential in complex scenarios like autonomous driving.   The fundamental architecture of a DQN involves a neural network taking in states and outputting Q-values for all possible actions. The action with the highest Q-value is chosen according to an ε-greedy strategy, ensuring a balance between exploration and exploitation.   The key idea in DQN is the use of a separate target network to compute the Q-learning targets, which stabilizes the training. This approach helps us mitigate the risk of harmful feedback loops and fluctuating Q-values, often observed in traditional RL methods.   Our Simulated Environment: The OpenAI Gym   The CarRacing-v0 environment in OpenAI’s gym is an excellent playground for autonomous vehicle algorithms. It offers a top-down view of a simple track, where our autonomous vehicle needs to navigate the optimal path.   # Environment setup import gym env = gym.make('CarRacing-v0')   DQN in Action   We’ll utilize the stable_baselines3 library, which offers a user-friendly implementation of DQN. Let’s define and train our model:   from stable_baselines3 import DQN  # Model definition model = DQN(\"MlpPolicy\", env, verbose=1, learning_rate=0.0005, buffer_size=50000, exploration_fraction=0.1, exploration_final_eps=0.02)  # Model training model.learn(total_timesteps=10000)   Here, MlpPolicy is a feed-forward neural network policy that DQN uses to decide which action to take based on the current state. We set a relatively small learning rate (0.0005) to ensure smooth convergence, and we use a large buffer size (50000) to store more past experiences for sampling. Our ε-greedy strategy starts at 0.1 and decays to 0.02, meaning that our agent starts by exploring the environment quite a lot, but over time, it focuses more on exploiting its learned policy.   Model Evaluation   After the training phase, we evaluate the performance of our model:   from stable_baselines3.common.evaluation import evaluate_policy  # Model evaluation mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10) print(f\"Mean reward: {mean_reward} +/- {std_reward}\")   This evaluation provides us with the mean reward and its standard deviation over 10 episodes, offering a snapshot of our DQN model’s performance.   The Road Ahead   Tesla, Comma.ai, and others are pushing the boundaries of autonomous driving using complex networks of sensors and deep learning models. These models have to solve challenging tasks, from recognizing objects and predicting their trajectories to making safe and efficient driving decisions.   While my DQN experiment is a simplified version of this complex problem, the principles remain the same. The ability of DRL algorithms to learn from interactions with the environment, and to improve through trial and error, is at the heart of developing vehicles that can drive themselves safely and efficiently.   In future posts, we’ll delve deeper into more advanced techniques such as policy gradients and actor-critic methods, which have shown great promise in autonomous driving applications. We’ll also touch upon the ethical, legal, and societal implications of this transformative technology.   This exploration serves as a testament to my commitment to understanding and applying advanced machine learning concepts, with the hope of being part of the change we’re witnessing in the world of transportation.   ","categories": ["blog"],
        "tags": ["study","Self Driving Cars","Reinforcement Learning"],
        "url": "http://localhost:4000/blog/Deep-RL-Self-Driving/",
        "teaser": null
      },{
        "title": "AI Resources",
        "excerpt":"Introduction  AI is revolutionizing various aspects of our daily lives, with new applications and advancements emerging every week. Here’s a roundup of some of the most impactful and intriguing AI news from the past week.   AI Resource Highlights  To stay informed and explore the latest developments in AI, I rely on a variety of trusted resources. Here are some of my favorites:           The Batch by Andrew Ng  Andrew Ng, a globally recognized leader in AI, provides valuable insights through his platform, The Batch. As the founder of DeepLearning.AI and other prominent organizations, his expertise and contributions have made a significant impact on the AI community. Website            MarkeTechPost  MarkeTechPost is a California-based AI News Platform with a thriving community of over 1.5 million AI professionals and developers. They deliver technical AI research news in a digestible and applicable format, making it a valuable resource for staying updated with the latest trends. Website            Papers with Code  Papers with Code aims to create a free and open resource that combines machine learning papers, code, datasets, methods, and evaluation tables. This platform provides a comprehensive collection of research papers, making it easier to access and implement cutting-edge AI techniques. Website            TLDR AI  TLDR AI offers quick and concise summaries of AI, machine learning, and data science topics, all within a five-minute read. This resource is perfect for busy individuals who want to stay informed about the latest AI advancements without spending too much time. Website            Bloomberg Technology  Bloomberg Technology covers a wide range of technology-related news, including AI. Their insightful articles and analysis provide valuable perspectives on the intersection of AI and various industries, making it a reliable resource for industry trends and developments. Website       In addition to these platforms, I also find Twitter to be a valuable source of AI news and insights. Here are some Twitter accounts I follow:      @dair_ai: Focuses on democratizing AI research, education, and technologies.   @papers_daily: Tweets popular AI papers, curated by @labmlai and AK @_akhaliq.   @Gradio: Shares information about AI, ML, and data science, often focusing on practical applications and tools.   @ai__pub: Provides AI papers and AI research explanations for technical individuals.   @Deeplearning_AI: Shares information about deep learning, AI, and machine learning advancements.   @DLdotHub: Focuses on open-source software, tools, datasets, news, and research in the machine learning and data science domain, with an emphasis on deep learning.   By utilizing these resources, I can stay up-to-date with the latest AI developments, learn from industry leaders, and explore practical applications of AI.   Conclusion  As AI continues to evolve at a rapid pace, it is more important than ever to stay informed about the latest developments and discussions. The resources mentioned in this blog post provide valuable insights, technical knowledge, and practical applications of AI. By leveraging these resources, we can actively participate in shaping the future of AI and harness its transformative power across various domains.   ","categories": ["blog"],
        "tags": ["pytorch","news"],
        "url": "http://localhost:4000/blog/AI-Resources/",
        "teaser": null
      },{
        "title": "PyTorch RNN from Scratch",
        "excerpt":"In this post, we’ll take a look at RNNs, or recurrent neural networks, and attempt to implement parts of it in scratch through PyTorch. Yes, it’s not entirely from scratch in the sense that we’re still relying on PyTorch autograd to compute gradients and implement backprop, but I still think there are valuable insights we can glean from this implementation as well.   Data Preparation   The task is to build a simple classification model that can correctly determine the nationality of a person given their name. Put more simply, we want to be able to tell where a particular name is from.   Download   We will be using some labeled data from the PyTorch tutorial. We can download it simply by typing   !curl -O https://download.pytorch.org/tutorial/data.zip; unzip data.zip   This command will download and unzip the files into the current directory, under the folder name of data.   Now that we have downloaded the data we need, let’s take a look at the data in more detail. First, here are the dependencies we will need.   import os import random from string import ascii_letters  import torch from torch import nn import torch.nn.functional as F from unidecode import unidecode  _ = torch.manual_seed(42) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   We first specify a directory, then try to print out all the labels there are. We can then construct a dictionary that maps a language to a numerical label.   data_dir = \"./data/names\"  lang2label = {     file_name.split(\".\")[0]: torch.tensor([i], dtype=torch.long)     for i, file_name in enumerate(os.listdir(data_dir)) }   We see that there are a total of 18 languages. I wrapped each label as a tensor so that we can use them directly during training.   lang2label   {'Czech': tensor([0]),  'German': tensor([1]),  'Arabic': tensor([2]),  'Japanese': tensor([3]),  'Chinese': tensor([4]),  'Vietnamese': tensor([5]),  'Russian': tensor([6]),  'French': tensor([7]),  'Irish': tensor([8]),  'English': tensor([9]),  'Spanish': tensor([10]),  'Greek': tensor([11]),  'Italian': tensor([12]),  'Portuguese': tensor([13]),  'Scottish': tensor([14]),  'Dutch': tensor([15]),  'Korean': tensor([16]),  'Polish': tensor([17])}   Let’s store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer.   num_langs = len(lang2label)   Preprocessing   Now, let’s preprocess the names. We first want to use unidecode to standardize all names and remove any acute symbols or the likes. For example,   unidecode(\"Ślusàrski\")   'Slusarski'   Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. This can first be done by constructing a char2idx mapping, as shown below.   char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")} num_letters = len(char2idx); num_letters   59   We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-‘. This also means that each name will now be expressed as a tensor of size (num_char, 59); in other words, each character will be a tensor of size (59,)`. We can now build a function that accomplishes this task, as shown below:   def name2tensor(name):     tensor = torch.zeros(len(name), 1, num_letters)     for i, char in enumerate(name):         tensor[i][0][char2idx[char]] = 1     return tensor   If you read the code carefully, you’ll realize that the output tensor is of size (num_char, 1, 59), which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case. In PyTorch, RNN layers expect the input tensor to be of size (seq_len, batch_size, input_size). Since every name is going to have a different length, we don’t batch the inputs for simplicity purposes and simply use each input as a single batch. For a more detailed discussion, check out this forum discussion.   Let’s quickly verify the output of the name2tensor() function with a dummy input.   name2tensor(\"abc\")   tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0.]],          [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0.]],          [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,           0., 0., 0., 0., 0., 0., 0., 0.]]])   Dataset Creation   Now we need to build a our dataset with all the preprocessing steps. Let’s collect all the decoded and converted tensors in a list, with accompanying labels. The labels can be obtained easily from the file name, for example german.txt.   tensor_names = [] target_langs = []  for file in os.listdir(data_dir):     with open(os.path.join(data_dir, file)) as f:         lang = file.split(\".\")[0]         names = [unidecode(line.rstrip()) for line in f]         for name in names:             try:                 tensor_names.append(name2tensor(name))                 target_langs.append(lang2label[lang])             except KeyError:                 pass   We could wrap this in a PyTorch Dataset class, but for simplicity sake let’s just use a good old for loop to feed this data into our model. Since we are dealing with normal lists, we can easily use sklearn’s train_test_split() to separate the training data from the testing data.   from sklearn.model_selection import train_test_split  train_idx, test_idx = train_test_split(     range(len(target_langs)),      test_size=0.1,      shuffle=True,      stratify=target_langs )  train_dataset = [     (tensor_names[i], target_langs[i])     for i in train_idx ]  test_dataset = [     (tensor_names[i], target_langs[i])     for i in test_idx ]   Let’s see how many training and testing data we have. Note that we used a test_size of 0.1.   print(f\"Train: {len(train_dataset)}\") print(f\"Test: {len(test_dataset)}\")   Train: 18063 Test: 2007   Model   We will be building two models: a simple RNN, which is going to be built from scratch, and a GRU-based model using PyTorch’s layers.   Simple RNN   Now we can build our model. This is a very simple RNN that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. Notice that it is just some fully connected layers with a sigmoid non-linearity applied during the hidden state computation.   class MyRNN(nn.Module):     def __init__(self, input_size, hidden_size, output_size):         super(MyRNN, self).__init__()         self.hidden_size = hidden_size         self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)         self.in2output = nn.Linear(input_size + hidden_size, output_size)          def forward(self, x, hidden_state):         combined = torch.cat((x, hidden_state), 1)         hidden = torch.sigmoid(self.in2hidden(combined))         output = self.in2output(combined)         return output, hidden          def init_hidden(self):         return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))   We call init_hidden() at the start of every new batch. For easier training and learning, I decided to use kaiming_uniform_() to initialize these hidden states.   We can now build our model and start training it.   hidden_size = 256 learning_rate = 0.001  model = MyRNN(num_letters, hidden_size, num_langs) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   I realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. Nonetheless, I didn’t want to cook my 13-inch MacBook Pro so I decided to stop at two epochs.   num_epochs = 2 print_interval = 3000  for epoch in range(num_epochs):     random.shuffle(train_dataset)     for i, (name, label) in enumerate(train_dataset):         hidden_state = model.init_hidden()         for char in name:             output, hidden_state = model(char, hidden_state)         loss = criterion(output, label)          optimizer.zero_grad()         loss.backward()         nn.utils.clip_grad_norm_(model.parameters(), 1)         optimizer.step()                  if (i + 1) % print_interval == 0:             print(                 f\"Epoch [{epoch + 1}/{num_epochs}], \"                 f\"Step [{i + 1}/{len(train_dataset)}], \"                 f\"Loss: {loss.item():.4f}\"             )   Epoch [1/2], Step [3000/18063], Loss: 0.0390 Epoch [1/2], Step [6000/18063], Loss: 1.0368 Epoch [1/2], Step [9000/18063], Loss: 0.6718 Epoch [1/2], Step [12000/18063], Loss: 0.0003 Epoch [1/2], Step [15000/18063], Loss: 1.0658 Epoch [1/2], Step [18000/18063], Loss: 1.0021 Epoch [2/2], Step [3000/18063], Loss: 0.0021 Epoch [2/2], Step [6000/18063], Loss: 0.0131 Epoch [2/2], Step [9000/18063], Loss: 0.3842 Epoch [2/2], Step [12000/18063], Loss: 0.0002 Epoch [2/2], Step [15000/18063], Loss: 2.5420 Epoch [2/2], Step [18000/18063], Loss: 0.0172   Now we can test our model. We could look at other metrics, but accuracy is by far the simplest, so let’s go with that.   num_correct = 0 num_samples = len(test_dataset)  model.eval()  with torch.no_grad():     for name, label in test_dataset:         hidden_state = model.init_hidden()         for char in name:             output, hidden_state = model(char, hidden_state)         _, pred = torch.max(output, dim=1)         num_correct += bool(pred == label)  print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")   Accuracy: 72.2471%   The model records a 72 percent accuracy rate. This is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple RNN model was at least able to learn something.   Let’s see how well our model does with some concrete examples. Below is a function that accepts a string as input and outputs a decoded prediction.   label2lang = {label.item(): lang for lang, label in lang2label.items()}  def myrnn_predict(name):     model.eval()     tensor_name = name2tensor(name)     with torch.no_grad():         hidden_state = model.init_hidden()         for char in tensor_name:             output, hidden_state = model(char, hidden_state)         _, pred = torch.max(output, dim=1)     model.train()         return label2lang[pred.item()]   I don’t know if any of these names were actually in the training or testing set; these are just some random names I came up with that I thought would be pretty reasonable. And voila, the results are promising.   myrnn_predict(\"Mike\")   'English'   myrnn_predict(\"Qin\")   'Chinese'   myrnn_predict(\"Slaveya\")   'Russian'   The model seems to have classified all the names into correct categories!   PyTorch GRU   This is cool and all, and I could probably stop here, but I wanted to see how this custom model fares in comparison to, say, a model using PyTorch layers. GRU is probably not fair game for our simple RNN, but let’s see how well it does.   class GRUModel(nn.Module):     def __init__(self, num_layers, hidden_size):         super(GRUModel, self).__init__()         self.num_layers = num_layers         self.hidden_size = hidden_size         self.gru = nn.GRU(             input_size=num_letters,              hidden_size=hidden_size,              num_layers=num_layers,         )         self.fc = nn.Linear(hidden_size, num_langs)          def forward(self, x):         hidden_state = self.init_hidden()         output, hidden_state = self.gru(x, hidden_state)         output = self.fc(output[-1])         return output          def init_hidden(self):         return torch.zeros(self.num_layers, 1, self.hidden_size).to(device)   Let’s declare the model and an optimizer to go with it. Notice that we are using a two-layer GRU, which is already one more than our current RNN implementation.   model = GRUModel(num_layers=2, hidden_size=hidden_size) optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   for epoch in range(num_epochs):     random.shuffle(train_dataset)     for i, (name, label) in enumerate(train_dataset):         output = model(name)         loss = criterion(output, label)          optimizer.zero_grad()         loss.backward()         optimizer.step()                   if (i + 1) % print_interval == 0:             print(                 f\"Epoch [{epoch + 1}/{num_epochs}], \"                 f\"Step [{i + 1}/{len(train_dataset)}], \"                 f\"Loss: {loss.item():.4f}\"             )   Epoch [1/2], Step [3000/18063], Loss: 1.8497 Epoch [1/2], Step [6000/18063], Loss: 0.4908 Epoch [1/2], Step [9000/18063], Loss: 1.0299 Epoch [1/2], Step [12000/18063], Loss: 0.0855 Epoch [1/2], Step [15000/18063], Loss: 0.0053 Epoch [1/2], Step [18000/18063], Loss: 2.6417 Epoch [2/2], Step [3000/18063], Loss: 0.0004 Epoch [2/2], Step [6000/18063], Loss: 0.0008 Epoch [2/2], Step [9000/18063], Loss: 0.1446 Epoch [2/2], Step [12000/18063], Loss: 0.2125 Epoch [2/2], Step [15000/18063], Loss: 3.7883 Epoch [2/2], Step [18000/18063], Loss: 0.4862   The training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. This is partially because I didn’t use gradient clipping for this GRU model, and we might see better results with clipping applied.   Let’s see the accuracy of this model.   num_correct = 0  model.eval()  with torch.no_grad():     for name, label in test_dataset:         output = model(name)         _, pred = torch.max(output, dim=1)         num_correct += bool(pred == label)  print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")   Accuracy: 81.4150%   And we get an accuracy of around 80 percent for this model. This is better than our simple RNN model, which is somewhat expected given that it had one additional layer and was using a more complicated RNN cell model.   Let’s see how this model predicts given some raw name string.   def pytorch_predict(name):     model.eval()     tensor_name = name2tensor(name)     with torch.no_grad():         output = model(tensor_name)         _, pred = torch.max(output, dim=1)     model.train()     return label2lang[pred.item()]   pytorch_predict(\"Jake\")   'English'   pytorch_predict(\"Qin\")   'Chinese'   pytorch_predict(\"Fernando\")   'Spanish'   pytorch_predict(\"Demirkan\")   'Russian'   The last one is interesting, because it is the name of a close Turkish friend of mine. The model obviously isn’t able to tell us that the name is Turkish since it didn’t see any data points that were labeled as Turkish, but it tells us what nationality the name might fall under among the 18 labels it has been trained on. It’s obviously wrong, but perhaps not too far off in some regards; at least it didn’t say Japanese, for instance. It’s also not entirely fair game for the model since there are many names that might be described as multi-national: perhaps there is a Russian person with the name of Demirkan.   Conclusion   I learned quite a bit about RNNs by implementing this RNN. It is admittedly simple, and it is somewhat different from the PyTorch layer-based approach in that it requires us to loop through each character manually, but the low-level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. It was also a healthy reminder of how RNNs can be difficult to train.   In the coming posts, we will be looking at sequence-to-sequence models, or seq2seq for short. Ever since I heard about seq2seq, I was fascinated by tthe power of transforming one form of data to another. Although these models cannot be realistically trained on a CPU given the constraints of my local machine, I think implementing them themselves will be an exciting challenge.   Catch you up in the next one!  ","categories": ["study"],
        "tags": ["pytorch","deep_learning","from_scratch"],
        "url": "http://localhost:4000/study/pytorch-rnn/",
        "teaser": null
      },{
        "title": "AI Evolution: Weekly News Digest",
        "excerpt":"Introduction  AI is revolutionizing various aspects of our daily lives, with new applications and advancements emerging every week. Here’s a roundup of some of the most impactful and intriguing AI news from the past week.   Chatbots in the Classroom  In a groundbreaking move, certain US primary and secondary schools are testing an automated tutor called ‘Khanmigo’, built by online educator Khan Academy. Based on the GPT-4 architecture, the bot challenges students by replying to queries with questions, fostering critical thinking. Integrated with the Khan Academy’s previous tutoring software, it aids students in vocabulary practice, creative writing, debates, and even in navigating university admissions and financial aid. While some educators fear it may encourage cheating or spread misinformation, others see it as an invaluable 24/7 learning resource.  Training Data Free-For-All in Japan   In a striking legislative decision, Japan has given the green light for AI developers to train models on copyrighted works, a move that has sparked debates about fairness and copyright laws. This law permits developers to use copyrighted works for commercial purposes, which is quite unique globally. As the G7 countries strive to create mutually compatible regulations for generative AI, Japan’s stance could influence the direction these policies take.   Image Generation Becomes Swift with Paella  A new system, Paella, developed by Dominic Rampas and colleagues, leverages diffusion processes to generate high-quality images swiftly. By utilizing tokens from a predefined list, the number of steps needed for image generation is greatly reduced, making the process quicker without sacrificing quality. This technique could pave the way for a host of applications, from engineering to entertainment.   Google’s Confidentiality Concerns with Chatbots  In light of potential information leaks, Google has advised its employees against entering confidential information into chatbots like OpenAI’s ChatGPT or Google’s own Bard. It highlights the security concerns related to AI, prompting an essential conversation about data privacy and confidentiality in the age of AI.   Self-driving Cars: Motion Prediction Improves  The research into self-driving cars is far from over. Waymo has showcased how diffusion models can be used to predict distributions of motion for multiple “agents” on the road. This innovation improves performance over physics-based methods and other neural algorithms, nudging us closer to the reality of self-driving cars on our streets.   Fine-tuning Large Models Becomes More Accessible  A significant development in the field of AI is the advancement of Low Rank Adaptation (LoRA), a task-specific and model-specific module used for fine-tuning large models. The improved LoRA enables fine-tuning on relatively inexpensive hardware, thus democratizing access to sophisticated AI capabilities.   Research Section: FinGPT: An Open-Source Framework for FinLLMs  In response to the aforementioned challenges and in pursuit of democratizing FinLLMs, we present FinGPT. A data-centric, open-source framework, FinGPT aims to provide a robust and comprehensive solution for the development and application of FinLLMs.   4.1 Data Source Layer   As a starting point, we put forward the data source layer, which ensures thorough market coverage. In consideration of the temporal sensitivity of financial data, this layer is designed to provide real-time information capture from various sources, including financial news, company filings and announcements, social media discussions, and financial trends. This rich collection of diverse data types allows FinGPT to provide multi-faceted insights into the financial landscape.   4.2 Data Engineering Layer   The next layer in the FinGPT framework is the data engineering layer. Primed for real-time NLP data processing, this layer grapples with the inherent challenges of high temporal sensitivity and low signal-to-noise ratio in financial data. The data engineering layer involves comprehensive data cleaning, preprocessing, and formatting to ensure the quality and usefulness of the financial data fed into the subsequent layers of the FinGPT framework.   4.3 LLMs Layer   Building on the previous layers, we introduce the LLMs layer, which focuses on the implementation of a range of fine-tuning methodologies for the LLMs. Recognizing the highly dynamic nature of financial data, this layer prioritizes keeping the model’s relevance and accuracy up-to-date, ensuring that FinGPT maintains its ability to provide accurate and actionable insights.   4.4 Application Layer   The final layer of the FinGPT framework is the application layer, which showcases practical applications and demonstrations of FinGPT in the financial sector. This layer provides concrete examples of how FinGPT can be utilized in various contexts, such as robo-advising, algorithmic trading, and low-code development. It serves as a testament to FinGPT’s potential to revolutionize financial operations and decision-making processes.   5 Paper Conclusion   Through our development of FinGPT, we aim to foster a thriving, open-source ecosystem for financial large language models (FinLLMs). We hope that this framework will stimulate further innovation in the finance domain, facilitate the democratization of FinLLMs, and unlock new opportunities in open finance. By championing data accessibility and transparency, FinGPT is positioned to reshape the understanding and application of FinLLMs in financial research and practice.   Data Preparation for Price Data and Tweets First, we fetch price data and Tweets data from stocknet-dataset Second, we input Tweets data to a GPT model, say \"text-curie-001\" or \"text-davinci-003\", and get the corresponding sentiment scores Third, we save the sentiment scores to a file under ./data   ChatGPT Trading Agent We calculate the average sentiment score S.  We implement a simple strategy that buys 100 shares when S &gt;= 0.3 and sells 100 shares when S &lt;= -0.3  Parameters of GPT Model are:  \"model_name\": \"text-davinci-003\",  # \"text-curie-001\",\"text-davinci-003\" \"source\": \"local\",                 # \"local\",\"openai\" \"api_key\": OPEN_AI_TOKEN,          # not necessary when the \"source\" is \"local\" \"buy_threshold\": 0.3,              # the max positive sentiment is 1, so this should range from 0 to 1  \"sell_threshold\": -0.3             # the min negative sentiment is -1, so this should range from -1 to 0   Backtest We backtest the agent's performance from '2014-01-01' to '2015-12-30'.  Parameters are:  \"stock_name\" : \"AAPL\",        # please refer to the stocks provided by stocknet-dataset \"start_date\":\"2014-01-01\",    # should be later than 2014-01-01 \"end_date\":\"2015-12-30\",      # should be earlier than 2015-12-30 \"init_cash\": 100,             # initial available cash \"init_hold\": 0,               # initial available stock holdings \"cal_on\": \"Close\",            # The column that used to calculate prices \"trade_volumn\": 100,          # Volumns to trade   The result is shown as follows:      The performance metrics are as follows metrics\tresult Annual return\t30.603% Cumulative returns\t66.112% Annual volatility\t13.453% Sharpe ratio\t2.06 Calmar ratio\t4.51 Stability\t0.87 Max drawdown\t-6.778% Omega ratio\t2.00 Sortino ratio\t4.30 Tail ratio\t1.84 Daily value at risk\t-1.585% Alpha\t0.24 Beta\t0.31   Conclusion  As AI continues to evolve at a rapid pace, it is more important than ever to stay informed about the latest developments and discussions. The articles featured in this week’s AI Evolution: Weekly News Digest provide a glimpse into the diverse and transformative applications of AI.   From the integration of chatbots in classrooms to the implications of training data free-for-all in Japan, these stories shed light on the opportunities and challenges presented by AI. The advancements in image generation, motion prediction for self-driving cars, and the democratization of fine-tuning large models further emphasize the progress we are making.   Moreover, the introduction of FinGPT as an open-source framework for FinLLMs opens up new possibilities in the financial domain, fostering innovation and accessibility. It holds the potential to revolutionize financial research and decision-making processes, making them more inclusive and transparent.   By staying up-to-date with these developments, I can actively participate in shaping the future of AI.   ","categories": ["blog"],
        "tags": ["pytorch","news"],
        "url": "http://localhost:4000/blog/AI-Weeky/",
        "teaser": null
      },{
        "title": "Stock Price Prediction of Apple with PyTorch",
        "excerpt":"LSTM and GRU   Time Series  Machine Learning’s captivating domain of time series forecasting commands attention, offering potentially significant benefits when integrated with advanced subjects like stock price prediction. Essentially, time series forecasting employs a specific model to anticipate future data points by utilizing the patterns found in previously observed value.   A time series, by definition, is a sequence of data points arranged in chronological order. This kind of problem holds significant relevance because numerous prediction challenges incorporate a temporal aspect. Uncovering the relationship between data and time is crucial to these analyses, such as weather forecasting or earthquake prediction. However, these issues are occasionally overlooked due to the non-trivial complexities involved in modeling these temporal relationships.   Stock market prediction entails efforts to estimate the future value of a company’s stock. The accurate prognostication of a stock’s future price can result in substantial gains, fitting this scenario into the realm of time series problems.   Over time, numerous methods have been developed to predict stock prices accurately, given their volatile and complex fluctuations. Neural networks, particularly Recurrent Neural Networks (RNNs), have exhibited significant applicability in this field. In this context, we will construct two distinct RNN models — Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) — using PyTorch. We aim to forecast Apples’s stock market price and compare these models’ performance in aspects of time and efficiency.   Recurrent Neural Network (RNN)   A Recurrent Neural Network (RNN) is a particular breed of artificial neural network crafted to discern patterns in sequential data to anticipate ensuing events. The power of this architecture lies in its interconnected nodes, enabling it to demonstrate dynamic behavior over time. Another notable attribute of this structure is the utilization of feedback loops for sequence processing. This feature facilitates the persistence of information, often likened to memory, rendering RNNs ideal for Natural Language Processing (NLP) and time series problems. This foundational structure gave rise to advanced architectures such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU).   An LSTM unit comprises a cell and three gates: an input gate, an output gate, and a forget gate. The cell retains values over arbitrary time intervals, while the trio of gates control the influx and efflux of information from the cell.      Conversely, a GRU possesses fewer parameters compared to an LSTM as it lacks an output gate. However, both configurations are capable of resolving the “short-term memory” problem typically associated with basic RNNs, and successfully maintain long-term correlations in sequential data.      While LSTM enjoys greater popularity at present, it’s anticipated that GRU will ultimately surpass it due to its enhanced speed while maintaining comparable accuracy and effectiveness. It’s likely that we’ll observe a similar outcome in this case, with the GRU model demonstrating superior performance under these conditions.   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory  import os for dirname, _, filenames in os.walk('./archive'):     for filename in filenames:         print(os.path.join(dirname, filename))    ./archive/JPM_2006-01-01_to_2018-01-01.csv ./archive/MSFT_2006-01-01_to_2018-01-01.csv ./archive/JNJ_2006-01-01_to_2018-01-01.csv ./archive/UNH_2006-01-01_to_2018-01-01.csv ./archive/CAT_2006-01-01_to_2018-01-01.csv ./archive/AABA_2006-01-01_to_2018-01-01.csv ./archive/HD_2006-01-01_to_2018-01-01.csv ./archive/CVX_2006-01-01_to_2018-01-01.csv ./archive/MMM_2006-01-01_to_2018-01-01.csv ./archive/AMZN_2006-01-01_to_2018-01-01.csv ./archive/CSCO_2006-01-01_to_2018-01-01.csv ./archive/XOM_2006-01-01_to_2018-01-01.csv ./archive/all_stocks_2017-01-01_to_2018-01-01.csv ./archive/VZ_2006-01-01_to_2018-01-01.csv ./archive/WMT_2006-01-01_to_2018-01-01.csv ./archive/GS_2006-01-01_to_2018-01-01.csv ./archive/AAPL_2006-01-01_to_2018-01-01.csv ./archive/AXP_2006-01-01_to_2018-01-01.csv ./archive/all_stocks_2006-01-01_to_2018-01-01.csv ./archive/GOOGL_2006-01-01_to_2018-01-01.csv ./archive/UTX_2006-01-01_to_2018-01-01.csv ./archive/KO_2006-01-01_to_2018-01-01.csv ./archive/MRK_2006-01-01_to_2018-01-01.csv ./archive/TRV_2006-01-01_to_2018-01-01.csv ./archive/IBM_2006-01-01_to_2018-01-01.csv ./archive/INTC_2006-01-01_to_2018-01-01.csv ./archive/PFE_2006-01-01_to_2018-01-01.csv ./archive/GE_2006-01-01_to_2018-01-01.csv ./archive/DIS_2006-01-01_to_2018-01-01.csv ./archive/PG_2006-01-01_to_2018-01-01.csv ./archive/BA_2006-01-01_to_2018-01-01.csv ./archive/MCD_2006-01-01_to_2018-01-01.csv ./archive/NKE_2006-01-01_to_2018-01-01.csv   Implementation   The dataset contains historical stock prices. We are going to predict the Close price of the stock, and the following is the data behavior over the years.   filepath = './archive/AAPL_2006-01-01_to_2018-01-01.csv' data = pd.read_csv(filepath) data = data.sort_values('Date') data.head()                               Date       Open       High       Low       Close       Volume       Name                       0       2006-01-03       10.34       10.68       10.32       10.68       201853036       AAPL                 1       2006-01-04       10.73       10.85       10.64       10.71       155225609       AAPL                 2       2006-01-05       10.69       10.70       10.54       10.63       112396081       AAPL                 3       2006-01-06       10.75       10.96       10.65       10.90       176139334       AAPL                 4       2006-01-09       10.96       11.03       10.82       10.86       168861224       AAPL               import matplotlib.pyplot as plt import seaborn as sns  sns.set_style(\"darkgrid\") plt.figure(figsize = (15,9)) plt.plot(data[['Close']]) plt.xticks(range(0,data.shape[0],500),data['Date'].loc[::500],rotation=45) plt.title(\"Apple Stock Price\",fontsize=18, fontweight='bold') plt.xlabel('Date',fontsize=18) plt.ylabel('Close Price (USD)',fontsize=18) plt.show()   price = data[['Close']] price.info()   &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 3019 entries, 0 to 3018 Data columns (total 1 columns):  #   Column  Non-Null Count  Dtype   ---  ------  --------------  -----    0   Close   3019 non-null   float64 dtypes: float64(1) memory usage: 47.2 KB   I slice the data frame to get the column we want and normalize the data.   from sklearn.preprocessing import MinMaxScaler  scaler = MinMaxScaler(feature_range=(-1, 1)) price['Close'] = scaler.fit_transform(price['Close'].values.reshape(-1,1))   /var/folders/8_/k8h_gshs3_qdkr8glv1ff08h0000gn/T/ipykernel_43516/68737012.py:4: SettingWithCopyWarning:  A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead  See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy   price['Close'] = scaler.fit_transform(price['Close'].values.reshape(-1,1))   We’re now ready to partition the data into training and test sets. But prior to that, it’s necessary to determine the width of the analysis window. This technique of employing previous time steps to forecast the subsequent time step is referred to as the sliding window approach.   def split_data(stock, lookback):     data_raw = stock.to_numpy() # convert to numpy array     data = []          # create all possible sequences of length seq_len     for index in range(len(data_raw) - lookback):          data.append(data_raw[index: index + lookback])          data = np.array(data);     test_set_size = int(np.round(0.2*data.shape[0]));     train_set_size = data.shape[0] - (test_set_size);          x_train = data[:train_set_size,:-1,:]     y_train = data[:train_set_size,-1,:]          x_test = data[train_set_size:,:-1]     y_test = data[train_set_size:,-1,:]          return [x_train, y_train, x_test, y_test]   lookback = 20 # choose sequence length x_train, y_train, x_test, y_test = split_data(price, lookback) print('x_train.shape = ',x_train.shape) print('y_train.shape = ',y_train.shape) print('x_test.shape = ',x_test.shape) print('y_test.shape = ',y_test.shape)   x_train.shape =  (2399, 19, 1) y_train.shape =  (2399, 1) x_test.shape =  (600, 19, 1) y_test.shape =  (600, 1)   Next, we convert them into tensors, the foundational data structure required for constructing a model in PyTorch.   import torch import torch.nn as nn  x_train = torch.from_numpy(x_train).type(torch.Tensor) x_test = torch.from_numpy(x_test).type(torch.Tensor) y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor) y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor) y_train_gru = torch.from_numpy(y_train).type(torch.Tensor) y_test_gru = torch.from_numpy(y_test).type(torch.Tensor)   input_dim = 1 hidden_dim = 32 num_layers = 2 output_dim = 1 num_epochs = 100   LSTM   class LSTM(nn.Module):     def __init__(self, input_dim, hidden_dim, num_layers, output_dim):         super(LSTM, self).__init__()         self.hidden_dim = hidden_dim         self.num_layers = num_layers                  self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)         self.fc = nn.Linear(hidden_dim, output_dim)      def forward(self, x):         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()         out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))         out = self.fc(out[:, -1, :])          return out   model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers) criterion = torch.nn.MSELoss(reduction='mean') optimiser = torch.optim.Adam(model.parameters(), lr=0.01)   import time  hist = np.zeros(num_epochs) start_time = time.time() lstm = []  for t in range(num_epochs):     y_train_pred = model(x_train)      loss = criterion(y_train_pred, y_train_lstm)     print(\"Epoch \", t, \"MSE: \", loss.item())     hist[t] = loss.item()      optimiser.zero_grad()     loss.backward()     optimiser.step()      training_time = time.time()-start_time print(\"Training time: {}\".format(training_time))   Epoch  0 MSE:  0.25819653272628784 Epoch  1 MSE:  0.15166231989860535 Epoch  2 MSE:  0.20903076231479645 Epoch  3 MSE:  0.13991586863994598 Epoch  4 MSE:  0.1287676990032196 Epoch  5 MSE:  0.13563665747642517 Epoch  6 MSE:  0.13573406636714935 Epoch  7 MSE:  0.12434989213943481 Epoch  8 MSE:  0.1031389907002449 Epoch  9 MSE:  0.0779111385345459 Epoch  10 MSE:  0.06311900913715363 Epoch  11 MSE:  0.07091287523508072 Epoch  12 MSE:  0.052488457411527634 Epoch  13 MSE:  0.023740172386169434 Epoch  14 MSE:  0.01886197179555893 Epoch  15 MSE:  0.023339685052633286 Epoch  16 MSE:  0.017914501950144768 Epoch  17 MSE:  0.010528038255870342 Epoch  18 MSE:  0.020709635689854622 Epoch  19 MSE:  0.022142166271805763 Epoch  20 MSE:  0.00913378968834877 Epoch  21 MSE:  0.0032837125472724438 Epoch  22 MSE:  0.006005624774843454 Epoch  23 MSE:  0.009221922606229782 Epoch  24 MSE:  0.009343614801764488 Epoch  25 MSE:  0.007402882911264896 Epoch  26 MSE:  0.006014988292008638 Epoch  27 MSE:  0.006637887097895145 Epoch  28 MSE:  0.007978024892508984 Epoch  29 MSE:  0.007520338520407677 Epoch  30 MSE:  0.005026531405746937 Epoch  31 MSE:  0.002815255429595709 Epoch  32 MSE:  0.002627915469929576 Epoch  33 MSE:  0.0038487249985337257 Epoch  34 MSE:  0.004618681501597166 Epoch  35 MSE:  0.0039092665538191795 Epoch  36 MSE:  0.002484086435288191 Epoch  37 MSE:  0.0018542427569627762 Epoch  38 MSE:  0.002269477816298604 Epoch  39 MSE:  0.002432651352137327 Epoch  40 MSE:  0.0017461515963077545 Epoch  41 MSE:  0.001250344910658896 Epoch  42 MSE:  0.0016412724507972598 Epoch  43 MSE:  0.0022176974453032017 Epoch  44 MSE:  0.002139537362381816 Epoch  45 MSE:  0.0015903041930869222 Epoch  46 MSE:  0.0013555067125707865 Epoch  47 MSE:  0.0015729618025943637 Epoch  48 MSE:  0.0015560296596959233 Epoch  49 MSE:  0.0010814378038048744 Epoch  50 MSE:  0.0007583849364891648 Epoch  51 MSE:  0.0008783523226156831 Epoch  52 MSE:  0.0010345984483137727 Epoch  53 MSE:  0.0009140677284449339 Epoch  54 MSE:  0.0007315980619750917 Epoch  55 MSE:  0.0007604123093187809 Epoch  56 MSE:  0.0008461487013846636 Epoch  57 MSE:  0.0007311741355806589 Epoch  58 MSE:  0.0005497800884768367 Epoch  59 MSE:  0.0005577158881351352 Epoch  60 MSE:  0.0006879133288748562 Epoch  61 MSE:  0.0007144041010178626 Epoch  62 MSE:  0.0006207934347912669 Epoch  63 MSE:  0.0005688412929885089 Epoch  64 MSE:  0.0005930980551056564 Epoch  65 MSE:  0.0005647227517329156 Epoch  66 MSE:  0.00047004505177028477 Epoch  67 MSE:  0.0004422623314894736 Epoch  68 MSE:  0.0004969367873854935 Epoch  69 MSE:  0.0005126534379087389 Epoch  70 MSE:  0.00046223439858295023 Epoch  71 MSE:  0.00043931364780291915 Epoch  72 MSE:  0.0004609820316545665 Epoch  73 MSE:  0.00045188714284449816 Epoch  74 MSE:  0.0004201307019684464 Epoch  75 MSE:  0.00043058270239271224 Epoch  76 MSE:  0.0004576134087983519 Epoch  77 MSE:  0.0004446248640306294 Epoch  78 MSE:  0.0004175296053290367 Epoch  79 MSE:  0.00041787829832173884 Epoch  80 MSE:  0.00041931969462893903 Epoch  81 MSE:  0.0004008542455267161 Epoch  82 MSE:  0.0003951751277782023 Epoch  83 MSE:  0.0004096981429029256 Epoch  84 MSE:  0.00041126698488369584 Epoch  85 MSE:  0.0003979630710091442 Epoch  86 MSE:  0.0003955823485739529 Epoch  87 MSE:  0.0004000987682957202 Epoch  88 MSE:  0.0003947264631278813 Epoch  89 MSE:  0.00039027887396514416 Epoch  90 MSE:  0.00039583834586665034 Epoch  91 MSE:  0.00039670622209087014 Epoch  92 MSE:  0.0003883809840772301 Epoch  93 MSE:  0.00038467306876555085 Epoch  94 MSE:  0.00038626548484899104 Epoch  95 MSE:  0.00038355711149051785 Epoch  96 MSE:  0.0003808493784163147 Epoch  97 MSE:  0.00038343065534718335 Epoch  98 MSE:  0.0003837483818642795 Epoch  99 MSE:  0.00037956441519781947 Training time: 8.352406024932861   predict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy())) original = pd.DataFrame(scaler.inverse_transform(y_train_lstm.detach().numpy()))   import seaborn as sns sns.set_style(\"darkgrid\")      fig = plt.figure() fig.subplots_adjust(hspace=0.2, wspace=0.2)  plt.subplot(1, 2, 1) ax = sns.lineplot(x = original.index, y = original[0], label=\"Data\", color='royalblue') ax = sns.lineplot(x = predict.index, y = predict[0], label=\"Training Prediction (LSTM)\", color='tomato') ax.set_title('Stock price', size = 14, fontweight='bold') ax.set_xlabel(\"Days\", size = 14) ax.set_ylabel(\"Cost (USD)\", size = 14) ax.set_xticklabels('', size=10)   plt.subplot(1, 2, 2) ax = sns.lineplot(data=hist, color='royalblue') ax.set_xlabel(\"Epoch\", size = 14) ax.set_ylabel(\"Loss\", size = 14) ax.set_title(\"Training Loss\", size = 14, fontweight='bold') fig.set_figheight(6) fig.set_figwidth(16)      import math, time from sklearn.metrics import mean_squared_error  # make predictions y_test_pred = model(x_test)  # invert predictions y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy()) y_train = scaler.inverse_transform(y_train_lstm.detach().numpy()) y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy()) y_test = scaler.inverse_transform(y_test_lstm.detach().numpy())  # calculate root mean squared error trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0])) print('Train Score: %.2f RMSE' % (trainScore)) testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0])) print('Test Score: %.2f RMSE' % (testScore)) lstm.append(trainScore) lstm.append(testScore) lstm.append(training_time)   Train Score: 1.65 RMSE Test Score: 5.56 RMSE   # shift train predictions for plotting trainPredictPlot = np.empty_like(price) trainPredictPlot[:, :] = np.nan trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred  # shift test predictions for plotting testPredictPlot = np.empty_like(price) testPredictPlot[:, :] = np.nan testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred  original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))  predictions = np.append(trainPredictPlot, testPredictPlot, axis=1) predictions = np.append(predictions, original, axis=1) result = pd.DataFrame(predictions)   import plotly.express as px import plotly.graph_objects as go  fig = go.Figure() fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],                     mode='lines',                     name='Train prediction'))) fig.add_trace(go.Scatter(x=result.index, y=result[1],                     mode='lines',                     name='Test prediction')) fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],                     mode='lines',                     name='Actual Value'))) fig.update_layout(     xaxis=dict(         showline=True,         showgrid=True,         showticklabels=False,         linecolor='white',         linewidth=2     ),     yaxis=dict(         title_text='Close (USD)',         titlefont=dict(             family='Rockwell',             size=12,             color='white',         ),         showline=True,         showgrid=True,         showticklabels=True,         linecolor='white',         linewidth=2,         ticks='outside',         tickfont=dict(             family='Rockwell',             size=12,             color='white',         ),     ),     showlegend=True,     template = 'plotly_dark'  )    annotations = [] annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,                               xanchor='left', yanchor='bottom',                               text='Results (LSTM)',                               font=dict(family='Rockwell',                                         size=26,                                         color='white'),                               showarrow=False)) fig.update_layout(annotations=annotations)  fig.show()     The model behaves well with the training set, and it also has solid performace with the test set. The model is probably overfitting, especially taking into consideration that the loss is minimal after the 40th epoch.   GRU   class GRU(nn.Module):     def __init__(self, input_dim, hidden_dim, num_layers, output_dim):         super(GRU, self).__init__()         self.hidden_dim = hidden_dim         self.num_layers = num_layers                  self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)         self.fc = nn.Linear(hidden_dim, output_dim)      def forward(self, x):         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()         out, (hn) = self.gru(x, (h0.detach()))         out = self.fc(out[:, -1, :])          return out   model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers) criterion = torch.nn.MSELoss(reduction='mean') optimiser = torch.optim.Adam(model.parameters(), lr=0.01)   hist = np.zeros(num_epochs) start_time = time.time() gru = []  for t in range(num_epochs):     y_train_pred = model(x_train)      loss = criterion(y_train_pred, y_train_gru)     print(\"Epoch \", t, \"MSE: \", loss.item())     hist[t] = loss.item()      optimiser.zero_grad()     loss.backward()     optimiser.step()  training_time = time.time()-start_time     print(\"Training time: {}\".format(training_time))   Epoch  0 MSE:  0.3607046902179718 Epoch  1 MSE:  0.13102510571479797 Epoch  2 MSE:  0.19682788848876953 Epoch  3 MSE:  0.12917079031467438 Epoch  4 MSE:  0.075187087059021 Epoch  5 MSE:  0.07155207544565201 Epoch  6 MSE:  0.06979498267173767 Epoch  7 MSE:  0.045164529234170914 Epoch  8 MSE:  0.010227248072624207 Epoch  9 MSE:  0.007098929025232792 Epoch  10 MSE:  0.03696290776133537 Epoch  11 MSE:  0.023822534829378128 Epoch  12 MSE:  0.007158784195780754 Epoch  13 MSE:  0.011737179011106491 Epoch  14 MSE:  0.018340380862355232 Epoch  15 MSE:  0.01571972481906414 Epoch  16 MSE:  0.007737305015325546 Epoch  17 MSE:  0.0022043471690267324 Epoch  18 MSE:  0.003535511204972863 Epoch  19 MSE:  0.009062006138265133 Epoch  20 MSE:  0.011892840266227722 Epoch  21 MSE:  0.009278996847569942 Epoch  22 MSE:  0.004890399053692818 Epoch  23 MSE:  0.00290724472142756 Epoch  24 MSE:  0.0037367662880569696 Epoch  25 MSE:  0.005075107794255018 Epoch  26 MSE:  0.0048552630469202995 Epoch  27 MSE:  0.0029767893720418215 Epoch  28 MSE:  0.0011302019702270627 Epoch  29 MSE:  0.0010998218785971403 Epoch  30 MSE:  0.0027486730832606554 Epoch  31 MSE:  0.003973710350692272 Epoch  32 MSE:  0.0033208823297172785 Epoch  33 MSE:  0.001725937006995082 Epoch  34 MSE:  0.0009095442364923656 Epoch  35 MSE:  0.0012437441619113088 Epoch  36 MSE:  0.001808099914342165 Epoch  37 MSE:  0.001749989576637745 Epoch  38 MSE:  0.0011168664786964655 Epoch  39 MSE:  0.0006078396691009402 Epoch  40 MSE:  0.0007392823463305831 Epoch  41 MSE:  0.0012739860685542226 Epoch  42 MSE:  0.001527499407529831 Epoch  43 MSE:  0.0012058777501806617 Epoch  44 MSE:  0.0006985657382756472 Epoch  45 MSE:  0.0005090595805086195 Epoch  46 MSE:  0.0006709578447043896 Epoch  47 MSE:  0.0008276336011476815 Epoch  48 MSE:  0.0007188778254203498 Epoch  49 MSE:  0.00046144291991367936 Epoch  50 MSE:  0.00036167059442959726 Epoch  51 MSE:  0.0005161706358194351 Epoch  52 MSE:  0.0006932021933607757 Epoch  53 MSE:  0.0006540967733599246 Epoch  54 MSE:  0.00046293693594634533 Epoch  55 MSE:  0.0003537530137691647 Epoch  56 MSE:  0.0004015856538899243 Epoch  57 MSE:  0.0004694756935350597 Epoch  58 MSE:  0.0004315198748372495 Epoch  59 MSE:  0.00033121410524472594 Epoch  60 MSE:  0.0002968600601889193 Epoch  61 MSE:  0.00035973641206510365 Epoch  62 MSE:  0.00042053856304846704 Epoch  63 MSE:  0.00039580956217832863 Epoch  64 MSE:  0.00032569453469477594 Epoch  65 MSE:  0.000298373750410974 Epoch  66 MSE:  0.0003244410618208349 Epoch  67 MSE:  0.00034042992047034204 Epoch  68 MSE:  0.00031064936774782836 Epoch  69 MSE:  0.0002729821135289967 Epoch  70 MSE:  0.00027573812985792756 Epoch  71 MSE:  0.0003066273347940296 Epoch  72 MSE:  0.0003163278743159026 Epoch  73 MSE:  0.0002932495262939483 Epoch  74 MSE:  0.00027294218307361007 Epoch  75 MSE:  0.0002778717316687107 Epoch  76 MSE:  0.0002882281842175871 Epoch  77 MSE:  0.0002800409565679729 Epoch  78 MSE:  0.0002621126768644899 Epoch  79 MSE:  0.0002582546148914844 Epoch  80 MSE:  0.000269515992840752 Epoch  81 MSE:  0.0002754448796622455 Epoch  82 MSE:  0.0002673306444194168 Epoch  83 MSE:  0.0002585184993222356 Epoch  84 MSE:  0.0002598936844151467 Epoch  85 MSE:  0.0002639705198816955 Epoch  86 MSE:  0.00026010669535025954 Epoch  87 MSE:  0.0002519670524634421 Epoch  88 MSE:  0.0002499922120478004 Epoch  89 MSE:  0.0002542092406656593 Epoch  90 MSE:  0.0002556447288952768 Epoch  91 MSE:  0.0002515747328288853 Epoch  92 MSE:  0.0002483536081854254 Epoch  93 MSE:  0.0002494044601917267 Epoch  94 MSE:  0.0002502511197235435 Epoch  95 MSE:  0.00024729460710659623 Epoch  96 MSE:  0.00024373934138566256 Epoch  97 MSE:  0.00024349824525415897 Epoch  98 MSE:  0.000244816328631714 Epoch  99 MSE:  0.0002440418174955994 Training time: 8.153863906860352   predict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy())) original = pd.DataFrame(scaler.inverse_transform(y_train_gru.detach().numpy()))   import seaborn as sns sns.set_style(\"darkgrid\")      fig = plt.figure() fig.subplots_adjust(hspace=0.2, wspace=0.2)  plt.subplot(1, 2, 1) ax = sns.lineplot(x = original.index, y = original[0], label=\"Data\", color='royalblue') ax = sns.lineplot(x = predict.index, y = predict[0], label=\"Training Prediction (GRU)\", color='tomato') ax.set_title('Stock price', size = 14, fontweight='bold') ax.set_xlabel(\"Days\", size = 14) ax.set_ylabel(\"Cost (USD)\", size = 14) ax.set_xticklabels('', size=10)   plt.subplot(1, 2, 2) ax = sns.lineplot(data=hist, color='royalblue') ax.set_xlabel(\"Epoch\", size = 14) ax.set_ylabel(\"Loss\", size = 14) ax.set_title(\"Training Loss\", size = 14, fontweight='bold') fig.set_figheight(6) fig.set_figwidth(16)      import math, time from sklearn.metrics import mean_squared_error  # make predictions y_test_pred = model(x_test)  # invert predictions y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy()) y_train = scaler.inverse_transform(y_train_gru.detach().numpy()) y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy()) y_test = scaler.inverse_transform(y_test_gru.detach().numpy())  # calculate root mean squared error trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0])) print('Train Score: %.2f RMSE' % (trainScore)) testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0])) print('Test Score: %.2f RMSE' % (testScore)) gru.append(trainScore) gru.append(testScore) gru.append(training_time)   Train Score: 1.32 RMSE Test Score: 4.90 RMSE   # shift train predictions for plotting trainPredictPlot = np.empty_like(price) trainPredictPlot[:, :] = np.nan trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred  # shift test predictions for plotting testPredictPlot = np.empty_like(price) testPredictPlot[:, :] = np.nan testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred  original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))  predictions = np.append(trainPredictPlot, testPredictPlot, axis=1) predictions = np.append(predictions, original, axis=1) result = pd.DataFrame(predictions)   import plotly.express as px import plotly.graph_objects as go  fig = go.Figure() fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],                     mode='lines',                     name='Train prediction'))) fig.add_trace(go.Scatter(x=result.index, y=result[1],                     mode='lines',                     name='Test prediction')) fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],                     mode='lines',                     name='Actual Value'))) fig.update_layout(     xaxis=dict(         showline=True,         showgrid=True,         showticklabels=False,         linecolor='white',         linewidth=2     ),     yaxis=dict(         title_text='Close (USD)',         titlefont=dict(             family='Rockwell',             size=12,             color='white',         ),         showline=True,         showgrid=True,         showticklabels=True,         linecolor='white',         linewidth=2,         ticks='outside',         tickfont=dict(             family='Rockwell',             size=12,             color='white',         ),     ),     showlegend=True,     template = 'plotly_dark'  )    annotations = [] annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,                               xanchor='left', yanchor='bottom',                               text='Results (GRU)',                               font=dict(family='Rockwell',                                         size=26,                                         color='white'),                               showarrow=False)) fig.update_layout(annotations=annotations)  fig.show()   lstm = pd.DataFrame(lstm, columns=['LSTM']) gru = pd.DataFrame(gru, columns=['GRU']) result = pd.concat([lstm, gru], axis=1, join='inner') result.index = ['Train RMSE', 'Test RMSE', 'Train Time'] result                               LSTM       GRU                       Train RMSE       1.648017       1.321451                 Test RMSE       5.562765       4.904847                 Train Time       8.352406       8.153864               Conclusion  Both models demonstrate commendable performance during the training phase, but their progress seems to plateau around the 40th epoch, indicating that the predefined 100 epochs may not be necessary.   In line with our expectations, the GRU neural network excelled over the LSTM in terms of accuracy, achieving a lower mean square error (in both training and, crucially, in the test set) and faster processing speed.   ","categories": ["blog"],
        "tags": ["pytorch","news"],
        "url": "http://localhost:4000/blog/Stock-Predcition/",
        "teaser": null
      },{
        "title": "AI Roundup: Advances, Scrutiny, and Future Projections",
        "excerpt":"Introduction  Welcome to this week’s digest of AI news. We’ve seen significant strides in technology, with innovations like 3D dog reconstruction from a single image and the continued economic impact projections of generative AI. Yet, it’s not all smooth sailing as Google faces privacy hurdles in the EU, and legal practitioners grapple with the challenges posed by AI in law. Let’s delve into these stories and more as we explore the evolving landscape of artificial intelligence.   As a someone who has deeply involved in the 3D AI realm with make work on Heymesh.ai, it thrills me to witness such significant breakthroughs. We find ourselves at a crossroads in history, where regulation bears the potential to either impede or accelerate the progress of AI. Thus, it is of paramount importance that lawmakers make informed, thoughtful decisions, enabling both the safety and continued evolution of Artificial Intelligence.   1. BITE: A New Breakthrough for 3D Dog Pose Reconstruction   A research team from ETH Zurich, the Max Planck Institute, and IMATI-CNR Italy has unveiled BITE (Body Interaction &amp; Tactile Engagement), a new technique that can accurately reconstruct a dog’s 3D shape and pose from just a single image. Even complex poses, like sitting or lying down, can be accurately represented. This marks a significant advancement in the field, as previous methods struggled with such complex poses due to a lack of 3D training data. BITE leverages ground contact information to regress 3D dogs from a single RGB image. The innovation is further enriched with a new parametric 3D dog shape model, the D-SMAL, which can represent a wide array of breeds and mixed breeds.Read More.      2. Google’s Bard AI EU Launch Delayed Over Privacy Concerns   Google has been forced to delay the European Union launch of its Bard AI due to privacy concerns. The Irish Data Protection Commission (IDPC) raised these concerns, leading to an ongoing examination of Bard. The generative AI was set to launch in the EU this week, but the IDPC hadn’t received a detailed privacy briefing, data impact assessment, or supporting information. Other AI developers, including OpenAI’s ChatGPT, are also facing scrutiny in the EU. Read More.   3. Lawyers: Beware LLMs   In a recent case, a U.S. federal judge rejected a legal brief generated by ChatGPT. The attorney who used ChatGPT to create the brief faces disciplinary action after it was discovered the brief referred to fictional cases and quotes invented by the bot. In response to this case, a federal judge in Texas decreed that lawyers in cases before him may only use generative AI to write their briefs if they manually verify the output for accuracy. This case serves as a cautionary tale for the use of AI in sensitive fields like law, where accuracy and reliability are paramount.      4. Economic Forecast: GenAI Boom   A new report by management consultancy McKinsey projects that generative AI could add between $2.6 trillion and $4.4 trillion to the global economy annually. This represents roughly 2 percent to 4 percent of the world’s combined gross domestic product. The authors examined adoption scenarios between 2040 and 2060, their effect on labor productivity through 2040, and the business impact of generative AI use cases. Despite these prospective economic gains, the technology’s potential to displace human workers is causing substantial public anxiety.      Papers Of The Week           DynIBaR: Neural Dynamic Image-Based Rendering       A research team from Google Research and Cornell Tech have put forward an intriguing paper, introducing a novel method for synthesizing new views from a monocular video of a complex dynamic scene. Current methods, such as dynamic Neural Radiance Fields (NeRFs), face challenges with long videos, complex object motions, and uncontrolled camera trajectories, often resulting in inaccurate or blurry renderings. In contrast, this research presents a volumetric image-based rendering framework, aggregating features from nearby views in a scene-motion-aware fashion. This approach not only models complex scenes and view-dependent effects but also synthesizes photo-realistic novel views from long videos with complex dynamics and unconstrained camera trajectories. The results reveal significant improvements over state-of-the-art methods and successful application to real-world videos with challenging camera and object motion.            3D Registration with Maximal Cliques       This study, by Northwestern Polytechnical University, China, showcases an innovative method for 3D point cloud registration (PCR), which involves aligning a pair of point clouds to pinpoint the optimal pose. Named Maximal Cliques (MAC), the technique uses maximal cliques in a graph to extract local consensus information and generate accurate pose hypotheses. The process consists of several steps, including constructing a compatibility graph, identifying maximal cliques, and computing transformation hypotheses. Thorough experiments on various benchmark datasets such as U3M, 3DMatch, 3DLoMatch, and KITTI revealed that MAC significantly boosts registration accuracy compared to leading methods, enhancing deep-learning-based approaches’ performance. Combined with deep-learned methods, MAC achieved state-of-the-art registration recall on 3DMatch and 3DLoMatch. The findings illustrate MAC as a highly effective technique for 3D point cloud registration, surpassing existing methods in both accuracy and performance.            Fast Segment Anything       The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.       Conclusion   This week has again demonstrated the power and potential of artificial intelligence, alongside the various challenges and uncertainties the field continues to grapple with. The groundbreaking BITE technique, developed by ETH Zurich, the Max Planck Institute, and IMATI-CNR Italy, showcases the impressive advancements in 3D shape and pose reconstruction technology. Such advancements are not only fascinating in themselves, but they are instrumental in broadening our understanding and application of AI.   On the regulatory front, we are reminded of the crucial role that law and ethics play in AI development. The delay of Google’s Bard AI launch in the EU and the implications of a generative AI boom highlight that we must navigate these developments with care. Regulatory bodies and developers must work together to ensure AI innovations can progress, while also maintaining the necessary safeguards to protect user privacy and manage socio-economic impacts.   The legal case around ChatGPT serves as a timely reminder of the limitations of AI. While AI has come a long way, it’s crucial to understand that these models, regardless of how advanced, may sometimes provide false or inaccurate information, underscoring the importance of human oversight in AI applications.   The week’s standout research papers, ‘DynIBaR: Neural Dynamic Image-Based Rendering’ and ‘3D Registration with Maximal Cliques’, demonstrate the vast scope of problems AI can help solve, as well as the continued innovation in this space.   As we navigate the exciting yet challenging terrain of AI, it’s crucial to continue exploring, questioning, and pushing the boundaries of what we believe AI can achieve. The journey of AI is one filled with immense possibilities and one we all have a stake in shaping.  ","categories": ["blog"],
        "tags": ["news"],
        "url": "http://localhost:4000/blog/AI-Weeky/",
        "teaser": null
      },{
        "title": "The AI Insider: Advancements, Challenges, and Breakthroughs in Artificial Intelligence",
        "excerpt":"Introduction   Stay Informed about AI Advancements and Trends   Welcome to this week’s edition of The AI Insider, your trusted source for the latest developments and trends in the world of artificial intelligence. As the AI landscape continues to evolve at an unprecedented pace, staying informed about breakthroughs, controversies, regulatory changes, and the commercial implications of these rapid technological evolutions becomes increasingly vital. In this edition, we delve into a wide range of captivating topics, including Meta’s struggles with their generative AI initiative, groundbreaking research at Stanford, OpenAI’s highly-anticipated release of GPT-4, recent traffic metrics for ChatGPT, AI regulation initiatives in Washington, and U.S. export control policies impacting AI chips bound for China. Additionally, we feature a groundbreaking paper on a novel approach to generative AI in dance synthesis that is sure to inspire and captivate you. So, let’s dive right in and explore the exciting world of AI.   Feature Stories   Meta’s Generative Play   Despite substantial investments in AI technologies, Meta has encountered obstacles in launching their high-profile generative AI initiative. Staff turnover, resource allocation challenges, and the lingering effects of past controversies have posed significant hurdles. To address these issues, Meta is currently undergoing a reorganization and has ambitious plans to roll out new generative AI products. They are also establishing a dedicated AI group aimed at overcoming these challenges and advancing the field of generative AI.   Fine-Tuning Neural Networks   At Stanford, researchers have achieved a groundbreaking advancement in neural networks through a method known as ‘surgical fine-tuning.’ By specifically modifying layers based on the disparity between fine-tuning data and pre-training data, they have achieved remarkable improvements in network performance. This innovative approach has the potential to revolutionize the field of neural network optimization and further enhance the capabilities of AI systems.   OpenAI Makes GPT-4 Generally Available   OpenAI has set new standards with the release of GPT-4, which is now generally available to the public. This advanced language model surpasses its predecessor, GPT-3.5, offering significantly improved capabilities. GPT-4 is expected to revolutionize AI applications across a wide range of domains, from natural language understanding to creative content generation. The enhanced performance and versatility of GPT-4 mark a significant milestone in the evolution of language models.   ChatGPT Traffic Decline   As the novelty factor begins to wane, the worldwide desktop and mobile web traffic to the ChatGPT website saw a slight decline of 9.7% from May to June. The United States experienced a slightly higher drop of 10.3%. However, it is noteworthy that ChatGPT continues to attract more global visitors than Microsoft’s search engine, Bing. This decline in traffic highlights the evolving dynamics of user engagement with AI-driven conversational platforms.   AI Regulation in Washington   In a move to foster a deeper understanding of AI regulation among legislators, Senate Majority Leader Chuck Schumer has unveiled the SAFE Innovation program. This program includes nonpartisan sessions with industry professionals and experts to balance the promotion of AI innovation with safeguarding public interest. By facilitating informed discussions, the program aims to shape AI regulations that foster responsible and ethical AI development while promoting technological progress.   U.S. Tightens Export Controls on AI Chips to China   The Biden administration has recently implemented restrictions on the sale of specific AI chips, including Nvidia’s A800, to China. This policy represents an effort to control China’s rapid technological progress and safeguard national security interests. The tightening of export controls underscores the geopolitical implications of advanced AI technologies and the evolving dynamics of AI competition on a global scale.   Paper of the Week: DisCo: Disentangled Control for Referring Human Dance Generation in Real World      This week, we feature an extraordinary paper titled “DisCo: Disentangled Control for Referring Human Dance Generation in Real World.” Published on June 30th, 2023, by a team including Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang, this groundbreaking research sets a new milestone in the field of generative AI.   Dance synthesis has long been a challenging task, particularly in generating human-centric content that accurately represents real-world dance scenarios. Existing methods often struggle to bridge the gap between synthesized content and the intricacies of genuine dance movements. This paper introduces a novel approach called “Referring Human Dance Generation,” which focuses on real-world dance scenarios with three essential properties: faithfulness, generalizability, and compositionality.   The researchers propose DISCO, a groundbreaking approach that utilizes a unique model architecture with disentangled control to enhance the faithfulness and compositionality of dance synthesis. Additionally, an innovative human attribute pre-training method improves the generalizability of the model to unseen humans. Extensive qualitative and quantitative results demonstrate that DISCO can generate high-quality human dance images and videos with diverse appearances and flexible motions.   To explore this revolutionary development further, we invite you to watch the video and visit the code, demo, and visualization resources provided by the authors.   Conclusion   That concludes this week’s edition of The AI Insider. As the world of artificial intelligence continues to evolve at an unprecedented pace, it is crucial to stay informed about the latest advancements, challenges, and breakthroughs. From Meta’s generative AI initiatives to the regulatory landscape in Washington and the tightening of export controls on AI chips, the AI landscape is ever-changing and impactful. We hope that our featured paper on generative AI in dance synthesis has inspired you and showcased the incredible potential of AI-driven creativity. Stay curious, stay informed, and be prepared for the next edition of The AI Insider as we continue to explore the frontiers of artificial intelligence together.  ","categories": ["blog"],
        "tags": ["news"],
        "url": "http://localhost:4000/blog/AI-Weeky/",
        "teaser": null
      },{
        "title": "Enhancing Recommender Systems with Graph Neural Networks",
        "excerpt":"Introduction   Recommendation systems have become ubiquitous in the digital world, and they play an integral role in the user experience. From suggesting items on an e-commerce website to recommending a movie or a song, these systems have a significant impact on user engagement and retention. Traditional recommendation systems, such as collaborative filtering and content-based filtering, have their limitations, which can be addressed by using Graph Neural Networks (GNNs).   GNNs are powerful tools that allow us to represent and work with data in the form of graphs, a flexible structure where entities (nodes) and their relationships (edges) are central. Let’s dive deep into how we can leverage GNNs to enhance recommender systems.   We will use the PyTorch Geometric library to implement our GNN-based recommendation system.   # Let's first import the necessary libraries import torch import torch.nn.functional as F from torch_geometric.nn import GCNConv from torch_geometric.data import Data   Graph Construction   One of the first steps to leveraging GNNs is the proper construction of the graph. In the context of recommender systems, we can consider users and items as nodes, and the interactions between users and items as edges. Here, we will assume we have user_item_interaction_data available, which is a pandas DataFrame having user_id, item_id, and interaction (e.g., rating) as its columns.   # Let's convert users and items into nodes unique_users = user_item_interaction_data['user_id'].unique() unique_items = user_item_interaction_data['item_id'].unique()  # Mapping users and items to unique integer values for our graph user_to_node_id = {user: i for i, user in enumerate(unique_users)} item_to_node_id = {item: i + len(unique_users) for i, item in enumerate(unique_items)}  # Now, let's construct the edges edges = torch.tensor([(user_to_node_id[row['user_id']], item_to_node_id[row['item_id']]) for _, row in user_item_interaction_data.iterrows()], dtype=torch.long).t().contiguous()   Defining Graph Convolution Network   Now that we have the edges defined, we can define our Graph Convolutional Network (GCN). A GCN is a type of GNN that leverages the power of convolutions directly on graphs.   class RecommenderGCN(torch.nn.Module):     def __init__(self, num_nodes, hidden_channels):         super(RecommenderGCN, self).__init__()         self.conv1 = GCNConv(num_nodes, hidden_channels)         self.conv2 = GCNConv(hidden_channels, hidden_channels)         self.conv3 = GCNConv(hidden_channels, 1)      def forward(self, x, edge_index):         x = self.conv1(x, edge_index)         x = x.relu()         x = self.conv2(x, edge_index)         x = x.relu()         x = self.conv3(x, edge_index)         return x.view(-1)   The model’s architecture contains three graph convolution layers. Each layer performs a specific transformation defined by a graph convolution operation, followed by a ReLU activation function.   Training the Model   Let’s train the model with the interaction data. We’ll use Mean Squared Error as our loss function, which is typically used in recommendation system tasks.   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  model = RecommenderGCN(num_nodes, 64).to(device) edges = edges.to(device) optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  model.train() for epoch in range(200):     optimizer.zero_grad()     out = model(None, edges)     loss = F.mse_loss(out, torch.tensor(user_item_interaction_data['interaction'].values, device=device))     loss.backward()     optimizer.step()     print(f'Epoch: {epoch+1}, Loss: {loss.item():.4f}')   This code will train the model for 200 epochs and print the loss at each epoch. The model parameters are updated using the Adam optimizer with a learning rate of 0.01.   By using GNNs and more specifically GCNs, we can design more powerful and flexible recommender systems that can capture complex user-item interactions. This approach paves the way for more efficient, effective, and personalized recommendation systems that could significantly improve user experiences in various applications.     I hope this in-depth exploration into the intersection of GNNs and recommendation systems provided you with useful insights. As we’ve seen, leveraging the power of graphs can significantly enhance the capabilities of recommendation systems, ultimately leading to better user experiences and increased engagement. This is just the tip of the iceberg, and I look forward to further exploration of this exciting and rapidly evolving field!   ","categories": ["blog"],
        "tags": ["study","GNNS"],
        "url": "http://localhost:4000/blog/Reccomender-Systems-GNNs/",
        "teaser": null
      }]
