<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.3 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang=" en" class="no-js">

<head>
  <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>CS229 Problem Set #1: Supervised Learning - Drew Levin</title>
<meta name="description" content="Introduction In this blog post, I will delve into the first problem set of the CS229 course on Supervised Learning. This problem set focuses on linear classifiers, specifically logistic regression and Gaussian discriminant analysis (GDA). I will explore the concepts, assumptions, and strengths and Iaknesses of these two algorithms. Additionally, I will discuss the Poisson regression and locally Iighted linear regression.">


  <meta name="author" content="Drew Levin">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Drew Levin">
<meta property="og:title" content="CS229 Problem Set #1: Supervised Learning">
<meta property="og:url" content="http://localhost:4000/blog/cs229/CS229-ProblemSet1/">


  <meta property="og:description" content="Introduction In this blog post, I will delve into the first problem set of the CS229 course on Supervised Learning. This problem set focuses on linear classifiers, specifically logistic regression and Gaussian discriminant analysis (GDA). I will explore the concepts, assumptions, and strengths and Iaknesses of these two algorithms. Additionally, I will discuss the Poisson regression and locally Iighted linear regression.">







  <meta property="article:published_time" content="2023-02-23T00:00:00-05:00">






<link rel="canonical" href="http://localhost:4000/blog/cs229/CS229-ProblemSet1/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml"
  type="application/atom+xml" rel="alternate" title="Drew Levin Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css" id="theme_source">

<link rel="stylesheet alternate" href="/assets/css/theme2.css" id="theme_source_2">
<script>
  let theme = sessionStorage.getItem('theme');
  if (theme === "dark") {
    sessionStorage.setItem('theme', 'dark');
    node1 = document.getElementById('theme_source');
    node2 = document.getElementById('theme_source_2');
    node1.setAttribute('rel', 'stylesheet alternate');
    node2.setAttribute('rel', 'stylesheet');
  }
  else {
    sessionStorage.setItem('theme', 'light');
  }
</script>

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


  <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body
  class="layout--single">
  <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

  <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

  

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Drew Levin
          <span class="site-subtitle"></span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
            <a href="/posts/" >Posts</a>
          </li><li class="masthead__menu-item">
            <a href="/resume/" >Resume</a>
          </li><li class="masthead__menu-item">
            <a href="/categories/" >Categories</a>
          </li><li class="masthead__menu-item">
            <a href="/tags/" >Tags</a>
          </li></ul>
        
        <i class="fas fa-fw fa-adjust" aria-hidden="true"
          onclick="node1=document.getElementById('theme_source');node2=document.getElementById('theme_source_2');if(node1.getAttribute('rel')=='stylesheet'){node1.setAttribute('rel', 'stylesheet alternate'); node2.setAttribute('rel', 'stylesheet');sessionStorage.setItem('theme', 'dark');}else{node2.setAttribute('rel', 'stylesheet alternate'); node1.setAttribute('rel', 'stylesheet');sessionStorage.setItem('theme', 'light');} return false;"></i>
        
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path
              d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z"
              transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

  <div class="initial-content">
    



<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="CS229 Problem Set #1: Supervised Learning">
    <meta itemprop="description" content="IntroductionIn this blog post, I will delve into the first problem set of the CS229 course on Supervised Learning. This problem set focuses on linear classifiers, specifically logistic regression and Gaussian discriminant analysis (GDA). I will explore the concepts, assumptions, and strengths and Iaknesses of these two algorithms. Additionally, I will discuss the Poisson regression and locally Iighted linear regression.">
    <meta itemprop="datePublished" content="2023-02-23T00:00:00-05:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">CS229 Problem Set #1: Supervised Learning
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read

</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu">
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#problem-1-linear-classifiers-logistic-regression-and-gda">Problem 1: Linear Classifiers (Logistic Regression and GDA)</a>
    <ul>
      <li><a href="#part-a-logistic-regression">Part (a): Logistic Regression</a></li>
      <li><a href="#part-b-logistic-regression-implementation">Part (b): Logistic Regression Implementation</a></li>
      <li><a href="#part-c-gaussian-discriminant-analysis-gda">Part (c): Gaussian Discriminant Analysis (GDA)</a></li>
      <li><a href="#part-d-maximum-likelihood-estimation">Part (d): Maximum Likelihood Estimation</a></li>
      <li><a href="#part-e-gda-implementation">Part (e): GDA Implementation</a></li>
      <li><a href="#part-f-visualization-of-decision-boundaries">Part (f): Visualization of Decision Boundaries</a></li>
      <li><a href="#part-g-comparison-of-logistic-regression-and-gda">Part (g): Comparison of Logistic Regression and GDA</a></li>
      <li><a href="#part-h-transformation-for-improved-gda-performance">Part (h): Transformation for Improved GDA Performance</a></li>
    </ul>
  </li>
  <li><a href="#problem-2-incomplete-positive-only-labels">Problem 2: Incomplete, Positive-Only Labels</a>
    <ul>
      <li><a href="#part-a-probability-of-being-labeled">Part (a): Probability of Being Labeled</a></li>
      <li><a href="#part-b-estimating-α">Part (b): Estimating α</a></li>
      <li><a href="#part-c-partial-label-classification-implementation">Part (c): Partial Label Classification Implementation</a></li>
    </ul>
  </li>
  <li><a href="#problem-3-poisson-regression">Problem 3: Poisson Regression</a>
    <ul>
      <li><a href="#part-a-exponential-family-representation">Part (a): Exponential Family Representation</a></li>
      <li><a href="#part-b-canonical-response-function">Part (b): Canonical Response Function</a></li>
      <li><a href="#part-c-stochastic-gradient-ascent-update-rule">Part (c): Stochastic Gradient Ascent Update Rule</a></li>
      <li><a href="#part-d-poisson-regression-implementation">Part (d): Poisson Regression Implementation</a></li>
    </ul>
  </li>
  <li><a href="#problem-4-convexity-of-generalized-linear-models">Problem 4: Convexity of Generalized Linear Models</a>
    <ul>
      <li><a href="#part-a-mean-of-the-distribution">Part (a): Mean of the Distribution</a></li>
      <li><a href="#part-b-variance-of-the-distribution">Part (b): Variance of the Distribution</a></li>
      <li><a href="#part-c-convexity-of-nll-loss">Part (c): Convexity of NLL Loss</a></li>
    </ul>
  </li>
  <li><a href="#problem-5-locally-iighted-linear-regression">Problem 5: Locally Iighted Linear Regression</a>
    <ul>
      <li><a href="#part-a-iighted-loss-function">Part (a): Iighted Loss Function</a></li>
      <li><a href="#part-b-normal-equation-in-iighted-setting">Part (b): Normal Equation in Iighted Setting</a></li>
      <li><a href="#part-c-locally-iighted-linear-regression-implementation">Part (c): Locally Iighted Linear Regression Implementation</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

            </nav>
          </aside>
        
        <h2 id="introduction">Introduction</h2>
<p>In this blog post, I will delve into the first problem set of the CS229 course on Supervised Learning. This problem set focuses on linear classifiers, specifically logistic regression and Gaussian discriminant analysis (GDA). I will explore the concepts, assumptions, and strengths and Iaknesses of these two algorithms. Additionally, I will discuss the Poisson regression and locally Iighted linear regression.</p>

<h2 id="problem-1-linear-classifiers-logistic-regression-and-gda">Problem 1: Linear Classifiers (Logistic Regression and GDA)</h2>
<p>The first problem in the CS229 problem set deals with logistic regression and Gaussian discriminant analysis (GDA) as linear classifiers. I are given two datasets, and our task is to perform binary classification on these datasets using logistic regression and GDA.</p>

<h3 id="part-a-logistic-regression">Part (a): Logistic Regression</h3>
<p>I are asked to find the Hessian of the average empirical loss function for logistic regression and show that it is positive semi-definite. The average empirical loss for logistic regression is defined as:</p>

<p>J(θ) = -1/m * Σ [y(i)log(hθ(x(i))) + (1-y(i))log(1-hθ(x(i)))],</p>

<p>where y(i) ∈ {0, 1}, hθ(x) = g(θ^T * x), and g(z) = 1/(1 + exp(-z)).</p>

<p>To show that the Hessian is positive semi-definite, I need to prove that z^T * Hz ≥ 0 for any vector z. I can start by showing that PΣΣz_i * x_i * x_j * z_j = (x^T * z)^2 ≥ 0. I can also use the fact that g’(z) = g(z)(1 - g(z)).</p>

<h3 id="part-b-logistic-regression-implementation">Part (b): Logistic Regression Implementation</h3>
<p>The next part of the problem involves implementing logistic regression using Newton’s Method. I need to train the logistic regression classifier using the provided training dataset and write the model’s predictions to a file. I continue training until the updates to θ become small.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">util</span>
<span class="kn">from</span> <span class="nn">linear_model</span> <span class="kn">import</span> <span class="n">LinearModel</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">eval_path</span><span class="p">,</span> <span class="n">pred_path</span><span class="p">):</span>
    <span class="s">"""Problem 1(b): Logistic regression with Newton's Method.
    
    Args:
        train_path: Path to CSV file containing dataset for training.
        eval_path: Path to CSV file containing dataset for evaluation.
        pred_path: Path to save predictions.
    """</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Train logistic regression
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Plot data and decision boundary
</span>    <span class="n">util</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">theta</span><span class="p">,</span> <span class="s">'output/p01b_{}.png'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">pred_path</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]))</span>

    <span class="c1"># Save predictions
</span>    <span class="n">x_eval</span><span class="p">,</span> <span class="n">y_eval</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">eval_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_eval</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">pred_path</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'%d'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">):</span>
    <span class="s">"""Logistic regression with Newton's Method as the solver."""</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Run Newton's Method to minimize J(theta) for logistic regression.
        
        Args:
            x: Training example inputs. Shape (m, n).
            y: Training example labels. Shape (m,).
        """</span>
        <span class="c1"># Init theta
</span>        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

        <span class="c1"># Newton's method
</span>        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="c1"># Save old theta
</span>            <span class="n">theta_old</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span>
            
            <span class="c1"># Compute Hessian Matrix
</span>            <span class="n">h_x</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)))</span>
            <span class="n">H</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">h_x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h_x</span><span class="p">)).</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>
            <span class="n">gradient_J_theta</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h_x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>

            <span class="c1"># Update theta
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">-=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">H</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">gradient_J_theta</span><span class="p">)</span>

            <span class="c1"># Check for convergence
</span>            <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">-</span> <span class="n">theta_old</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">eps</span><span class="p">:</span>
                <span class="k">break</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""Make predictions given new inputs x.
        
        Args:
            x: Inputs of shape (m, n).
            
        Returns:
            Predicted outputs of shape (m,).
        """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.5</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""Compute the predicted probabilities of class 1 given new inputs x.
        
        Args:
            x: Inputs of shape (m, n).
            
        Returns:
            Predicted probabilities of class 1 of shape (m,).
        """</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)))</span>

</code></pre></div></div>

<h3 id="part-c-gaussian-discriminant-analysis-gda">Part (c): Gaussian Discriminant Analysis (GDA)</h3>
<p>In this part, I revisit Gaussian discriminant analysis (GDA). I are given the joint distribution of (x, y) and asked to show that the posterior distribution can be written as p(y=1|x; φ, µ0, µ1, Σ) = 1/(1 + exp(-(θ^T * x + θ0))), where θ ∈ R^n and θ0 ∈ R are functions of φ, Σ, µ0, and µ1.</p>

<h3 id="part-d-maximum-likelihood-estimation">Part (d): Maximum Likelihood Estimation</h3>
<p>For this part, I assume that n (the dimension of x) is 1, and I are asked to derive the maximum likelihood estimates of the parameters φ, µ0, µ1, and Σ. I are given the dataset and need to calculate the maximum likelihood estimates based on the formulas provided.</p>

<h3 id="part-e-gda-implementation">Part (e): GDA Implementation</h3>
<p>I need to implement GDA using the provided dataset and calculate the parameters φ, µ0, µ1, and Σ. I then derive θ based on these parameters and use the resulting GDA model to make predictions on the validation set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">util</span>

<span class="kn">from</span> <span class="nn">linear_model</span> <span class="kn">import</span> <span class="n">LinearModel</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">eval_path</span><span class="p">,</span> <span class="n">pred_path</span><span class="p">):</span>
    <span class="s">"""Problem 1(e): Gaussian discriminant analysis (GDA)

    Args:
        train_path: Path to CSV file containing dataset for training.
        eval_path: Path to CSV file containing dataset for evaluation.
        pred_path: Path to save predictions.
    """</span>
    <span class="c1"># Load dataset
</span>    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># *** START CODE HERE ***
</span>    
    <span class="c1"># Train GDA
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">GDA</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Plot data and decision boundary
</span>    <span class="n">util</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">theta</span><span class="p">,</span> <span class="s">'output/p01e_{}.png'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">pred_path</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]))</span>

    <span class="c1"># Save predictions
</span>    <span class="n">x_eval</span><span class="p">,</span> <span class="n">y_eval</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">eval_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_eval</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">pred_path</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'%d'</span><span class="p">)</span>

    <span class="c1"># *** END CODE HERE ***
</span>

<span class="k">class</span> <span class="nc">GDA</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Fit a GDA model to training set given by x and y.

        Args:
            x: Training example inputs. Shape (m, n).
            y: Training example labels. Shape (m,).

        Returns:
            theta: GDA model parameters.
        """</span>
        <span class="c1"># *** START CODE HERE ***
</span>        
        <span class="c1"># Init theta
</span>        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Compute phi, mu_0, mu_1, sigma
</span>        <span class="n">y_1</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="n">y_1</span> <span class="o">/</span> <span class="n">m</span>
        <span class="n">mu_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">y_1</span><span class="p">)</span>
        <span class="n">mu_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">y_1</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">).</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu_1</span><span class="p">).</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu_1</span><span class="p">))</span> <span class="o">/</span> <span class="n">m</span>

        <span class="c1"># Compute theta
</span>        <span class="n">sigma_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">mu_0</span> <span class="o">+</span> <span class="n">mu_1</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">sigma_inv</span><span class="p">).</span><span class="n">dot</span><span class="p">(</span><span class="n">mu_0</span> <span class="o">-</span> <span class="n">mu_1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">phi</span><span class="p">)</span> <span class="o">/</span> <span class="n">phi</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">sigma_inv</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mu_1</span> <span class="o">-</span> <span class="n">mu_0</span><span class="p">)</span>
        
        <span class="c1"># Return theta
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span>

        <span class="c1"># *** END CODE HERE ***
</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""Make a prediction given new inputs x.

        Args:
            x: Inputs of shape (m, n).

        Returns:
            Outputs of shape (m,).
        """</span>
        <span class="c1"># *** START CODE HERE ***
</span>        
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)))</span>

        <span class="c1"># *** END CODE HERE
</span></code></pre></div></div>

<h3 id="part-f-visualization-of-decision-boundaries">Part (f): Visualization of Decision Boundaries</h3>
<p>In this part, I visualize the training data for Dataset 1 and plot the decision boundary found by logistic regression and GDA on the same figure. I use different symbols to represent examples with y=0 and y=1.</p>

<h3 id="part-g-comparison-of-logistic-regression-and-gda">Part (g): Comparison of Logistic Regression and GDA</h3>
<p>I repeat the visualization and comparison process for Dataset 2. I analyze which algorithm performs better and discuss the reasons for the observed performance.</p>

<h3 id="part-h-transformation-for-improved-gda-performance">Part (h): Transformation for Improved GDA Performance</h3>
<p>As an extra credit task, I explore if a transformation of the x’s can significantly improve the performance of GDA on the dataset where it initially performed worse. I discuss the transformation and its impact on GDA’s performance.</p>

<h2 id="problem-2-incomplete-positive-only-labels">Problem 2: Incomplete, Positive-Only Labels</h2>
<p>The second problem focuses on training binary classifiers in situations where I have labels only for a subset of the positive examples. I are given a dataset with true labels t(i), partial labels y(i), and input features x(i). Our task is to construct a binary classifier h for the true labels t using only the partial labels y and the input features x.</p>

<h3 id="part-a-probability-of-being-labeled">Part (a): Probability of Being Labeled</h3>
<p>I are asked to show that the probability of an example being labeled differs by a constant factor from the probability of an example being positive. I need to prove that p(t=1|x) = p(y=1|x)/α for some α ∈ R.</p>

<h3 id="part-b-estimating-α">Part (b): Estimating α</h3>
<p>In this part, I derive an expression for α using a trained classifier h and a held-out validation set V. I show that h(x(i)) ≈ α for all x(i) ∈ V+ (labeled examples). I assume that p(t=1|x) ≈ 1 when x(i) ∈ V+.</p>

<h3 id="part-c-partial-label-classification-implementation">Part (c): Partial Label Classification Implementation</h3>
<p>I are provided with a dataset and asked to implement logistic regression using the partial labels y. I train the classifier, rescale the predictions using the estimated value of α, and visualize the decision boundaries on the test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">util</span>

<span class="kn">from</span> <span class="nn">p01b_logreg</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Character to replace with sub-problem letter in plot_path/pred_path
</span><span class="n">WILDCARD</span> <span class="o">=</span> <span class="s">'X'</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">valid_path</span><span class="p">,</span> <span class="n">test_path</span><span class="p">,</span> <span class="n">pred_path</span><span class="p">):</span>
    <span class="s">"""Problem 2: Logistic regression for incomplete, positive-only labels.

    Run under the following conditions:
        1. on y-labels,
        2. on l-labels,
        3. on l-labels with correction factor alpha.

    Args:
        train_path: Path to CSV file containing training set.
        valid_path: Path to CSV file containing validation set.
        test_path: Path to CSV file containing test set.
        pred_path: Path to save predictions.
    """</span>
    <span class="n">pred_path_c</span> <span class="o">=</span> <span class="n">pred_path</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="n">WILDCARD</span><span class="p">,</span> <span class="s">'c'</span><span class="p">)</span>
    <span class="n">pred_path_d</span> <span class="o">=</span> <span class="n">pred_path</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="n">WILDCARD</span><span class="p">,</span> <span class="s">'d'</span><span class="p">)</span>
    <span class="n">pred_path_e</span> <span class="o">=</span> <span class="n">pred_path</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="n">WILDCARD</span><span class="p">,</span> <span class="s">'e'</span><span class="p">)</span>

    <span class="c1"># *** START CODE HERE ***
</span>    <span class="c1">#######################################################################################
</span>    <span class="c1"># Problem (c)
</span>    <span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">label_col</span><span class="o">=</span><span class="s">'t'</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> <span class="n">label_col</span><span class="o">=</span><span class="s">'t'</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">model_t</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
    <span class="n">model_t</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>

    <span class="n">util</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">,</span> <span class="n">model_t</span><span class="p">.</span><span class="n">theta</span><span class="p">,</span> <span class="s">'output/p02c.png'</span><span class="p">)</span>

    <span class="n">t_pred_c</span> <span class="o">=</span> <span class="n">model_t</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">pred_path_c</span><span class="p">,</span> <span class="n">t_pred_c</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'%d'</span><span class="p">)</span>
    <span class="c1">#######################################################################################
</span>    <span class="c1"># Problem (d)
</span>    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">label_col</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">test_path</span><span class="p">,</span> <span class="n">label_col</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">model_y</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
    <span class="n">model_y</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">util</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">model_y</span><span class="p">.</span><span class="n">theta</span><span class="p">,</span> <span class="s">'output/p02d.png'</span><span class="p">)</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_y</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">pred_path_d</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'%d'</span><span class="p">)</span>
    <span class="c1">#######################################################################################  
</span>    <span class="c1"># Problem (e)
</span>    <span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">valid_path</span><span class="p">,</span> <span class="n">label_col</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">model_y</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_valid</span><span class="p">))</span>

    <span class="n">correction</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">model_y</span><span class="p">.</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">util</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">,</span> <span class="n">model_y</span><span class="p">.</span><span class="n">theta</span><span class="p">,</span> <span class="s">'output/p02e.png'</span><span class="p">,</span> <span class="n">correction</span><span class="p">)</span>

    <span class="n">t_pred_e</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">/</span> <span class="n">alpha</span>
    <span class="n">np</span><span class="p">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">pred_path_e</span><span class="p">,</span> <span class="n">t_pred_e</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'%d'</span><span class="p">)</span>
    <span class="c1">#######################################################################################
</span>    <span class="c1"># *** END CODER HERE
</span>
</code></pre></div></div>

<h2 id="problem-3-poisson-regression">Problem 3: Poisson Regression</h2>
<p>The third problem focuses on Poisson regression, which is a type of generalized linear model (GLM) used for count data. I are asked to derive the properties of Poisson distribution and Poisson regression.</p>

<h3 id="part-a-exponential-family-representation">Part (a): Exponential Family Representation</h3>
<p>I need to show that the Poisson distribution is in the exponential family and provide the values for b(y), η, T(y), and a(η). The exponential family representation for the Poisson distribution is given by p(y; η) = b(y)exp(ηT(y) - a(η)).</p>

<h3 id="part-b-canonical-response-function">Part (b): Canonical Response Function</h3>
<p>I are asked to determine the canonical response function for Poisson regression. The canonical response function for a GLM with a Poisson response variable is derived by setting the mean of the Poisson distribution (λ) equal to the linear combination of the input features (θ^T * x).</p>

<h3 id="part-c-stochastic-gradient-ascent-update-rule">Part (c): Stochastic Gradient Ascent Update Rule</h3>
<p>In this part, I derive the stochastic gradient ascent update rule for Poisson regression using the negative log-likelihood loss function. I take the derivative of the log-likelihood with respect to θ and set it to zero to find the optimal θ.</p>

<h3 id="part-d-poisson-regression-implementation">Part (d): Poisson Regression Implementation</h3>
<p>I are provided with a dataset and asked to implement Poisson regression using gradient ascent to maximize the log-likelihood of θ. I train the model on the training split and make predictions on the test split.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">util</span>

<span class="kn">from</span> <span class="nn">linear_model</span> <span class="kn">import</span> <span class="n">LinearModel</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">train_path</span><span class="p">,</span> <span class="n">eval_path</span><span class="p">,</span> <span class="n">pred_path</span><span class="p">):</span>
    <span class="s">"""Problem 3(d): Poisson regression with gradient ascent.

    Args:
        lr: Learning rate for gradient ascent.
        train_path: Path to CSV file containing dataset for training.
        eval_path: Path to CSV file containing dataset for evaluation.
        pred_path: Path to save predictions.
    """</span>
    <span class="c1"># Load training set
</span>    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">train_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># *** START CODE HERE ***
</span>    
    <span class="n">model</span> <span class="o">=</span> <span class="n">PoissonRegression</span><span class="p">(</span><span class="n">step_size</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">x_eval</span><span class="p">,</span> <span class="n">y_eval</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="n">eval_path</span><span class="p">,</span> <span class="n">add_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_eval</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">pred_path</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_eval</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="s">'bx'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'true counts'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'predict counts'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'output/p03d.png'</span><span class="p">)</span>

    <span class="c1"># *** END CODE HERE ***
</span>

<span class="k">class</span> <span class="nc">PoissonRegression</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="s">"""Run gradient ascent to maximize likelihood for Poisson regression.

        Args:
            x: Training example inputs. Shape (m, n).
            y: Training example labels. Shape (m,).
        """</span>
        <span class="c1"># *** START CODE HERE ***
</span>        
        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)))</span> <span class="o">/</span> <span class="n">m</span>

            <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">-</span> <span class="n">theta</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">eps</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="c1"># *** END CODE HERE ***
</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""Make a prediction given inputs x.

        Args:
            x: Inputs of shape (m, n).

        Returns:
            Floating-point prediction for each input, shape (m,).
        """</span>
        <span class="c1"># *** START CODE HERE ***
</span>        
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">))</span>

        <span class="c1"># *** END CODE HERE ***
</span></code></pre></div></div>

<h2 id="problem-4-convexity-of-generalized-linear-models">Problem 4: Convexity of Generalized Linear Models</h2>
<p>The fourth problem explores the convexity of Generalized Linear Models (GLMs) and their use of exponential family distributions to model the output.</p>

<h3 id="part-a-mean-of-the-distribution">Part (a): Mean of the Distribution</h3>
<p>I derive an expression for the mean of an exponential family distribution and show that it can be represented as the gradient of the log-partition function with respect to the natural parameter.</p>

<h3 id="part-b-variance-of-the-distribution">Part (b): Variance of the Distribution</h3>
<p>I derive an expression for the variance of an exponential family distribution and show that it can be expressed as the derivative of the mean with respect to the</p>

<p>natural parameter.</p>

<h3 id="part-c-convexity-of-nll-loss">Part (c): Convexity of NLL Loss</h3>
<p>In this part, I write the negative log-likelihood (NLL) loss function as a function of the model parameters and calculate its Hessian. I show that the Hessian is positive semi-definite, indicating that the NLL loss of GLMs is a convex function.</p>

<h2 id="problem-5-locally-iighted-linear-regression">Problem 5: Locally Iighted Linear Regression</h2>
<p>The fifth problem introduces locally Iighted linear regression, where different training examples are Iighted differently. I minimize a Iighted sum of squared errors to find the optimal parameters.</p>

<h3 id="part-a-iighted-loss-function">Part (a): Iighted Loss Function</h3>
<p>I express the Iighted loss function in matrix form and define the appropriate Iight matrix W.</p>

<h3 id="part-b-normal-equation-in-iighted-setting">Part (b): Normal Equation in Iighted Setting</h3>
<p>I generalize the normal equation for the Iighted setting by finding the derivative of the Iighted loss function and setting it to zero. I derive the new value of θ that minimizes the Iighted loss function.</p>

<h3 id="part-c-locally-iighted-linear-regression-implementation">Part (c): Locally Iighted Linear Regression Implementation</h3>
<p>I implement locally Iighted linear regression using the derived normal equation and train the model on the provided dataset. I tune the hyperparameter τ and evaluate the model’s performance on the validation and test sets.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In this blog post, I have covered the CS229 Problem Set #1 on Supervised Learning. I explored logistic regression, Gaussian discriminant analysis (GDA), Poisson regression, and locally Iighted linear regression. I discussed the concepts, assumptions, implementations, and evaluations of these algorithms. Through this problem set, I gained a deeper understanding of linear classifiers, the use of exponential family distributions, and the importance of parameter estimation and model evaluation.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#problem-set" class="page__taxonomy-item" rel="tag">problem set</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#blog" class="page__taxonomy-item" rel="tag">blog</a><span class="sep">, </span>
    
      
      
      <a href="/categories/#cs229" class="page__taxonomy-item" rel="tag">cs229</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2023-02-23T00:00:00-05:00">February 23, 2023</time></p>


      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=CS229+Problem+Set+%231%3A+Supervised+Learning%20http%3A%2F%2Flocalhost%3A4000%2Fblog%2Fcs229%2FCS229-ProblemSet1%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fblog%2Fcs229%2FCS229-ProblemSet1%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fblog%2Fcs229%2FCS229-ProblemSet1%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/blog/CS229-Lecture1/" class="pagination--pager" title="Stanford CS229:  Linear Regression and Gradient Descent
">Previous</a>
    
    
      <a href="/blog/AI-Resources/" class="pagination--pager" title="AI Resources
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/blog/AI-Weeky/" rel="permalink">The AI Insider: Advancements, Challenges, and Breakthroughs in Artificial Intelligence
</a>
      
    </h2>
<!--
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
-->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> July 02 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">Introduction

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/blog/AI-Weeky/" rel="permalink">AI Roundup: Advances, Scrutiny, and Future Projections
</a>
      
    </h2>
<!--
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  6 minute read

</p>
    
-->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> June 25 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">Introduction
Welcome to this week’s digest of AI news. We’ve seen significant strides in technology, with innovations like 3D dog reconstruction from a singl...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/blog/Stock-Predcition/" rel="permalink">Stock Price Prediction of Apple with PyTorch
</a>
      
    </h2>
<!--
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  13 minute read

</p>
    
-->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> June 20 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">LSTM and GRU

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/blog/AI-Weeky/" rel="permalink">AI Evolution: Weekly News Digest
</a>
      
    </h2>
<!--
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read

</p>
    
-->
    
      <p class="page__meta"><i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i> June 18 2023</p>
    
    <p class="archive__item-excerpt" itemprop="description">Introduction
AI is revolutionizing various aspects of our daily lives, with new applications and advancements emerging every week. Here’s a roundup of some o...</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>

  </div>

  
  <div class="search-content">
    <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

  </div>
  

  <div class="page__footer">
    <footer>
      <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
      <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://github.com/DLevin02" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/drewlevin-/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Drew Levin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

    </footer>
  </div>

  
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154432000-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-154432000-1');
</script>





</body>

</html>